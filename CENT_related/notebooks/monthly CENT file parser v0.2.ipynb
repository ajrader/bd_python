{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser/ Exploration of new file formats from Cindy Oakley\n",
    "February 23, 2015\n",
    "\n",
    "### Outline\n",
    "1. Preliminaries\n",
    "    * load libraries and helper functions\n",
    "    * define useful dictionaries relating states to state-codes and names\n",
    "    * define listings of coverages by state\n",
    "2. Read in the raw data files\n",
    "    * process the meta-data as column headers\n",
    "    * clean up the column header names\n",
    "    * create a timestamp for the dates based upon month & year columns\n",
    "    * keep the Losses file\n",
    "\n",
    "3. Process the losses file\n",
    "    * drop Company wide and Zone totals\n",
    "    * drop sub-sections of big states (NY, CA, FL, TX) in lieu of totaled values\n",
    "    * keep only total voluntary losses: LINE=='TOTVOL'\n",
    "    * zero out the PIP or MPC values for states that lack these coverages.\n",
    "    * Calculate derived fields:\n",
    "        * Injury = BI+UBI+WBI\n",
    "        * PIP/MPC = PIP+MPC\n",
    "        * Property = PD+COLL+COMP\n",
    "        * Severity = Paid_Amt / Paid_Count\n",
    "    * Drop unused columns (for now)\n",
    "        * CWP\n",
    "        * SUIT_CNT\n",
    "        * ALAE\n",
    "        * OIE_CNT\n",
    "        * MONTH/YEAR?\n",
    "    * save as csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printall(X, max_rows=10):\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(X.to_html(max_rows=max_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary for the STATECODES\n",
    "# dictionary mapping state_codes to state-abbreviations\n",
    "stateDict ={ '01':'AL', '02':'AK', '03':'AZ', '04':'AR', '05':'CA', '06':'CO', '07':'CT', '08':'DE', '09':'DC', \n",
    "             '11':'GA', '12':'ID', '13':'IL', '14':'IN', '15':'IA', '16':'KS', '17':'KY', '18':'LA', '19':'ME',\n",
    "             '20':'MD', '22':'MI', '23':'MN', '24':'MS', '25':'MO', '26':'MT', '27':'NE', '28':'NV', '29':'NH', \n",
    "             '30':'NJ', '31':'NM', '32':'NY', '33':'NC', '34':'ND', '35':'OH', '36':'OK', '37':'OR', '38':'PA', \n",
    "             '40':'SC', '41':'SD', '42':'TN', '43':'TX', '44':'UT', '45':'VT', '46':'VA', '47':'WA', '48':'WV', \n",
    "             '49':'WI', '50':'WY', '51':'HI', '52':'NY', '53':'TX', '55':'CA', '59':'FL', '75':'CA', \n",
    "             '21':'MA', '39':'RI',\n",
    "             '94':'TX','96':'CA','97':'NY'}\n",
    "print len(stateDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stateNamesDict = {'01': 'ALABAMA', '02': 'ALASKA', '03':'ARIZONA','04':'ARKANSAS','06':'COLORADO',\n",
    "                  '07':'CONNECTICUT','08' : 'DELAWARE','09':'DIST. OF COL.','11':'GEORGIA','12':'IDAHO',    \n",
    "#when state = '05' then 'CALIF N COAST'       \n",
    "                  '13':'ILLINOIS','14':'INDIANA','15':'IOWA','16':'KANSAS','17':'KENTUCKY',\n",
    "                  '18': 'LOUISIANA','19' : 'MAINE','20' : 'MARYLAND','21' : 'MASSACHUSETTS','22' : 'MICHIGAN',       \n",
    "                  '23' : 'MINNESOTA','24' : 'MISSISSIPPI','25':'MISSOURI','26' :'MONTANA',            \n",
    "                  '27':'NEBRASKA','28':'NEVADA','29':'NEW HAMPSHIRE','30':'NEW JERSEY','31':'NEW MEXICO',\n",
    "                  #'32':'NEW YORK METRO'      \n",
    "                  '33':'NORTH CAROLINA','34':'NORTH DAKOTA','35':'OHIO','36':'OKLAHOMA','37':'OREGON',\n",
    "                  '38':'PENNSYLVANIA','39':'RHODE ISLAND','40':'SOUTH CAROLINA','41':'SOUTH DAKOTA','42':'TENNESSEE',\n",
    "                  #'43':'TEXAS NO.'           \n",
    "                  '44':'UTAH','45':'VERMONT','46':'VIRGINIA','47':'WASHINGTON','48':'WEST VIRGINIA',\n",
    "                  '49':'WISCONSIN','50':'WYOMING','51':'HAWAII',              #'52':'NEW YORK HERIT'      \n",
    "                  #'53':'TEXAS SO.'           #'55':'CALIF. GR.'          \n",
    "                  '59':'FLORIDA',#'95':'FLORIDA',\n",
    "                  '94':'TEXAS','96':'CALIFORNIA','97':'NEW YORK'}\n",
    "                    #'75':'CALIF S COAST' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of pip/mpc/both \n",
    "pipCoverageMap = defaultdict(list)\n",
    "both_state_list = ['FLORIDA','MASSACHUSETTS','TEXAS','VIRGINIA']\n",
    "mpc_state_list = ['ALABAMA',\n",
    " 'ALASKA',\n",
    " 'ARIZONA',\n",
    " 'ARKANSAS',\n",
    " 'CALIFORNIA',\n",
    " 'COLORADO',\n",
    " 'CONNECTICUT',\n",
    " 'GEORGIA',\n",
    " 'IDAHO',\n",
    " 'ILLINOIS',\n",
    " 'INDIANA',\n",
    " 'IOWA',\n",
    " 'LOUISIANA',\n",
    " 'MAINE',\n",
    " 'MISSISSIPPI',\n",
    " 'MISSOURI',\n",
    " 'MONTANA',\n",
    " 'NEBRASKA',\n",
    " 'NEVADA',\n",
    " 'NEW HAMPSHIRE',\n",
    " 'NEW MEXICO',\n",
    " 'NORTH CAROLINA',\n",
    " 'OHIO',\n",
    " 'OKLAHOMA',\n",
    " 'RHODE ISLAND',\n",
    " 'SOUTH DAKOTA',\n",
    " 'TENNESSEE',\n",
    " 'VERMONT',\n",
    " 'WEST VIRGINIA',\n",
    " 'WISCONSIN',\n",
    " 'WYOMING']\n",
    "pip_state_list = [\n",
    " 'DELAWARE',\n",
    " 'DIST. OF COL.',\n",
    " 'HAWAII',\n",
    " 'KANSAS',\n",
    " 'KENTUCKY',\n",
    " 'MARYLAND',\n",
    " 'MICHIGAN',\n",
    " 'MINNESOTA',\n",
    " 'NEW JERSEY',\n",
    " 'NEW YORK',\n",
    " 'NORTH DAKOTA',\n",
    " 'OREGON',\n",
    " 'PENNSYLVANIA',\n",
    " 'SOUTH CAROLINA',\n",
    " 'UTAH',\n",
    " 'WASHINGTON']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read in the raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd '../../../projects/CENT/dataSets/2015-01/'\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# glob the files\n",
    "from glob import glob\n",
    "flist = glob('*.txt')\n",
    "len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## process the meta-data as column headers\n",
    "meta_line = []\n",
    "import re\n",
    "\n",
    "with open(flist[0]) as infile:\n",
    "    for line in infile:\n",
    "        if len(line)>1:\n",
    "            line=line.strip('\\n')#meta_line.append(line.strip('\\n'))\n",
    "            #print line.split()\n",
    "            #print line.strip('\\n').split(None,2)\n",
    "            tline = re.sub(\"     \", \"\\t\", line)\n",
    "            elements = tline.split('\\t')\n",
    "            elements = [x.strip() for x in elements]\n",
    "            meta_line.append(elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now process these meta_line values\n",
    "from collections import defaultdict\n",
    "meta_names = defaultdict(list)\n",
    "for row in meta_line[1:]:\n",
    "    if len(row)==1:\n",
    "        #print row\n",
    "        infile_names = row[0].split(' File')[:-1]\n",
    "        infile_names = [x.strip() for x in infile_names]\n",
    "        print infile_names\n",
    "    elif len(row)==3:\n",
    "        for j in xrange(0,3):\n",
    "            meta_names[infile_names[j]].append(row[j])\n",
    "    elif len(row)>3:\n",
    "        if len(row[0]):\n",
    "            meta_names[infile_names[0]].append(row[0])\n",
    "        meta_names[infile_names[2]].append(row[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in meta_names.iteritems():\n",
    "    print k,v, len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## so these are the headers for each of the next set of files\n",
    "input_df = []\n",
    "for f in flist[1:]:\n",
    "    # convert to dict_key\n",
    "    my_key = f[11:14]\n",
    "    if my_key == 'Los':\n",
    "        my_key = 'Loss'\n",
    "    print \"opening file {0} with {1} columns\".format(f, len(meta_names[my_key]))\n",
    "    ## now open that file using pandas\n",
    "    input_df.append(pd.read_csv(f,header=None,sep='~',names=meta_names[my_key]))\n",
    "    \n",
    "print [df.shape for df in input_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now clean up these dataframes\n",
    " #### begin by condensing the name values in the LINE & Coverage columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create a dictionary for each \n",
    "uniq_line_name_dict = {}\n",
    "uniq_coverage_name_dict = {}\n",
    "for adf in input_df:\n",
    "    uline_names = list(adf.LINE.unique())\n",
    "    if 'COVERAGE' in list(adf.columns):\n",
    "        ucoverage_names = list(adf.COVERAGE.unique())\n",
    "        for y in ucoverage_names:\n",
    "            uniq_coverage_name_dict[y] = y.strip()\n",
    "    \n",
    "    for x in uline_names:\n",
    "        uniq_line_name_dict[x]= x.strip()\n",
    "    \n",
    "        \n",
    "print uniq_line_name_dict\n",
    "print uniq_coverage_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use these dictionaries to replace the values in each dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(0,len(input_df)):\n",
    "    input_df[i].replace(to_replace=uniq_line_name_dict,inplace=True)\n",
    "    if 'COVERAGE' in list(input_df[i].columns):\n",
    "        input_df[i].replace(to_replace=uniq_coverage_name_dict,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print input_df[1].COVERAGE.unique(), input_df[1].LINE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next combine YEAR+MONTH to datetime (beginning of month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create new column 'date' in these cases\n",
    "for i in xrange(0,len(input_df)):\n",
    "    input_df[i]['date'] =input_df[i][['MONTH','YEAR']].apply(lambda x: pd.to_datetime(\"-\".join(map(str,x)),format='%m-%Y'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printall(input_df[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process the losses file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate out the aggregated values for Company wide (CW), zones (ZN), and special states \n",
    "\n",
    "* California --> total is 96; drop 05,55,75\n",
    "* Texas --> total is 94; drop 43,53\n",
    "* New York --> toal is 97; drop 32, 52\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first separate out the aggregated values for Company wide (CW) and zones (ZN)\n",
    "input_df[0].STATE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cif_df = input_df[0][~input_df[0].STATE.isin(['CW','ZN'])].copy()\n",
    "loss_df = input_df[1][~input_df[1].STATE.isin(['CW','ZN'])].copy()\n",
    "pif_df = input_df[2][~input_df[2].STATE.isin(['CW','ZN'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## drop TX: 43,53; CA: 05,55,75; NY: 32,52\n",
    "substate_codes_to_drop = ['43','53','05','55','75','32','52']\n",
    "cif_df = cif_df[~cif_df.STATE.isin(substate_codes_to_drop)].copy()\n",
    "loss_df = loss_df[~loss_df.STATE.isin(substate_codes_to_drop)].copy()\n",
    "pif_df = pif_df[~pif_df.STATE.isin(substate_codes_to_drop)].copy()\n",
    "print len(cif_df),len(pif_df),len(loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### drop MONTH & YEAR columns\n",
    "for i in xrange(0,len(input_df)):\n",
    "    input_df[i].drop(['YEAR','MONTH'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside dealing with CIF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cif_df[cif_df.columns[5:-1]].sum(axis=1) - cif_df.ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cif_df.groupby('STATE')[['MPC','PIP']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(cif_df[cif_df.columns[5:-1]].sum(axis=1), cif_df.ALL)\n",
    "x = np.linspace(0,20000000)\n",
    "y = x\n",
    "plt.plot(x,y,color='red')\n",
    "plt.xlabel('sum CIF_columns')\n",
    "plt.ylabel('ALL for CIF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_df.LINE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(loss_df[((loss_df.STATE=='05') |(loss_df.STATE=='55') | (loss_df.STATE=='75')) & (loss_df.LINE =='TOTVOL') & (loss_df.YEAR ==2007) & (loss_df.MONTH == 1)]).sort('COVERAGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# okay so at least for TX '43'+'53' yields '94'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_df[(loss_df.STATE=='96') & (loss_df.LINE =='TOTVOL ') & (loss_df.YEAR ==2007) & (loss_df.MONTH == 1)].sort('COVERAGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(loss_df[((loss_df.STATE=='05') |(loss_df.STATE=='55') | (loss_df.STATE=='75')) & (loss_df.LINE =='TOTVOL ') & (loss_df.YEAR ==2007) & (loss_df.MONTH == 1)]).groupby('COVERAGE').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_df.COVERAGE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_df[(loss_df.STATE=='01') & (loss_df.YEAR==2009)&(loss_df.MONTH<6) & (loss_df.COVERAGE=='BI ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_df.COVERAGE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep only the TOTAL VOLUNTARY AUTO POLICIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "volloss= loss_df[loss_df.LINE=='TOTVOL'].copy()\n",
    "volloss.drop(['ZONE','LINE'],inplace=True,axis=1)#.sort('COVERAGE',inplace=True)\n",
    "# & (loss_df.COVERAGE==')][alabama_loss_temp.columns[3:]]\n",
    "volloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.date_range('01-01-2007','02-01-2015',freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "volloss.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace state codes with abbreviations/names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "volloss.replace(to_replace={'STATE':stateNamesDict},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "volloss.sort(['STATE','COVERAGE','date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saveRaw = False\n",
    "if saveRaw = True:\n",
    "    volloss.to_csv('totalvol_loss_raw.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Correct\" for cases where certain coverages are not offered\n",
    "Logic is to zero out the values if PIP or MPC is not available in that state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def zero_missing_coverage(df,coverage,states_list,\n",
    "                          cols_to_zero= ['REPORTED_CNT','PAID_CNT','PENDING_CNT','CWP','OIE_CNT','SUIT_CNT','PD_AMT','ALAE']):\n",
    "    zero_index = df[(df.COVERAGE==coverage)&df.STATE.isin(states_list)].index\n",
    "    df.loc[zero_index,cols_to_zero]=0\n",
    "    print len(zero_index)\n",
    "    return df\n",
    "#idx=3\n",
    "#mtrc='REPORTED_CNT'\n",
    "#sub_df = volloss[(volloss.COVERAGE==cvr) & (volloss.STATE == mpc_state_list[idx])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv = zero_missing_coverage(volloss, 'MPC',pip_state_list)\n",
    "vv = zero_missing_coverage(vv,'PIP',mpc_state_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop those called COVERAGE == 'ALL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv = vv[vv.COVERAGE!='ALL'].copy()\n",
    "vv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns not previously used:\n",
    "* ALAE\n",
    "* SUIT_CNT\n",
    "* OIE_CNT\n",
    "* CWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_drop =  ['CWP','OIE_CNT','SUIT_CNT','ALAE','YEAR','MONTH']\n",
    "col_to_rename ={'REPORTED_CNT':'Reported Count','PAID_CNT':'Paid Count','PENDING_CNT':'Pending Count','PD_AMT':'Indemnity'}#,'date':'YEAR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv.drop(cols_to_drop,axis=1, inplace=True)\n",
    "vv.rename(columns=col_to_rename,inplace=True)\n",
    "print vv.columns\n",
    "print vv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate with Derived/Calculated Fields:\n",
    "\n",
    "2. Injury = BI+UBI+WBI\n",
    "3. PIP/MPC = PIP+MPC\n",
    "4. Property = PD+COMP+COLL\n",
    "1. Severity = Indemnity (PD_AMT) / PAID_CNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the combined coverages\n",
    "combo_coverages = ['Injury','Property','PIP/MPC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_summed_coverages(df,coverage_list,coverage_name):\n",
    "    new_df = df[df.COVERAGE.isin(coverage_list)].groupby(('STATE','date'),as_index=False).sum()\n",
    "    new_df['COVERAGE']=coverage_name\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "injury_df = return_summed_coverages(vv,['BI','UBI','WBI'],'Injury')\n",
    "property_df = return_summed_coverages(vv,['PD','COLL','COMP'],'Property')\n",
    "pipmpc_df = return_summed_coverages(vv,['PIP','MPC'],'PIP/MPC')\n",
    "vv = pd.concat([vv,injury_df,property_df,pipmpc_df])\n",
    "#vv.append(adf)\n",
    "print vv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv.COVERAGE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Severity (and replace inf/-inf with zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv['Severity']=vv['Indemnity']/vv['Paid Count']#volloss['PD_AMT']/volloss['PAID_CNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sum(vv[vv.Severity==np.inf])\n",
    "vv.replace([np.inf, -np.inf], 0.0,inplace=True)\n",
    "vv.Severity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reorder the columns to match previous dataset order\n",
    "vcols = vv.columns\n",
    "print vcols\n",
    "mod_order = [vcols[-3],vcols[-2],vcols[0],vcols[4],vcols[2],vcols[3],vcols[1],vcols[-1]]\n",
    "print mod_order\n",
    "vv=vv[mod_order]\n",
    "vv.rename(columns={'date':'YEAR'},inplace=True)\n",
    "vv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort this by state/coverage/year\n",
    "vv.sort(['STATE','COVERAGE','YEAR'],inplace=True)\n",
    "# relabel the indices\n",
    "vv.index=xrange(0,len(vv))\n",
    "vv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Save the file\n",
    "saveDerived = True\n",
    "if saveDerived:\n",
    "    vv.to_csv('base_CENT_01_2015.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv[(vv.STATE=='ALABAMA') & (vv.YEAR>'12-31-2008')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look to see if Indemnity & Severity are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv[vv.columns[2:]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv[vv.columns[2:]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv.YEAR.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
