{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for Hadoop Streaming\n",
    "In this version we create the code that can be run in a stand-alone mode rather than using ipython shortcuts.\n",
    "\n",
    "### Components of the code\n",
    "1. program to pass python code to a hadoop_streaming instance (hadoop_streaming2())\n",
    "2. program to preprocess data\n",
    "3. program to post-process results\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd ../../work/claiment/CENT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load hadoop_streamer.py\n",
    "# program to pass python code to hadoop streaming\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def create_hdfs_directory(usecase):\n",
    "    \"\"\"\n",
    "    Function to check on the existence of the related hdfs namespace for this project\n",
    "    :rtype : string\n",
    "    \"\"\"\n",
    "    from uuid import uuid4\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    # check if the usecase directory exists\n",
    "    hdfs_prepend = '/data/discovery/'\n",
    "    hdfs_path = hdfs_prepend+usecase\n",
    "\n",
    "    if subprocess.call(['hadoop','fs','-ls',hdfs_path]): # directory doesn't exist.\n",
    "        # place in the users directory space\n",
    "        uname = os.getenv('USER', '')\n",
    "        random_folder = str(uuid4())\n",
    "        # build it in the users space\n",
    "        hdfs_path = '/user/' + uname + '/' + random_folder\n",
    "        if subprocess.call(['hadoop','fs','-ls','/user/'+uname]): # if directory doesn't exist.\n",
    "            hdfs_path = '/tmp/' + uname + '/' + random_folder # assign to the tmpdirectory\n",
    "\n",
    "    print usecase,hdfs_path\n",
    "    return hdfs_path\n",
    "\n",
    "def hadoop_streamer(mapper, reducer, input_file, outpath, files=None, options=''):\n",
    "    \"\"\"\n",
    "    :param mapper: the name of the python file to use as a mapper\n",
    "    :param reducer: the name of the python file to use as a reducer\n",
    "    :param input_file: the name of the input data file (directory?)\n",
    "    :param outpath: HDFS location where data is to be stored.\n",
    "    :param files: additional files to add to the hadoop cmd line call for python streaming\n",
    "    :param options: additional java options to pass to the mapreduce job\n",
    "    :return: the result of concatenating the files produced by the hadoop streaming job\n",
    "    \"\"\"\n",
    "    #\n",
    "    # TODO: define jar as a variable\n",
    "    jar = '/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar'\n",
    "    # attach necessary mapper & reducer files (python) to the streaming file path.\n",
    "    files.append(mapper)\n",
    "    files.append(reducer)\n",
    "    files = ','.join(files)\n",
    "\n",
    "    # create a tmp directory in HDFS to store results\n",
    "    tcmd = ['hadoop', 'fs', '-mkdir']\n",
    "    tcmd.append(outpath+'/tmp')\n",
    "    #to do check if it exists\n",
    "    subprocess.call(tcmd)\n",
    "\n",
    "    \"\"\"# check if input file is in hdfs already\n",
    "    file_not_in_hdfs = subprocess.call(['hadoop', 'fs', '-ls', bdir + input_file])\n",
    "    #print \"check if the file {0} is in hdfs {1}:\".format(input_file,file_not_in_hdfs)\n",
    "    ## output of the above system call is 0 if file exists, 1 otherwise\n",
    "    if file_not_in_hdfs:  #copy from input loation (local FS) if not there.\n",
    "        subprocess.call(['hadoop', 'fs', '-put', 'input_file', 'base_hdfs_dir'])\n",
    "\n",
    "    #file_not_in_hdfs = subprocess.call(['hadoop','fs','-ls',input_file])\n",
    "    #print \"check if the file {0} is in hdfs {1}:\".format(input_file,file_not_in_hdfs)\n",
    "    ## output of the above system call is 0 if file exists, 1 otherwise\n",
    "    #if file_not_in_hdfs: #copy from input loation (local FS) if not there.\n",
    "    #    subprocess.call(['hadoop', 'fs', '-put', 'input_file', 'baseHDFSdir'])\n",
    "\n",
    "    # append the staging and derived directories\n",
    "    list_file_path_names = ['/staging/','/derived/']\n",
    "\n",
    "    \"\"\"\n",
    "    # run the streaming job\n",
    "\n",
    "    ofile = outpath+'/tmp'+ '/output'\n",
    "    print input_file, ofile\n",
    "    #streaming commandline\n",
    "    stcmd = ['hadoop', 'jar', jar, '-files', files, options, '-mapper', mapper,\n",
    "             '-reducer', reducer, '-input', input_file,\n",
    "             '-output', ofile]\n",
    "    subprocess.call(stcmd)\n",
    "    # To Do create location to store the output to a log?\n",
    "\n",
    "    ocmd = ['hadoop', 'fs', '-cat']\n",
    "    ocmd.append(ofile + '/part*')\n",
    "    #output = subprocess.check_output(ocmd)#['hadoop','fs'#!hadoop fs -cat /tmp/{random_folder}/output/part*\n",
    "    output = subprocess.check_output(ocmd)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# program to pass python code to hadoop streaming\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def create_hdfs_directory(usecase):\n",
    "    \"\"\"\n",
    "    Function to check on the existence of the related hdfs namespace for this project\n",
    "    :rtype : string\n",
    "    \"\"\"\n",
    "    from uuid import uuid4\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    # check if the usecase directory exists\n",
    "    hdfs_prepend = '/data/discovery/'\n",
    "    hdfs_path = hdfs_prepend+usecase\n",
    "\n",
    "    if subprocess.call(['hadoop','fs','-ls',hdfs_path]): # directory doesn't exist.\n",
    "        # place in the users directory space\n",
    "        uname = os.getenv('USER', '')\n",
    "        random_folder = str(uuid4())\n",
    "        # build it in the users space\n",
    "        hdfs_path = '/user/' + uname + '/' + random_folder\n",
    "        if subprocess.call(['hadoop','fs','-ls','/user/'+uname]): # if directory doesn't exist.\n",
    "            hdfs_path = '/tmp/' + uname + '/' + random_folder # assign to the tmpdirectory\n",
    "\n",
    "    print usecase,hdfs_path\n",
    "    return hdfs_path\n",
    "\n",
    "def hadoop_streamer(mapper, reducer, input_file, outpath, files=None, options=''):\n",
    "    \"\"\"\n",
    "    :param mapper: the name of the python file to use as a mapper\n",
    "    :param reducer: the name of the python file to use as a reducer\n",
    "    :param input_file: the name of the input data file (directory?)\n",
    "    :param outpath: HDFS location where data is to be stored.\n",
    "    :param files: additional files to add to the hadoop cmd line call for python streaming\n",
    "    :param options: additional java options to pass to the mapreduce job\n",
    "    :return: the result of concatenating the files produced by the hadoop streaming job\n",
    "    \"\"\"\n",
    "    #\n",
    "    # TODO: define jar as a variable\n",
    "    jar = '/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar'\n",
    "    # attach necessary mapper & reducer files (python) to the streaming file path.\n",
    "    files.append(mapper)\n",
    "    files.append(reducer)\n",
    "    files = ','.join(files)\n",
    "\n",
    "    # create a tmp directory in HDFS to store results\n",
    "    tcmd = ['hadoop', 'fs', '-mkdir']\n",
    "    tcmd.append(outpath+'/tmp')\n",
    "    #to do check if it exists\n",
    "    subprocess.call(tcmd)\n",
    "\n",
    "    \"\"\"# check if input file is in hdfs already\n",
    "    file_not_in_hdfs = subprocess.call(['hadoop', 'fs', '-ls', bdir + input_file])\n",
    "    #print \"check if the file {0} is in hdfs {1}:\".format(input_file,file_not_in_hdfs)\n",
    "    ## output of the above system call is 0 if file exists, 1 otherwise\n",
    "    if file_not_in_hdfs:  #copy from input loation (local FS) if not there.\n",
    "        subprocess.call(['hadoop', 'fs', '-put', 'input_file', 'base_hdfs_dir'])\n",
    "\n",
    "    #file_not_in_hdfs = subprocess.call(['hadoop','fs','-ls',input_file])\n",
    "    #print \"check if the file {0} is in hdfs {1}:\".format(input_file,file_not_in_hdfs)\n",
    "    ## output of the above system call is 0 if file exists, 1 otherwise\n",
    "    #if file_not_in_hdfs: #copy from input loation (local FS) if not there.\n",
    "    #    subprocess.call(['hadoop', 'fs', '-put', 'input_file', 'baseHDFSdir'])\n",
    "\n",
    "    # append the staging and derived directories\n",
    "    list_file_path_names = ['/staging/','/derived/']\n",
    "\n",
    "    \"\"\"\n",
    "    # run the streaming job\n",
    "\n",
    "    ofile = outpath+'/tmp'+ '/output'\n",
    "    print input_file, ofile\n",
    "    #streaming commandline\n",
    "    stcmd = ['hadoop', 'jar', jar, '-files', files, options, '-mapper', mapper,\n",
    "             '-reducer', reducer, '-input', input_file,\n",
    "             '-output', ofile]\n",
    "    subprocess.call(stcmd)\n",
    "    # To Do create location to store the output to a log?\n",
    "\n",
    "    ocmd = ['hadoop', 'fs', '-cat']\n",
    "    ocmd.append(ofile + '/part*')\n",
    "    #output = subprocess.check_output(ocmd)#['hadoop','fs'#!hadoop fs -cat /tmp/{random_folder}/output/part*\n",
    "    output = subprocess.check_output(ocmd)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tpath = '/data/discovery/claiment/staging/v2'\n",
    "transformed_file = '/data/discovery/claiment/staging/base2_CENT_02_2015.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_infile = '/data/discovery/claiment/staging/base_CENT_01_2015.csv'\n",
    "transformed_file = '/data/discovery/claiment/staging/base_CENT_01_2015.json'\n",
    "tpath = '/data/discovery/claiment/staging/u1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = hadoop_streamer(mapper='identity.py',\n",
    "                      reducer ='build_models.py',\n",
    "                      input_file =transformed_file,\n",
    "                      outpath=tpath,\n",
    "                      files=['mltimeseries.py',\n",
    "                                 '/usr/lib/vmware-tools/lib/libXrender.so.1/libXrender.so.1',\n",
    "                                 # Required for matplotlib.pyplot\n",
    "                                 '/usr/lib/vmware-tools/lib/libXau.so.6/libXau.so.6'],  # Required for matplotlib.pyplot\n",
    "                       options='-D mapreduce.job.reduces=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try just reading the transformed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed_file = '/data/discovery/claiment/staging/base_CENT_01_2015.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmpdata = !hadoop fs -cat {transformed_file} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idmapping = []\n",
    "for line in tmpdata:\n",
    "    idmapping.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(idmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from mltimeseries import time_series_to_cross_section,optimized_rf,model_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Program to pass python code to hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# program to pass python code to hadoop streaming\n",
    "def hadoop_streaming2(mapper, reducer, input_file, files=[], options='',baseHDFSdir=None):\n",
    "    \"\"\"\n",
    "    :param mapper: the name of the python file to use as a mapper\n",
    "    :param reducer: the name of the python file to use as a reducer\n",
    "    :param input_file: the name of the input data file (directory?)\n",
    "    :param files: additional files to add to the hadoop cmd line call for python streaming\n",
    "    :param options: additional java options to pass to the mapreduce job\n",
    "    :param baseHDFSdir: HDFS location where data is to be stored.\n",
    "    :return: the result of concatenating the files produced by the hadoop streaming job\n",
    "    \"\"\"\n",
    "    random_folder = str(uuid4())\n",
    "    # define the basedir if default value not set\n",
    "    if baseHDFSdir==None:\n",
    "        #print baseHDFSdir\n",
    "        uname = os.getenv('USER','')\n",
    "        #print uname\n",
    "        if len(uname)!=4:\n",
    "            baseHDFSdir='/tmp/'+random_folder\n",
    "        else:\n",
    "            baseHDFSdir='/user/'+uname+'/'+random_folder\n",
    "        bdir = baseHDFSdir[:baseHDFSdir.find(random_folder)] #shortened version\n",
    "    elif baseHDFSdir[-1]=='/': # make sure it doesn't end with '/'\n",
    "        baseHDFSdir = baseHDFSdir[:-1]\n",
    "        bdir = baseHDFSdir[:baseHDFSdir.rfind('/')]\n",
    "        bdir+='/'\n",
    "        \n",
    "     \n",
    "    #print baseHDFSdir\n",
    "    print bdir, baseHDFSdir \n",
    "    \n",
    "    #\n",
    "    # TODO: define jar as a variable\n",
    "    jar = '/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar'\n",
    "    # attach necessary mapper & reducer files (python) to the streaming file path.\n",
    "    files.append(mapper)\n",
    "    files.append(reducer)\n",
    "    files = ','.join(files)\n",
    "    \n",
    "    # create a tmp directory in HDFS to store results\n",
    "    tcmd =['hadoop','fs','-mkdir']\n",
    "    tcmd.append(baseHDFSdir)\n",
    "    #to do check if it exists\n",
    "    subprocess.call(tcmd)\n",
    "    \n",
    "    # check if input file is in hdfs already\n",
    "    file_not_in_hdfs = subprocess.call(['hadoop','fs','-ls',bdir+input_file])\n",
    "    #print \"check if the file {0} is in hdfs {1}:\".format(input_file,file_not_in_hdfs)\n",
    "    ## output of the above system call is 0 if file exists, 1 otherwise\n",
    "    if file_not_in_hdfs: #copy from input loation (local FS) if not there.\n",
    "        subprocess.call(['hadoop', 'fs', '-put', 'input_file', 'baseHDFSdir'])\n",
    "    \n",
    "    #file_not_in_hdfs = subprocess.call(['hadoop','fs','-ls',input_file])\n",
    "    #print \"check if the file {0} is in hdfs {1}:\".format(input_file,file_not_in_hdfs)\n",
    "    ## output of the above system call is 0 if file exists, 1 otherwise\n",
    "    #if file_not_in_hdfs: #copy from input loation (local FS) if not there.\n",
    "    #    subprocess.call(['hadoop', 'fs', '-put', 'input_file', 'baseHDFSdir'])\n",
    "    \n",
    "    \n",
    "    # run the streaming job\n",
    "    infile=bdir[:-1]+'/'+input_file\n",
    "    ofile =baseHDFSdir+'/output'\n",
    "    print infile,ofile\n",
    "    #streaming commandline\n",
    "    stcmd = ['hadoop','jar',jar,'-files',files,options,'-mapper',mapper,'-reducer',reducer,'-input',infile,'-output',ofile]\n",
    "    subprocess.call(stcmd)\n",
    "    # To Do create location to store the output to a log?\n",
    "    \n",
    "    \n",
    "    ocmd = ['hadoop','fs','-cat']\n",
    "    ocmd.append(ofile+'/part*')\n",
    "    #output = subprocess.check_output(ocmd)#['hadoop','fs'#!hadoop fs -cat /tmp/{random_folder}/output/part*\n",
    "    output=subprocess.check_output(ocmd)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fund out where the source data is\n",
    "srcdata_dir = '/data/discovery/claiment/staging'\n",
    "!hadoop fs -ls {srcdata_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper\n",
    "Identity mapper -- just passes data along to the reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file identity.py\n",
    "#!/opt/anaconda/latest/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file build_models.py\n",
    "#!/opt/anaconda/latest/bin/python\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from mltimeseries import time_series_to_cross_section, optimized_rf\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split('\\t')\n",
    "    state, coverage = json.loads(key)\n",
    "    focused_data = pd.read_json(value)\n",
    "\n",
    "    variables_of_interest = [\"Reported Count\", \"Paid Count\", \"Pending Count\", \"Indemnity\", \"Severity\", \"Overall\"]\n",
    "    \n",
    "    X, y_data, x, undiff = time_series_to_cross_section(focused_data[variables_of_interest],\n",
    "                                                        forecast_horizon=1,\n",
    "                                                        max_fair_lags=2,\n",
    "                                                        seasonal_factor=12)\n",
    "    for variable in variables_of_interest:\n",
    "\n",
    "        # Account for time series with no variation\n",
    "        state_actuals = focused_data[variable]\n",
    "        std_of_actuals = state_actuals.std(ddof=1)\n",
    "        if std_of_actuals == 0:\n",
    "            output = {'state': state,\n",
    "                      'variable': variable,\n",
    "                      'coverage': coverage,\n",
    "                      'overview_data': [0]*len(state_actuals),\n",
    "                      'detail_data': {\"stddev\": 0,\n",
    "                                      \"actual\": list(state_actuals),\n",
    "                                      \"predicted\": list(state_actuals)},\n",
    "                      'abs_average_error': 0,\n",
    "                      'MSE': 0}\n",
    "            print json.dumps(output)\n",
    "            break\n",
    "\n",
    "        # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "        final_model, variables_in_model = optimized_rf(X, y_data[variable],\n",
    "                                                       variable_importance_n_estimators=120,\n",
    "                                                       n_estimators_in_grid_search=50,\n",
    "                                                       number_of_important_variables_to_use_options=[6,8,10,12,15,20],#[6],#, 8, 10, 12, 15, 20],\n",
    "                                                       variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                       n_estimators_to_retrain_best_model=200,\n",
    "                                                       verbose=False, n_random_models_to_test=6,\n",
    "                                                       charts=False, n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "        state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "\n",
    "        state_residuals = focused_data[variable] - state_predictions\n",
    "        data_consumed_for_model = sum(state_residuals == 0)\n",
    "        std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "        state_std_residuals = state_residuals/std_of_residuals\n",
    "        abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "        MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()         \n",
    "\n",
    "        output = {'state': state,\n",
    "                  'variable': variable,\n",
    "                  'coverage': coverage,\n",
    "                  'overview_data': list(state_std_residuals.values),\n",
    "                  'detail_data': {\"stddev\": std_of_residuals,\n",
    "                                  \"actual\": list(state_actuals),\n",
    "                                  \"predicted\": list(state_predictions.round(0).astype(int))},\n",
    "                  'abs_average_error': abs_average_error,\n",
    "                  'MSE': MSE}\n",
    "\n",
    "        print json.dumps(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running it\n",
    "Item 2: preprocess the input data (i.e. transform it into a key-value pair format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function(s)\n",
    "def sernorm(series):\n",
    "    \"\"\" a function that calculates the norm of a selected pandas Series\n",
    "    returns the normalized series values\n",
    "    \"\"\"\n",
    "    return (series - series.mean())/series.std(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infiles = []\n",
    "infiles.append(\"/opt/edge/kesj/data/claims-early-notification-tool/base_data_CENT.csv\")\n",
    "infiles.append('/opt/edge/kesj/data/claims-early-notification-tool/second_data_CENT.csv')\n",
    "print infiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infiles = '/data/discovery/claiment/staging/base_CENT_01_2015.csv' # in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the pre-processing steps:\n",
    "1. first write a function to transform the cent data specifically into 4D panel\n",
    "2. write a generic function that transforms it to a key-value pair and outputs this in some format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_cent_file_to_Panel4D(inputfile,x='STATE',y='COVERAGE',t='YEAR',\n",
    "                         important_variables=[],omit_column=[],create_overall_variable=True):\n",
    "    \"\"\"\n",
    "    A function that takes an input csv file of many timeseries data and processes it into the \n",
    "    format that we want for CENT analysis.\n",
    "    \n",
    "    return a pandas Panel4D object from the input data\n",
    "    the input parameters:\n",
    "    data -- the flat 2d datashape\n",
    "    important_variables -- the column name to use for the 'items' -- default is 'STATE'\n",
    "    y -- the column name to use for the 'minor-axis' -- default is 'COVERAGE'\n",
    "    t -- the oclumn name to use for the 'major-axis' <- where timeseres go; default is 'YEAR'\n",
    "    z -- the column title to use for 'labels' (these are the columns you can loop over)\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    data = pd.read_csv(inputfile)\n",
    "    \n",
    "    \n",
    "    if len(omit_column)!=0: # omit a list of columns if so given\n",
    "        data=data.drop(omit_column,axis=1,inplace=True)\n",
    "    \n",
    "    print \"input dataframe has the dimensions of {0}\".format(np.shape(data))\n",
    "    panelDict = {} # dictionary of panel information\n",
    "    print \"the column titles are: {0}\".format(data.columns)\n",
    "    # check that x,y,t,variables_of_interest are in columns\n",
    "    # create a set for the singletons:\n",
    "    s1 = set([x,y,t])\n",
    "    \n",
    "    # create a set for all columns\n",
    "    scolumns = set(data.columns.values)\n",
    "    # test that s1 is subset of scolumns\n",
    "    sdiff1 = s1-scolumns\n",
    "    if len(sdiff1)!=0:\n",
    "        # have to fix the s1 input files\n",
    "        print \"We have a problem because the columns you want to pivot on are not present\"\n",
    "        print sdiff1, \" is missing from \", scolumns\n",
    "        return 0\n",
    "    else:\n",
    "        # convert the year to a date-time object\n",
    "        data[t]=pd.to_datetime(data[t])\n",
    "    \n",
    "    if len(important_variables)==0: # define these based upon the input data if not defined\n",
    "        important_variables = list(scolumns-s1)\n",
    "        print \"important variables are determined by the input data.\"\n",
    "        #print \"{0} are the important variables determined based upon the input data\".format(important_variables)\n",
    "    \n",
    "        \n",
    "     # option to generate an overall variable that is the normalized average of the other variables of interest\n",
    "    if create_overall_variable:\n",
    "        data['Overall']= data.groupby([x, y]).transform(sernorm)[important_variables].sum(axis=1)\n",
    "        important_variables += ['Overall']\n",
    "    \n",
    "    # generate list of unique values for the 3 input parameters: x,y,t\n",
    "    print x,y,t\n",
    "    \n",
    "    xvals =data[x].unique()\n",
    "    yvals=data[y].unique()\n",
    "    tvals=data[t].unique()\n",
    "    \n",
    "    print len(xvals),len(yvals),len(tvals),len(important_variables)\n",
    "    # to do: check that all columns are used\n",
    "    # to do: reshape the 2d data frame into a multi-index, hierarchical df\n",
    "    midata = data.set_index([x,y,t])\n",
    "    \n",
    "    for my_variable in important_variables:\n",
    "        print \"cycling through the column values, {0}\".format(my_variable)\n",
    "        myPanel = midata[my_variable].unstack(y).to_panel()\n",
    "        panelDict[my_variable] = myPanel\n",
    "\n",
    "    print \"there are {0} elements in the panelDictionary\".format(len(panelDict))\n",
    "\n",
    "    p4d=pd.Panel4D(panelDict)\n",
    "    # I want to output it so that the timeseries is the major axix\n",
    "    p4d = p4d.swapaxes('minor','major')\n",
    "    # I want to output it so that the state -- minor, and coverage (items) are the keys for a dataframe value\n",
    "    p4d = p4d.swapaxes('labels','minor')\n",
    "    print np.shape(p4d)\n",
    "    \n",
    "    \n",
    "    return p4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -get {infiles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_cent_file(inputfile,x='STATE',y='COVERAGE',t='YEAR',\n",
    "                         important_variables=[],omit_column=[],hdfs_flag=False,create_overall_variable=True):\n",
    "    \"\"\"\n",
    "    A function that takes an input csv file of many timeseries data and processes it into the \n",
    "    format that we want for CENT analysis.\n",
    "    \n",
    "    hdfs_flag indicates if the based file is in hdfs or not.\n",
    "    return a pandas dataframe from the input data\n",
    "    the input parameters:\n",
    "    data -- the flat 2d datashape\n",
    "    important_variables -- the column name to use for the 'items' -- default is 'STATE'\n",
    "    y -- the column name to use for the 'minor-axis' -- default is 'COVERAGE'\n",
    "    t -- the oclumn name to use for the 'major-axis' <- where timeseres go; default is 'YEAR'\n",
    "    z -- the column title to use for 'labels' (these are the columns you can loop over)\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #if hdfs_flag:\n",
    "        \n",
    "    data = pd.read_csv(inputfile)\n",
    "    \n",
    "    #check format of a couple input formats. we want important_variables, omit_column to be lists\n",
    "    \n",
    "    \n",
    "    if len(omit_column)!=0: # omit a list of columns if so given\n",
    "        data=data.drop(omit_column,axis=1,inplace=True)\n",
    "    \n",
    "    print \"input dataframe has the dimensions of {0}\".format(np.shape(data))\n",
    "    panelDict = {} # dictionary of panel information\n",
    "    print \"the column titles are: {0}\".format(data.columns)\n",
    "    # check that x,y,t,variables_of_interest are in columns\n",
    "    # create a set for the singletons:\n",
    "    s1 = set([x,y,t])\n",
    "    \n",
    "    # create a set for all columns\n",
    "    scolumns = set(data.columns.values)\n",
    "    # test that s1 is subset of scolumns\n",
    "    sdiff1 = s1-scolumns\n",
    "    if len(sdiff1)!=0:\n",
    "        # have to fix the s1 input files\n",
    "        print \"We have a problem because the columns you want to pivot on are not present\"\n",
    "        print sdiff1, \" is missing from \", scolumns\n",
    "        return 0\n",
    "    else:\n",
    "        # convert the year to a date-time object\n",
    "        data[t]=pd.to_datetime(data[t])\n",
    "    \n",
    "    if len(important_variables)==0: # define these based upon the input data if not defined\n",
    "        important_variables = list(scolumns-s1)\n",
    "        print \"important variables are determined by the input data.\"\n",
    "        #print \"{0} are the important variables determined based upon the input data\".format(important_variables)\n",
    "    \n",
    "        \n",
    "     # option to generate an overall variable that is the normalized average of the other variables of interest\n",
    "    if create_overall_variable:\n",
    "        data['Overall']= data.groupby([x, y]).transform(sernorm)[important_variables].sum(axis=1)\n",
    "        important_variables += ['Overall']\n",
    "    \n",
    "    # generate list of unique values for the 3 input parameters: x,y,t\n",
    "    print s1\n",
    "    \n",
    "    xvals =data[x].unique()\n",
    "    yvals=data[y].unique()\n",
    "    tvals=data[t].unique()\n",
    "    \n",
    "    print len(xvals),len(yvals),len(tvals),len(important_variables)\n",
    "    # to do: check that all columns are used\n",
    "    # to do: reshape the 2d data frame into a multi-index, hierarchical df\n",
    "    midata = data.groupby((x,y))#set_index([x,y,t]\n",
    "    return (midata,important_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1,iv_1 = preprocess_cent_file('base_CENT_01_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1.COVERAGE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_variables=['Reported Count','Paid Count','Pending Count','Indemnity','Severity']\n",
    "dd['Overall']= dd.groupby(['STATE', 'COVERAGE']).transform(sernorm)[important_variables].sum(axis=1)\n",
    "important_variables += ['Overall']\n",
    "    \n",
    "dd.index=dd.YEAR\n",
    "dd.drop('YEAR',axis=1,inplace=True)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tdict = {}\n",
    "with open('mydict.out','w') as f:\n",
    "    for k,v in gdf:\n",
    "        key =str(list(k))\n",
    "        value = v.drop(['STATE','COVERAGE'],axis=1).to_dict()\n",
    "        #tdict[key] = value\n",
    "        line = key + '\\t' + str(value)#.to_csv()\n",
    "        f.write(line+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now write these as kv pairs\n",
    "print len(da1.groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "da1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in da1:#.groups.keys():\n",
    "    #print str(list(k)),len(da1.get_group)\n",
    "    print k,len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(da1.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_tokey_value_pairs(gdf,voi,ofile,ofmt='json'):\n",
    "    \"\"\"\n",
    "    A function that takes an input csv file of many timeseries data and pivots it into a key-value-pair format.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    print np.shape(gdf)\n",
    "    print len(voi), ofile\n",
    "#   \n",
    "    dfile = ofile+'.'+ofmt\n",
    "    if ofmt=='json':        \n",
    "        with open(dfile, 'w') as f:\n",
    "            for k in gdf.groups.keys():\n",
    "                key = list(k)\n",
    "                value =gdf.get_group(k)\n",
    "                line = json.dumps(key) + '\\t' + value.to_json()\n",
    "                f.write(line + '\\n')\n",
    "    # warning following option not really working yet\n",
    "    elif ofmt == 'tsv':\n",
    "        with open(dfile,'w') as f:\n",
    "            for k in gdf.groups.keys():\n",
    "                key = str(list(k))\n",
    "                value =gdf.get_group(k)\n",
    "                line = key + '\\t' + value.to_csv()\n",
    "                f.write(line+'\\n')\n",
    "                        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform_tokey_value_pairs(df_1,iv_1,'data_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srcdata_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -put data_15.json /data/discovery/claiment/staging/data_15.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 now run the application in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the Hadoop Streaming MapReduce job\n",
    "out = hadoop_streaming2(mapper='identity.py',\n",
    "                       reducer='build_models.py',\n",
    "                       input_file='data_15.json',\n",
    "                       files=['mltimeseries.py',\n",
    "                              '/usr/lib/vmware-tools/lib/libXrender.so.1/libXrender.so.1', # Required for matplotlib.pyplot\n",
    "                              '/usr/lib/vmware-tools/lib/libXau.so.6/libXau.so.6'],        # Required for matplotlib.pyplot\n",
    "                       options='-D mapreduce.job.reduces=100',\n",
    "                       baseHDFSdir=srcdata_dir+'/run1/'\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k1 = da1.groups.keys()[0]\n",
    "s1 = str(list(k1))\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my4['ALABAMA','BI'].to_csv('t1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# In the dataset, every state has 11 coverage types. There are 5 main\n",
    "# metrics for each coverage type. There are 69 months of data associated\n",
    "# with each metric.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def norm(series):\n",
    "    return (series - series.mean())/series.std(ddof=1)\n",
    "\n",
    "data = pd.read_csv(\"/opt/edge/kesj/data/claims-early-notification-tool/base_data_CENT.csv\")\n",
    "variables_of_interest = [\"Reported Count\", \"Paid Count\", \"Pending Count\", \"Indemnity\", \"Severity\"]\n",
    "\n",
    "# Create an \"Overall\" variable that is the normalized average of the other variables of interest\n",
    "data[\"Overall\"] = data.groupby([\"STATE\", \"COVERAGE\"]).transform(norm)[variables_of_interest].sum(axis=1)\n",
    "variables_of_interest += [\"Overall\"]\n",
    "\n",
    "# Group data by state\n",
    "state_data_group = data.groupby(\"STATE\")\n",
    "\n",
    "# Auto identify states and coverages\n",
    "states = state_data_group.groups.keys()\n",
    "coverages = state_data_group.get_group(states[0]).groupby(\"COVERAGE\").groups.keys()\n",
    "\n",
    "# Data structure for APIs\n",
    "startYear = 2007\n",
    "startMonth = 0\n",
    "\n",
    "overview_data = {\"startYear\": startYear,\n",
    "                \"startMonth\": startMonth,\n",
    "                \"metrics\": {}}\n",
    "for variable in variables_of_interest:\n",
    "    overview_data[\"metrics\"][variable] = {}\n",
    "    for coverage in coverages:\n",
    "        overview_data[\"metrics\"][variable][coverage] = {}\n",
    "        for state in states:\n",
    "            overview_data[\"metrics\"][variable][coverage][state] = []\n",
    "\n",
    "\n",
    "detail_data = {}\n",
    "for state in states:\n",
    "    detail_data[state] = {\"startYear\": startYear,\n",
    "                          \"startMonth\": startMonth,\n",
    "                          \"metrics\": {}}\n",
    "    for variable in variables_of_interest:\n",
    "        detail_data[state][\"metrics\"][variable] = {}\n",
    "        for coverage in coverages:\n",
    "            detail_data[state][\"metrics\"][variable][coverage] = {}\n",
    "\n",
    "\n",
    "# Loop through states & coverage groups, but instead of building the models here, prepare the data for the MapReduce job.\n",
    "with open('data.json', 'w') as f:\n",
    "    for state in states:\n",
    "        coverage_group = state_data_group.get_group(state).groupby(\"COVERAGE\")\n",
    "        for coverage, focused_data in coverage_group:\n",
    "            # Test code\n",
    "            #if test and models_built >=100:\n",
    "            #    break;\n",
    "            \n",
    "            key = [state, coverage]\n",
    "            value = focused_data\n",
    "            line = json.dumps(key) + '\\t' + value.to_json()\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "\n",
    "# Run the Hadoop Streaming MapReduce job\n",
    "out = hadoop_streaming(mapper='identity.py',\n",
    "                       reducer='build_models2.py',\n",
    "                       input_file='data.json',\n",
    "                       files=['mltimeseries.py',\n",
    "                              '/usr/lib/vmware-tools/lib/libXrender.so.1/libXrender.so.1', # Required for matplotlib.pyplot\n",
    "                              '/usr/lib/vmware-tools/lib/libXau.so.6/libXau.so.6'],        # Required for matplotlib.pyplot\n",
    "                       options='-D mapreduce.job.reduces=100')\n",
    "\n",
    "\n",
    "models_built = 0\n",
    "total_absolute_average_error = 0\n",
    "total_mean_squared_error = 0\n",
    "\n",
    "# Update primary data structures with model results\n",
    "for line in out:\n",
    "    line = json.loads(line)\n",
    "\n",
    "    models_built += 1\n",
    "\n",
    "    state = line['state']\n",
    "    variable = line['variable']\n",
    "    coverage = line['coverage']\n",
    "\n",
    "    total_absolute_average_error += line['abs_average_error']\n",
    "    total_mean_squared_error += line['MSE']\n",
    "\n",
    "    overview_data[\"metrics\"][variable][coverage][state] = line['overview_data']\n",
    "    detail_data[state][\"metrics\"][variable][coverage] = line['detail_data']\n",
    "        \n",
    "print '********************************************************************************************'\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)\n",
    "print models_built\n",
    "print '********************************************************************************************'\n",
    "\n",
    "with open('/tmp/kesj/overview.json', 'w') as outfile:\n",
    "    json.dump(overview_data, outfile)\n",
    "\n",
    "with open('/tmp/kesj/detail.json', 'w') as outfile:\n",
    "    json.dump(detail_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redefine using subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.check_call(['ls', '-l'],shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subprocess.check_call(['hadoop', 'fs', '-ls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subprocess.check_output(['hadoop','fs','-ls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = uuid4()\n",
    "print rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseHDFS = '/user/kesj/claiment'\n",
    "!hadoop fs -ls {baseHDFS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd = ['hadoop','fs','-ls']\n",
    "cmd.append(baseHDFS)\n",
    "print cmd\n",
    "subprocess.call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "uname = os.getenv('USER','') #returns '' if not set\n",
    "print uname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afile = 'claiment/base_data_CENT.csv'\n",
    "bfile = 'nada.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    out1 = subprocess.check_output([\"hadoop\",'fs','-ls',afile])\n",
    "    print out1\n",
    "except:\n",
    "    print bfile, \"doesn't exist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_file_in_hdfs = subprocess.call(['hadoop','fs','-ls',bfile])#subprocess.check_output(['hadoop','fs','-ls',input_file])\n",
    "is_file_in_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afile[:afile.find('base_data_CENT.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omfile = '/user/kesj/4675a790-0300-4f98-9393-339df94672a3/output/part*'\n",
    "omfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ocmd =['hadoop','fs','-cat',omfile]\n",
    "subprocess.check_output(ocmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hadoop_streaming2(mapper='identity.py',\n",
    "                       reducer='build_models.py',\n",
    "                       input_file='data.json',\n",
    "                       files=['mltimeseries.py',\n",
    "                              '/usr/lib/vmware-tools/lib/libXrender.so.1/libXrender.so.1', # Required for matplotlib.pyplot\n",
    "                              '/usr/lib/vmware-tools/lib/libXau.so.6/libXau.so.6'],        # Required for matplotlib.pyplot\n",
    "                       options='-D mapreduce.job.reduces=100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function to write the Reducer_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_reducer_file(reducer_file_name,pypath='#!/opt/anaconda/latest/bin/python'):\n",
    "    #delim ='\\t'\n",
    "    with open(reducer_file_name, 'w') as f:\n",
    "        f.write(pypath+'\\n')\n",
    "        f.write('import os\\n')\n",
    "        f.write('import sys\\n')\n",
    "        f.write('sys.path.append(os.getcwd())\\n')\n",
    "        f.write('from mltimeseries import time_series_to_cross_section, optimized_rf\\n')\n",
    "        f.write('import json\\n')\n",
    "        f.write('import pandas as pd\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('for line in sys.stdin:\\n')\n",
    "        l1 = \"    key, value=line.strip().split('\\\\t')\"\n",
    "        f.write(l1+\"\\n\")#\"    key, value = line.strip().split(\"+\"'\\t'\"+\")\\n\")\n",
    "        f.write(\"    state, coverage = json.loads(key)\\n\")\n",
    "        f.write(\"    focused_data = pd.read_json(value)\\n\")\n",
    "    return\n",
    "\"\"\"\n",
    "    variables_of_interest = [\"Reported Count\", \"Paid Count\", \"Pending Count\", \"Indemnity\", \"Severity\", \"Overall\"]\n",
    "    \n",
    "    X, y_data, x, undiff = time_series_to_cross_section(focused_data[variables_of_interest],\n",
    "                                                        forecast_horizon=1,\n",
    "                                                        max_fair_lags=2,\n",
    "                                                        seasonal_factor=12)\n",
    "    for variable in variables_of_interest:\n",
    "\n",
    "        # Account for time series with no variation\n",
    "        state_actuals = focused_data[variable]\n",
    "        std_of_actuals = state_actuals.std(ddof=1)\n",
    "        if std_of_actuals == 0:\n",
    "            output = {'state': state,\n",
    "                      'variable': variable,\n",
    "                      'coverage': coverage,\n",
    "                      'overview_data': [0]*len(state_actuals),\n",
    "                      'detail_data': {\"stddev\": 0,\n",
    "                                      \"actual\": list(state_actuals),\n",
    "                                      \"predicted\": list(state_actuals)},\n",
    "                      'abs_average_error': 0,\n",
    "                      'MSE': 0}\n",
    "            print json.dumps(output)\n",
    "            break\n",
    ")\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delim=\"'\\t'\"\n",
    "l1 = \"    key, value=line.strip().split('\\\\t')\"#'\"+delim+\"')\"\n",
    "print l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_reducer_file('myfile1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_reducer_file(reducer_file_name,pypath='#!/opt/anaconda/latest/bin/python'):\n",
    "    with open(reducer_file_name, 'w') as f:\n",
    "        f.write(pypath+'\\n')\n",
    "        f.write('import os\\n')\n",
    "        f.write('import sys\\n')\n",
    "    return \n",
    "    \"\"\"#for state in states:\n",
    "        coverage_group = state_data_group.get_group(state).groupby(\"COVERAGE\")\n",
    "        for coverage, focused_data in coverage_group:\n",
    "            # Test code\n",
    "            #if test and models_built >=100:\n",
    "            #    break;\n",
    "            \n",
    "            key = [state, coverage]\n",
    "            value = focused_data\n",
    "            line = json.dumps(key) + '\\t' + value.to_json()\n",
    "            f.write(line + '\\n')\n",
    "\"\"\"\n",
    "\n",
    "#%%file build_models.py\n",
    "#!/opt/anaconda/latest/bin/python\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from mltimeseries import time_series_to_cross_section, optimized_rf\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split('\\t')\n",
    "    state, coverage = json.loads(key)\n",
    "    focused_data = pd.read_json(value)\n",
    "\n",
    "    variables_of_interest = [\"Reported Count\", \"Paid Count\", \"Pending Count\", \"Indemnity\", \"Severity\", \"Overall\"]\n",
    "    \n",
    "    X, y_data, x, undiff = time_series_to_cross_section(focused_data[variables_of_interest],\n",
    "                                                        forecast_horizon=1,\n",
    "                                                        max_fair_lags=2,\n",
    "                                                        seasonal_factor=12)\n",
    "    for variable in variables_of_interest:\n",
    "\n",
    "        # Account for time series with no variation\n",
    "        state_actuals = focused_data[variable]\n",
    "        std_of_actuals = state_actuals.std(ddof=1)\n",
    "        if std_of_actuals == 0:\n",
    "            output = {'state': state,\n",
    "                      'variable': variable,\n",
    "                      'coverage': coverage,\n",
    "                      'overview_data': [0]*len(state_actuals),\n",
    "                      'detail_data': {\"stddev\": 0,\n",
    "                                      \"actual\": list(state_actuals),\n",
    "                                      \"predicted\": list(state_actuals)},\n",
    "                      'abs_average_error': 0,\n",
    "                      'MSE': 0}\n",
    "            print json.dumps(output)\n",
    "            break\n",
    "\n",
    "        # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "        final_model, variables_in_model = optimized_rf(X, y_data[variable],\n",
    "                                                       variable_importance_n_estimators=20,\n",
    "                                                       n_estimators_in_grid_search=10,\n",
    "                                                       number_of_important_variables_to_use_options=[6],#, 8, 10, 12, 15, 20],\n",
    "                                                       variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                       n_estimators_to_retrain_best_model=10,\n",
    "                                                       verbose=False, n_random_models_to_test=1,\n",
    "                                                       charts=False, n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "        state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "\n",
    "        state_residuals = focused_data[variable] - state_predictions\n",
    "        data_consumed_for_model = sum(state_residuals == 0)\n",
    "        std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "        state_std_residuals = state_residuals/std_of_residuals\n",
    "        abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "        MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()         \n",
    "\n",
    "        output = {'state': state,\n",
    "                  'variable': variable,\n",
    "                  'coverage': coverage,\n",
    "                  'overview_data': list(state_std_residuals.values),\n",
    "                  'detail_data': {\"stddev\": std_of_residuals,\n",
    "                                  \"actual\": list(state_actuals),\n",
    "                                  \"predicted\": list(state_predictions.round(0).astype(int))},\n",
    "                  'abs_average_error': abs_average_error,\n",
    "                  'MSE': MSE}\n",
    "\n",
    "        print json.dumps(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### function to gather and group the streaming job file\n",
    "def clean_up_results2(streaming_output,tgtdir):\n",
    "    import json\n",
    "    # define local variables\n",
    "    models_built=0\n",
    "    total_absolute_average_error = 0\n",
    "    total_mean_squared_error = 0\n",
    "    \n",
    "    for sline in streaming_output.split('\\n'):\n",
    "        try:\n",
    "            line = json.loads(sline)\n",
    "        except: # skips non-json format\n",
    "            pass\n",
    "        \n",
    "        models_built += 1\n",
    "\n",
    "        state = line['state']\n",
    "        variable = line['variable']\n",
    "        coverage = line['coverage']\n",
    "\n",
    "        total_absolute_average_error += line['abs_average_error']\n",
    "        total_mean_squared_error += line['MSE']\n",
    "\n",
    "        overview_data[\"metrics\"][variable][coverage][state] = line['overview_data']\n",
    "        detail_data[state][\"metrics\"][variable][coverage] = line['detail_data']\n",
    "        \n",
    "    print '********************************************************************************************'\n",
    "    print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)\n",
    "    print models_built\n",
    "    print '********************************************************************************************'\n",
    "    # check that tgtdir ends in '/'; append if not\n",
    "    if not tgtdir.endswith('/'):\n",
    "        tgtdir+='/'\n",
    "        \n",
    "    overview_file = tgtdir+'overview.json'\n",
    "    detail_file = tgtdir+'detail.json'\n",
    "    # check on existence of the directory and create it if missing\n",
    "    make_sure_path_exists(tgtdir)\n",
    "    with open(overview_file, 'w') as outfile:\n",
    "        json.dump(overview_data, outfile)\n",
    "\n",
    "    with open(detail_file, 'w') as outfile:\n",
    "        json.dump(detail_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdir = '/home/kesj/work/cent1/output1/'\n",
    "pdir = '/home/kesj/work/claiment/out1'\n",
    "os.path.isdir(pdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_sure_path_exists(tdir)\n",
    "os.path.isdir(tdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_sure_path_exists(tdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sure_path_exists(path):\n",
    "    if not os.path.isdir(path):\n",
    "        try: \n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            if not os.path.isdir(path):\n",
    "                raise\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coverages = ['BI','COLL','COMP','Injury','MPC','PD','PIP','PIP/MPC','Property','UBI','WBI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = list(df_1.STATE.unique())\n",
    "states = list(set([a[0] for a in s1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create API data structures\n",
    "# Data structure for APIs\n",
    "startYear = 2007\n",
    "startMonth = 0\n",
    "\n",
    "overview_data = {\"startYear\": startYear,\n",
    "                \"startMonth\": startMonth,\n",
    "                \"metrics\": {}}\n",
    "variables_of_interest = iv_1\n",
    "for variable in variables_of_interest:\n",
    "    overview_data[\"metrics\"][variable] = {}\n",
    "    for coverage in coverages:\n",
    "        overview_data[\"metrics\"][variable][coverage] = {}\n",
    "        for state in states:\n",
    "            overview_data[\"metrics\"][variable][coverage][state] = []\n",
    "\n",
    "\n",
    "detail_data = {}\n",
    "for state in states:\n",
    "    detail_data[state] = {\"startYear\": startYear,\n",
    "                          \"startMonth\": startMonth,\n",
    "                          \"metrics\": {}}\n",
    "    for variable in variables_of_interest:\n",
    "        detail_data[state][\"metrics\"][variable] = {}\n",
    "        for coverage in coverages:\n",
    "            detail_data[state][\"metrics\"][variable][coverage] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_up_results2(out,tdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## RERUN with different parameters in build_model file (bmodel.py)\n",
    "# In the dataset, every state has 11 coverage types. There are 5 main\n",
    "# metrics for each coverage type. There are 69 months of data associated\n",
    "# with each metric.\n",
    "\n",
    "#import pandas as pd\n",
    "#import json\n",
    "\n",
    "#def norm(series):\n",
    "#    return (series - series.mean())/series.std(ddof=1)\n",
    "\n",
    "tdir = '/home/kesj/work/claiment/out3'\n",
    "\n",
    "# Run the Hadoop Streaming MapReduce job\n",
    "out = hadoop_streaming2(mapper='identity.py',\n",
    "                       reducer='build_models.py',\n",
    "                       input_file='data.json',\n",
    "                       files=['mltimeseries.py',\n",
    "                              '/usr/lib/vmware-tools/lib/libXrender.so.1/libXrender.so.1', # Required for matplotlib.pyplot\n",
    "                              '/usr/lib/vmware-tools/lib/libXau.so.6/libXau.so.6'],        # Required for matplotlib.pyplot\n",
    "                       options='-D mapreduce.job.reduces=100',\n",
    "                       baseHDFSdir=srcdata_dir+'/run1/')\n",
    "\n",
    "# possibly change this so that I can loop through the output files or concatenate them all --> could overflow memory?\n",
    "\n",
    "# Update primary data structures with model results\n",
    "clean_up_results2(out,tdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdir = '/opt/edge/kesj/work/claiment/out3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f0 = subprocess.check_output(['hadoop','fs','-cat','/user/kesj/13e7e990-37ee-4e8b-b04d-9857053e4ef3/output/part-00000'])\n",
    "len(f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlisting =subprocess.check_output(['hadoop','fs','-ls','/user/kesj/4675a790-0300-4f98-9393-339df94672a3/output/part*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_listing = [a.split(' ')[-1] for a in dlisting.split('\\n')][:-1]\n",
    "print len(file_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[f for f in file_listing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subprocess.check_output(['hadoop','fs','-cat',f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in out.split('\\n'):\n",
    "    \n",
    "    try:\n",
    "        mline = json.loads(line)\n",
    "    except:\n",
    "        print line\n",
    "        pass\n",
    "    \n",
    "    models_built += 1\n",
    "\n",
    "    state = mline['state']\n",
    "    variable = mline['variable']\n",
    "    coverage = mline['coverage']\n",
    "\n",
    "    total_absolute_average_error += mline['abs_average_error']\n",
    "    total_mean_squared_error += mline['MSE']\n",
    "\n",
    "    overview_data[\"metrics\"][variable][coverage][state] = mline['overview_data']\n",
    "    detail_data[state][\"metrics\"][variable][coverage] = mline['detail_data']\n",
    "        \n",
    "print '********************************************************************************************'\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)\n",
    "print models_built\n",
    "print '********************************************************************************************'\n",
    "\n",
    "with open('/tmp/kesj/overview.json', 'w') as outfile:\n",
    "    json.dump(overview_data, outfile)\n",
    "\n",
    "with open('/tmp/kesj/detail.json', 'w') as outfile:\n",
    "    json.dump(detail_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(out.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for line in out.split('\\n'):\n",
    "    k +=1\n",
    "    if len(line)>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at the rounding issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by creating a dumpJSON function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_dump_JSON(outfile,grouped_df,states):\n",
    "    with open(outfile,'w') as f:\n",
    "        for state in states:\n",
    "            coverage = grouped_df.get_group(state).groupby(\"COVERAGE\")\n",
    "            for coverage,focused_data in coverage_group:\n",
    "                key = [state,coverage]\n",
    "                value = focused_data\n",
    "                line=json.dumps(key)+'\\t'+value.to_json(double_precision=2)\n",
    "                f.write(line + '\\n')\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(state_data_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dump_JSON('myData2.json',state_data_group,states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through states & coverage groups, but instead of building the models here, prepare the data for the MapReduce job.\n",
    "with open('data2.json', 'w') as f:\n",
    "    for state in states:\n",
    "        coverage_group = state_data_group.get_group(state).groupby(\"COVERAGE\")\n",
    "        for coverage, focused_data in coverage_group:\n",
    "            # Test code\n",
    "            #if test and models_built >=100:\n",
    "            #    break;\n",
    "            \n",
    "            key = [state, coverage]\n",
    "            value = focused_data\n",
    "            line = json.dumps(key) + '\\t' + value.to_json()\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## RERUN with different parameters in build_model file (bmodel.py)\n",
    "# In the dataset, every state has 11 coverage types. There are 5 main\n",
    "# metrics for each coverage type. There are 69 months of data associated\n",
    "# with each metric.\n",
    "\n",
    "#import pandas as pd\n",
    "#import json\n",
    "\n",
    "#def norm(series):\n",
    "#    return (series - series.mean())/series.std(ddof=1)\n",
    "\n",
    "tdir = '/home/kesj/work/claiment/out3'\n",
    "\n",
    "# Run the Hadoop Streaming MapReduce job\n",
    "out = hadoop_streaming2(mapper='identity.py',\n",
    "                       reducer='bmodels3.py',\n",
    "                       input_file='data.json',\n",
    "                       files=['mltimeseries.py',\n",
    "                              '/usr/lib/vmware-tools/lib/libXrender.so.1/libXrender.so.1', # Required for matplotlib.pyplot\n",
    "                              '/usr/lib/vmware-tools/lib/libXau.so.6/libXau.so.6'],        # Required for matplotlib.pyplot\n",
    "                       options='-D mapreduce.job.reduces=100')\n",
    "\n",
    "# possibly change this so that I can loop through the output files or concatenate them all --> could overflow memory?\n",
    "\n",
    "# Update primary data structures with model results\n",
    "clean_up_results2(out,tdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now move to hdfs\n",
    "!hadoop fs -ls {bdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/data/discovery/claiment/derived/jan2015/v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls /data/discovery/monkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usecase='claiment'\n",
    "hdfs_prepend = '/data/discovery/'\n",
    "hdfs_path = hdfs_prepend+usecase\n",
    "print hdfs_path\n",
    "test_hdfs = hdfs_path + '/monkey'\n",
    "# check that this file exists\n",
    "file_in_hdfs = subprocess.call(['hadoop','fs','-ls',hdfs_path])#subprocess.check_output(['hadoop','fs','-ls',input_file])\n",
    "print is_file_in_hdfs\n",
    "tis_file_in_hdfs = subprocess.call(['hadoop','fs','-ls',test_hdfs])\n",
    "#subprocess.check_output(['hadoop','fs','-ls',input_file])\n",
    "print tis_file_in_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_hdfs_directory(usecase):\n",
    "    from uuid import uuid4\n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    # check if the usecase directory exists\n",
    "    hdfs_prepend = '/data/discovery/'\n",
    "    hdfs_path = hdfs_prepend+usecase\n",
    "    \n",
    "    if subprocess.call(['hadoop','fs','-ls',hdfs_path]): # directory doesn't exist.\n",
    "        # place in the users directory space\n",
    "        uname = os.getenv('USER', '')\n",
    "        random_folder = str(uuid4())\n",
    "        # build it in the users space\n",
    "        hdfs_path = '/user/' + uname + '/' + random_folder\n",
    "        if subprocess.call(['hadoop','fs','-ls','/user/'+uname]): # if directory doesn't exist.\n",
    "            hdfs_path = '/tmp/' + uname + '/' + random_folder # assign to the tmpdirectory\n",
    "                    \n",
    "    print hdfs_path\n",
    "            \n",
    "    #    # define the base directory as that file     \n",
    "    #    bdir = base_hdfs_dir[:base_hdfs_dir.find(random_folder)]  #shortened version\n",
    "    #elif base_hdfs_dir[-1] == '/':  # make sure it doesn't end with '/'\n",
    "    #    base_hdfs_dir = base_hdfs_dir[:-1]\n",
    "    #    bdir = base_hdfs_dir[:base_hdfs_dir.rfind('/')]\n",
    "    #    bdir += '/'\n",
    "    # now add the staging and derived directories to this path \n",
    "    \n",
    "    print usecase,hdfs_path\n",
    "    return hdfs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_files = ['/staging/','/derived/']\n",
    "bb = []\n",
    "for t in tmp_files:\n",
    "    bb.append(bfile_name+t)\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_file_paths = [bfile_name+x for x in tmp_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bfile_name = create_hdfs_directory('claiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_file = subprocess.call(['hadoop','fs','-ls','/data/discovery/claiment/'])\n",
    "print is_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = 'base_CENT_01_2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfile = infile[:infile.rfind('.csv')]+'.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_tfile_not_exists = subprocess.call(['hadoop','fs','-ls',tfile])\n",
    "is_tfile_not_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdfs_path = '/data/discovery/claiment/'\n",
    "tgt_directories = ['/staging/','/derived/']\n",
    "base_file_paths = [hdfs_path + x for x in tgt_directories]\n",
    "!hadoop fs -ls {base_file_paths[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls {base_file_paths[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls /data/discovery/claiment/derived/jan2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = '/home/kesj/work/cent1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls /home/kesj/work/cent1/out5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -put /home/kesj/work/cent1/out5/ /data/discovery/claiment/derived/012015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd ../../claiment/CENT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%loadpy build_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/opt/anaconda/latest/bin/python\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from mltimeseries import time_series_to_cross_section, optimized_rf\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# define a dictionary of rounding_digits based upon the variables of interest\n",
    "variables_of_interest = [\"Reported Count\", \"Paid Count\", \"Pending Count\", \"Indemnity\", \"Severity\", \"Overall\"]\n",
    "from collections import defaultdict\n",
    "rnd_digits = defaultdict(int)\n",
    "for v in variables_of_interest:\n",
    "    if v.endswith('Count'):\n",
    "        rnd_digits[v]=0\n",
    "    else:\n",
    "        rnd_digits[v]=2\n",
    "\n",
    "# define the time_horizon list\n",
    "time_horizon_list = [1,6,12]\n",
    "for line in sys.stdin:\n",
    "    # define a dictionary to hold the output\n",
    "    my_output = {}\n",
    "    key, value = line.strip().split('\\t')\n",
    "    state, coverage = json.loads(key)\n",
    "    focused_data = pd.read_json(value)\n",
    "    forecast = 0.0\n",
    "\n",
    "    for time_horizon in time_horizon_list:\n",
    "        X, y_data, x, undiff = time_series_to_cross_section(focused_data[variables_of_interest],\n",
    "                                                        forecast_horizon=time_horizon,\n",
    "                                                        max_fair_lags=13,\n",
    "                                                        seasonal_factor=12)\n",
    "        for variable in variables_of_interest:\n",
    "\n",
    "            # Account for time series with no variation\n",
    "            state_actuals = focused_data[variable]\n",
    "            std_of_actuals = state_actuals.std(ddof=1)\n",
    "            if std_of_actuals == 0:\n",
    "                output = {'state': state,\n",
    "                      'variable': variable,\n",
    "                      'coverage': coverage,\n",
    "                      'overview_data': [0]*len(state_actuals),\n",
    "                      'detail_data': {\"stddev\": 0,\n",
    "                                      \"actual\": list(state_actuals),\n",
    "                                      \"predicted\": list(state_actuals),\n",
    "                                      \"point_forecast\":(0,0)},\n",
    "                      'abs_average_error': 0,\n",
    "                      'MSE': 0}\n",
    "                #print json.dumps(output)\n",
    "                my_output[time_horizon]=output\n",
    "                break\n",
    "\n",
    "        # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "            final_model, variables_in_model = optimized_rf(X, y_data[variable],\n",
    "                                                       variable_importance_n_estimators=120,\n",
    "                                                       n_estimators_in_grid_search=50,\n",
    "                                                       number_of_important_variables_to_use_options=[6,8,10,12,15,20],#[6],#, 8, 10, 12, 15, 20],\n",
    "                                                       variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                       n_estimators_to_retrain_best_model=200,\n",
    "                                                       verbose=False, n_random_models_to_test=6,\n",
    "                                                       charts=False, n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "            state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "\n",
    "            state_residuals = focused_data[variable] - state_predictions\n",
    "            data_consumed_for_model = sum(state_residuals == 0)\n",
    "            std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "            state_std_residuals = state_residuals/std_of_residuals\n",
    "            abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "            MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()\n",
    "\n",
    "            # convert list of values to 2 floating point digits or 0 for counts\n",
    "            rnd_size = rnd_digits[variable]\n",
    "            if rnd_size > 0 :\n",
    "                state_actuals = [ round(elem,rnd_size) for elem in list(state_actuals)]\n",
    "                state_predictions = [ round(elem,rnd_size) for elem in list(state_predictions)]\n",
    "            else:\n",
    "                state_actuals = [ int(round(elem,0)) for elem in list(state_actuals)]\n",
    "                state_predictions = [ int(round(elem,0)) for elem in list(state_predictions)]\n",
    "\n",
    "            overview_values = [round(elem,2) for elem in list(state_std_residuals.values)]\n",
    "            output = {'state': state,\n",
    "                      'variable': variable,\n",
    "                      'coverage': coverage,\n",
    "                      'overview_data': overview_values,#list(state_std_residuals.values),\n",
    "                      'detail_data': {\"stddev\": round(std_of_residuals,5),\n",
    "                                  \"actual\": state_actuals,\n",
    "                                  \"predicted\": state_predictions,\n",
    "                                  \"point_forecast\": (forecast,round(std_of_residuals,5))},#list(state_predictions.round(0).astype(int))},\n",
    "                  'abs_average_error': round(abs_average_error,5),\n",
    "                  'MSE': round(MSE,5)}\n",
    "\n",
    "            my_output[time_horizon]=output\n",
    "        #print json.dumps(output)\n",
    "    print json.dumps(my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(idmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idmap in idmapping[:10]:\n",
    "    # define a dictionary to hold the output\n",
    "    my_output = {}\n",
    "    key, value = idmap.strip().split('\\t')\n",
    "    state, coverage = json.loads(key)\n",
    "    focused_data = pd.read_json(value)\n",
    "    forecast = 0.0\n",
    "    print key,value\n",
    "    for time_horizon in time_horizon_list:\n",
    "        X, y_data, x, undiff = time_series_to_cross_section(focused_data[variables_of_interest],\n",
    "                                                        forecast_horizon=time_horizon,\n",
    "                                                        max_fair_lags=13,\n",
    "                                                        seasonal_factor=12)\n",
    "        for variable in variables_of_interest:\n",
    "\n",
    "            # Account for time series with no variation\n",
    "            state_actuals = focused_data[variable]\n",
    "            std_of_actuals = state_actuals.std(ddof=1)\n",
    "            if std_of_actuals == 0:\n",
    "                output = {'state': state,\n",
    "                      'variable': variable,\n",
    "                      'coverage': coverage,\n",
    "                      'overview_data': [0]*len(state_actuals),\n",
    "                      'detail_data': {\"stddev\": 0,\n",
    "                                      \"actual\": list(state_actuals),\n",
    "                                      \"predicted\": list(state_actuals),\n",
    "                                      \"point_forecast\":(0,0)},\n",
    "                      'abs_average_error': 0,\n",
    "                      'MSE': 0}\n",
    "                #print json.dumps(output)\n",
    "                my_output[time_horizon]=output\n",
    "                break\n",
    "\n",
    "        # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "            final_model, variables_in_model = optimized_rf(X, y_data[variable],\n",
    "                                                       variable_importance_n_estimators=120,\n",
    "                                                       n_estimators_in_grid_search=50,\n",
    "                                                       number_of_important_variables_to_use_options=[6],#,8,10,12,15,20],#[6],#, 8, 10, 12, 15, 20],\n",
    "                                                       variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                       n_estimators_to_retrain_best_model=200,\n",
    "                                                       verbose=False, n_random_models_to_test=6,\n",
    "                                                       charts=False, n_jobs=1)\n",
    "\n",
    "\n",
    "            if final_model == None:\n",
    "                print key,value,time_horizon\n",
    "                break\n",
    "            state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "\n",
    "            state_residuals = focused_data[variable] - state_predictions\n",
    "            data_consumed_for_model = sum(state_residuals == 0)\n",
    "            std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "            state_std_residuals = state_residuals/std_of_residuals\n",
    "            abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "            MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()\n",
    "\n",
    "            # convert list of values to 2 floating point digits or 0 for counts\n",
    "            rnd_size = rnd_digits[variable]\n",
    "            if rnd_size > 0 :\n",
    "                state_actuals = [ round(elem,rnd_size) for elem in list(state_actuals)]\n",
    "                state_predictions = [ round(elem,rnd_size) for elem in list(state_predictions)]\n",
    "            else:\n",
    "                state_actuals = [ int(round(elem,0)) for elem in list(state_actuals)]\n",
    "                state_predictions = [ int(round(elem,0)) for elem in list(state_predictions)]\n",
    "\n",
    "            overview_values = [round(elem,2) for elem in list(state_std_residuals.values)]\n",
    "            output = {'state': state,\n",
    "                      'variable': variable,\n",
    "                      'coverage': coverage,\n",
    "                      'overview_data': overview_values,#list(state_std_residuals.values),\n",
    "                      'detail_data': {\"stddev\": round(std_of_residuals,5),\n",
    "                                  \"actual\": state_actuals,\n",
    "                                  \"predicted\": state_predictions,\n",
    "                                  \"point_forecast\": (forecast,round(std_of_residuals,5))},#list(state_predictions.round(0).astype(int))},\n",
    "                  'abs_average_error': round(abs_average_error,5),\n",
    "                  'MSE': round(MSE,5)}\n",
    "\n",
    "            my_output[time_horizon]=output\n",
    "        #print json.dumps(output)\n",
    "    print json.dumps(my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(X)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_output[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_output[1]['detail_data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_output[1]['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03.09.2015\n",
    "\n",
    "* I have triplicate.py as mapper and build_models_u13.py as reducer working\n",
    "* output in /data/discovery/claiment/staging/u13/\n",
    "* I need to work on initialize_CENT_api_containers and clean_up_results functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outB = subprocess.check_output(['hadoop','fs', '-cat',transformed_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outB2 = outB.split('\\n')\n",
    "len(outB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outB2[0].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#triplicate\n",
    "outC = []\n",
    "time_horizon_list = [1,6,12]\n",
    "for line in outB2:\n",
    "    try:\n",
    "        key, value = line.split('\\t')\n",
    "        state,coverage = json.loads(key)\n",
    "        for time_horizon in time_horizon_list:\n",
    "            key2 = [state,coverage,time_horizon]\n",
    "            line = json.dumps(key2) + '\\t' + value\n",
    "        #print line\n",
    "            outC.append(line)\n",
    "    except ValueError:\n",
    "        print \"valueError\",line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(outC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in outC:\n",
    "    key,value = line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a single file and keep it as the output\n",
    "testfile= '/data/discovery/claiment/staging/u15/tmp/output/part-*'\n",
    "outA = subprocess.check_output(['hadoop','fs','-cat',testfile])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_CENT_api_2(coverages, variables_of_interest,states, time_horizons=[1,6,12], start_year = 2007,start_month=0):\n",
    "    # overview_data is of the format overview_data[horizon][metric][variable][coverage][state]\n",
    "    \"\"\"overview_data = {\"startYear\": start_year,\n",
    "                     \"startMonth\": start_month,\n",
    "                     \"metrics\": {}}\n",
    "\n",
    "    for variable in variables_of_interest:\n",
    "        overview_data[\"metrics\"][variable] = {}\n",
    "        for coverage in coverages:\n",
    "            overview_data[\"metrics\"][variable][coverage] = {}\n",
    "            for state in states:\n",
    "                overview_data[\"metrics\"][variable][coverage][state] = []\n",
    "\"\"\"\n",
    "    \n",
    "    overview_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        print horizon\n",
    "        overview_data[horizon] = {\"startYear\": start_year,\n",
    "                     \"startMonth\": start_month,\n",
    "                     \"metrics\": {}}\n",
    "        for variable in variables_of_interest:\n",
    "            overview_data[horizon][\"metrics\"][variable] = {}\n",
    "            for coverage in coverages:\n",
    "                overview_data[horizon][\"metrics\"][variable][coverage] = {}\n",
    "                for state in states:\n",
    "                    overview_data[horizon][\"metrics\"][variable][coverage][state] = []\n",
    "\n",
    "    # detail_data is of the format of detail_data[horizon][state][\"metrics\"][variable][coverage]\n",
    "    detail_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        detail_data[horizon] = {}\n",
    "        for state in states:\n",
    "            detail_data[horizon][state] = {\"startYear\": start_year,\n",
    "                              \"startMonth\": start_month,\n",
    "                              \"metrics\": {}}\n",
    "            for variable in variables_of_interest:\n",
    "                detail_data[horizon][state][\"metrics\"][variable] = {}\n",
    "                for coverage in coverages:\n",
    "                    detail_data[horizon][state][\"metrics\"][variable][coverage] = {}\n",
    "\n",
    "    return (detail_data, overview_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmpPanel = pd.Panel4D(items=states,major_axis=list(coverages),labels=variables_of_interest,minor_axis=['horizon','value','std'])\n",
    "tmpPanel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmpPanel['Reported Count']['ALABAMA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tPanel = pd.Panel4D(labels=states, items=list(coverages),major_axis=variables_of_interest,minor_axis=['v1','v6','v12','e1','e6','e12'])#,'value','std'])\n",
    "tPanel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def slice_interp(x):\n",
    "    values = x[:3]\n",
    "    errors = x[3:]\n",
    "    forecast_values = interpolate_CENT_predictions(values)\n",
    "    forecast_errors = interpolate_CENT_predictions(errors)\n",
    "    return forecast_values,forecast_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interpolate_CENT_predictions(input_list,round_digit=2):\n",
    "    \"\"\"Returns list of interpolated values\"\"\"\n",
    "    interpolated_predictions=[]\n",
    "    forecast_1_month = input_list[0]\n",
    "    forecast_6_month = input_list[1]\n",
    "    forecast_12_month = input_list[2]\n",
    "    # Add 1 month prediction\n",
    "    interpolated_predictions.append(round(forecast_1_month,round_digit))\n",
    "    \n",
    "    # Interpolate values between 1 month and 6 months\n",
    "    one_six_gap = (forecast_6_month-forecast_1_month)/5.0\n",
    "    for e in range(1,5):\n",
    "        interpolated_predictions.append(round(forecast_1_month+e*one_six_gap,round_digit))\n",
    "    \n",
    "    # Add 6 month prediction\n",
    "    interpolated_predictions.append(round(forecast_6_month,round_digit))\n",
    "\n",
    "    # Interpolate values between 6 months and 12 months\n",
    "    six_twelve_gap = (forecast_12_month-forecast_6_month)/6.0\n",
    "    for e in range(1,6):\n",
    "        interpolated_predictions.append(round(forecast_6_month+e*six_twelve_gap,round_digit))\n",
    "    \n",
    "    # Add 12 month prediction\n",
    "    interpolated_predictions.append(round(forecast_12_month,round_digit))\n",
    "    return interpolated_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tPanel['ARKANSAS']['BI'].apply(lambda x: slice_interp(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tPanel['ARKANSAS']['BI'].loc['Severity','forecast'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = tmpPanel['Reported Count']['ALABAMA'].copy()\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coverages, variables_of_interest, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_CENT_api_3(coverages, variables_of_interest,states, time_horizons=[1,6,12], start_year = 2007,start_month=0):\n",
    "    # create a dataframe for the forecast data\n",
    "    #forecastDF = pd.Panel4D()\n",
    "    #forecast_dimension = ['v'+str(time) for time in time_horizons]\n",
    "    forecast_dimension=[]\n",
    "    for time in time_horizons:\n",
    "        forecast_dimension.append('v'+str(time))\n",
    "        forecast_dimension.append('e'+str(time))\n",
    "        \n",
    "    \n",
    "    tmpPanel = pd.Panel4D(labels=states, items=list(coverages),major_axis=variables_of_interest,minor_axis=forecast_dimension)#,'value','std'])\n",
    "\n",
    "    #tmpPanel = pd.Panel4D(labels=states,items=coverages,major_axis=variables_of_interest,minor_axis=['horizon','value','std'])\n",
    "    \n",
    "    # overview_data is of the format overview_data[horizon][metric][variable][coverage][state]\n",
    "    \"\"\"overview_data = {\"startYear\": start_year,\n",
    "                     \"startMonth\": start_month,\n",
    "                     \"metrics\": {}}\n",
    "\n",
    "    for variable in variables_of_interest:\n",
    "        overview_data[\"metrics\"][variable] = {}\n",
    "        for coverage in coverages:\n",
    "            overview_data[\"metrics\"][variable][coverage] = {}\n",
    "            for state in states:\n",
    "                overview_data[\"metrics\"][variable][coverage][state] = []\n",
    "\"\"\"\n",
    "    \n",
    "    overview_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        print horizon\n",
    "        overview_data[horizon] = {\"startYear\": start_year,\n",
    "                     \"startMonth\": start_month,\n",
    "                     \"metrics\": {}}\n",
    "        for variable in variables_of_interest:\n",
    "            overview_data[horizon][\"metrics\"][variable] = {}\n",
    "            for coverage in coverages:\n",
    "                overview_data[horizon][\"metrics\"][variable][coverage] = {}\n",
    "                for state in states:\n",
    "                    overview_data[horizon][\"metrics\"][variable][coverage][state] = []\n",
    "\n",
    "    # detail_data is of the format of detail_data[horizon][state][\"metrics\"][variable][coverage]\n",
    "    detail_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        detail_data[horizon] = {}\n",
    "        for state in states:\n",
    "            detail_data[horizon][state] = {\"startYear\": start_year,\n",
    "                              \"startMonth\": start_month,\n",
    "                              \"metrics\": {}}\n",
    "            for variable in variables_of_interest:\n",
    "                detail_data[horizon][state][\"metrics\"][variable] = {}\n",
    "                for coverage in coverages:\n",
    "                    detail_data[horizon][state][\"metrics\"][variable][coverage] = {}\n",
    "\n",
    "    print tmpPanel\n",
    "    return (detail_data, overview_data,tmpPanel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd ../CENT/\n",
    "%loadpy parallel_process_CENT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('base_CENT_01_2015.csv')\n",
    "states = list(data.STATE.unique())\n",
    "coverages = list(data.COVERAGE.unique())\n",
    "variables = ['Reported Count','Paid Count','Pending Count','Indemnity','Severity','Overall']\n",
    "time_horizons = [1,6,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3,overview3, my_panel = initialize_CENT_api_containers(coverages,variables,states,[1,6,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_up_results(outA,'../../cent1/out2',detail3,overview3,my_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(overview3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(my_panel.major_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3[1]['GEORGIA']['metrics']['Pending Count']['UBI'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Feb 25 2015\n",
    "@author: AJ Rader (kesj)\n",
    "aj.rader.kesj@statefarm.com\n",
    "\n",
    "Program to run the CENT program in parallel on hadoop\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_sure_path_exists(path):\n",
    "    if not os.path.isdir(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            if not os.path.isdir(path):\n",
    "                raise\n",
    "    return\n",
    "\n",
    "\n",
    "def transform_to_key_value_pairs(grouped_df,  ouput_file, output_format='json'):\n",
    "    \"\"\"\n",
    "    A function that takes an input csv file of many timeseries data and pivots it into a key-value-pair format.\n",
    "    :input grouped_df is a grouped dataframe\n",
    "    :output_file is an output file\n",
    "    :output_format is the output format\n",
    "    TODO make this work within HDFS/streaming\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    #print len(voi), ofile\n",
    "\n",
    "    dfile = ouput_file + '.' + output_format\n",
    "    if output_format == 'json':\n",
    "        with open(dfile, 'w') as f:\n",
    "            for k in grouped_df.groups.keys():\n",
    "                key = list(k)\n",
    "                value = grouped_df.get_group(k)\n",
    "                line = json.dumps(key) + '\\t' + value.to_json()\n",
    "                f.write(line + '\\n')\n",
    "    # warning following option not really working yet\n",
    "    elif output_format == 'tsv':\n",
    "        with open(dfile, 'w') as f:\n",
    "            for k in grouped_df.groups.keys():\n",
    "                key = str(list(k))\n",
    "                value = grouped_df.get_group(k)\n",
    "                line = key + '\\t' + value.to_csv()\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def preprocess_cent_file(inputfile, x='STATE', y='COVERAGE', t='YEAR',\n",
    "                         important_variables=[], omit_column=[], hdfs_flag=False, create_overall_variable=True):\n",
    "    \"\"\"\n",
    "    A function that takes an input csv file of many timeseries data and processes it into the\n",
    "    format that we want for CENT analysis.\n",
    "\n",
    "    hdfs_flag indicates if the based file is in hdfs or not.\n",
    "\n",
    "    the input parameters:\n",
    "    data -- the flat 2d datashape\n",
    "    important_variables -- the column name to use for the 'items' -- default is 'STATE'\n",
    "    y -- the column name to use for the 'minor-axis' -- default is 'COVERAGE'\n",
    "    t -- the column name to use for the 'major-axis' <- where timeseres go; default is 'YEAR'\n",
    "    z -- the column title to use for 'labels' (these are the columns you can loop over)\n",
    "\n",
    "    :returns\n",
    "      my_grouped_df ( a grouped hierarchical data frame ready to be printed as key,value pairs\n",
    "      important_variables ( the list of metrics used in the calculation)\n",
    "      xvals (the list of the first major axis -- i.e. the states)\n",
    "      yvals ( the list of the minor axis -- i.e. the coverages)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # if hdfs_flag:\n",
    "\n",
    "    data = pd.read_csv(inputfile)\n",
    "    # check format of a couple input formats. we want important_variables, omit_column to be lists\n",
    "\n",
    "    if len(omit_column) != 0:  # omit a list of columns if so given\n",
    "        data = data.drop(omit_column, axis=1, inplace=True)\n",
    "\n",
    "    print \"input dataframe has the dimensions of {0}\".format(np.shape(data))\n",
    "    print \"the column titles are: {0}\".format(data.columns)\n",
    "\n",
    "    # check that x,y,t,variables_of_interest are in columns\n",
    "    # create a set for the singletons:\n",
    "    s1 = set([x, y, t])\n",
    "\n",
    "    # create a set for all columns\n",
    "    scolumns = set(data.columns.values)\n",
    "    # test that s1 is subset of scolumns\n",
    "    sdiff1 = s1 - scolumns\n",
    "    if len(sdiff1) != 0:\n",
    "        # have to fix the s1 input files\n",
    "        print \"We have a problem because the columns you want to pivot on are not present\"\n",
    "        print sdiff1, \" is missing from \", scolumns\n",
    "        return 0\n",
    "    else:\n",
    "        # convert the year to a date-time object\n",
    "        data[t] = pd.to_datetime(data[t])\n",
    "\n",
    "    if len(important_variables) == 0:  # define these based upon the input data if not defined\n",
    "        important_variables = list(scolumns - s1)\n",
    "        print \"important variables are determined by the input data.\"\n",
    "        # print \"{0} are the important variables determined based upon the input data\".format(important_variables)\n",
    "\n",
    "\n",
    "        # option to generate an overall variable that is the normalized average of the other variables of interest\n",
    "    if create_overall_variable:\n",
    "        data['Overall'] = data.groupby([x, y]).transform(series_norm)[important_variables].sum(axis=1)\n",
    "        important_variables += ['Overall']\n",
    "\n",
    "    # generate list of unique values for the 3 input parameters: x,y,t\n",
    "    print s1\n",
    "\n",
    "    xvals = data[x].unique()\n",
    "    yvals = data[y].unique()\n",
    "    tvals = data[t].unique()\n",
    "\n",
    "    print len(xvals), len(yvals), len(tvals), len(important_variables)\n",
    "    # to do: check that all columns are used\n",
    "    # to do: reshape the 2d data frame into a multi-index, hierarchical df\n",
    "    my_grouped_data = data.groupby((x, y))  # set_index([x,y,t]\n",
    "    return (my_grouped_data, important_variables, xvals, yvals)\n",
    "\n",
    "\n",
    "def series_norm(series):\n",
    "    \"\"\" a function that calculates the norm of a selected pandas Series\n",
    "    returns the normalized series values\n",
    "    \"\"\"\n",
    "    return (series - series.mean()) / series.std(ddof=1)\n",
    "\n",
    "\n",
    "def interpolate_CENT_predictions(input_list,round_digit=2):\n",
    "    \"\"\"Returns list of interpolated values, round to the number of places given\n",
    "       strictly assumes spacing of 1 month, 6 month and 12 month.\n",
    "    \"\"\"\n",
    "    interpolated_predictions=[]\n",
    "    forecast_1_month = input_list[0]\n",
    "    forecast_6_month = input_list[1]\n",
    "    forecast_12_month = input_list[2]\n",
    "    # Add 1 month prediction\n",
    "    interpolated_predictions.append(round(forecast_1_month,round_digit))\n",
    "\n",
    "    # Interpolate values between 1 month and 6 months\n",
    "    one_six_gap = (forecast_6_month-forecast_1_month)/5.0\n",
    "    for e in range(1,5):\n",
    "        interpolated_predictions.append(round(forecast_1_month+e*one_six_gap,round_digit))\n",
    "\n",
    "    # Add 6 month prediction\n",
    "    interpolated_predictions.append(round(forecast_6_month,round_digit))\n",
    "\n",
    "    # Interpolate values between 6 months and 12 months\n",
    "    six_twelve_gap = (forecast_12_month-forecast_6_month)/6.0\n",
    "    for e in range(1,6):\n",
    "        interpolated_predictions.append(round(forecast_6_month+e*six_twelve_gap,round_digit))\n",
    "\n",
    "    # Add 12 month prediction\n",
    "    interpolated_predictions.append(round(forecast_12_month,round_digit))\n",
    "    return interpolated_predictions\n",
    "\n",
    "def make_interpolated_forecasts(x):\n",
    "    import numpy as np\n",
    "    \n",
    "    \"\"\"\n",
    "    function to generate the interpolated values from a list of values and errors\n",
    "    :param x: list of 3 values and 3 errors in the format of [v1,e1,v2,e2,v3,e3]\n",
    "    :return: series of forecasted_values and forecasted_errors for the 12 months\n",
    "    assuming v1 is 1 month value, v2 is 6 month value and v3 is 12 month valaue\n",
    "    \"\"\"\n",
    "    # if the input values are nan, just return an empty list\n",
    "    if sum(np.isnan(x)) == len(x) :\n",
    "        forecast_values = []\n",
    "        forecast_errors = []\n",
    "    else:\n",
    "        values = x[::2]\n",
    "        errors = x[1::2]\n",
    "        forecast_values = interpolate_CENT_predictions(values)\n",
    "        forecast_errors = interpolate_CENT_predictions(errors)\n",
    "\n",
    "    return pd.Series(dict(forecast=forecast_values, std=forecast_errors))\n",
    "\n",
    "\n",
    "### function to initialize API data structure containers\n",
    "def initialize_CENT_api_containers(coverages, variables_of_interest, states, time_horizons = [1,6,12], start_year=2007, start_month=0):\n",
    "    \"\"\"\n",
    "    Function for initializing the CENT api containers\n",
    "    :param coverages: list of possible coverages\n",
    "    :param variables_of_interest: list of variables of interest\n",
    "    :param states: list of states\n",
    "    :param time_horizons --> list of time horizons\n",
    "    :param start_year: starting year\n",
    "    :param start_month: starting month\n",
    "    :return: detail_data, overview_data, and panel\n",
    "    \"\"\"\n",
    "\n",
    "    # create a 4D panel data frame for the forecast data\n",
    "    forecast_dimension = []\n",
    "    for time in time_horizons:\n",
    "        forecast_dimension.append('v'+str(time))\n",
    "        forecast_dimension.append('e'+str(time))\n",
    "\n",
    "    my_panel = pd.Panel4D(labels=states, items=coverages,major_axis=variables_of_interest,minor_axis=forecast_dimension)\n",
    "\n",
    "    # overview_data is of the format overview_data[horizon][metric][variable][coverage][state]\n",
    "    overview_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        overview_data[horizon] = {\"startYear\": start_year,\n",
    "                                  \"startMonth\": start_month,\n",
    "                                  \"metrics\": {}}\n",
    "        for variable in variables_of_interest:\n",
    "            overview_data[horizon][\"metrics\"][variable] = {}\n",
    "            for coverage in coverages:\n",
    "                overview_data[horizon][\"metrics\"][variable][coverage] = {}\n",
    "                for state in states:\n",
    "                    overview_data[horizon][\"metrics\"][variable][coverage][state] = []\n",
    "\n",
    "    # detail_data is of the format of detail_data[horizon][state][\"metrics\"][variable][coverage]\n",
    "    detail_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        detail_data[horizon] = {}\n",
    "        for state in states:\n",
    "            detail_data[horizon][state] = {\"startYear\": start_year,\n",
    "                              \"startMonth\": start_month,\n",
    "                              \"metrics\": {}}\n",
    "            for variable in variables_of_interest:\n",
    "                detail_data[horizon][state][\"metrics\"][variable] = {}\n",
    "                for coverage in coverages:\n",
    "                    detail_data[horizon][state][\"metrics\"][variable][coverage] = {}\n",
    "\n",
    "\n",
    "    return (detail_data, overview_data, my_panel)\n",
    "\n",
    "\n",
    "def clean_up_results(streaming_output, tgtdir, detail_data, overview_data, my_panel):\n",
    "    \"\"\"\n",
    "    Function to aggregate the separate files produced by hadoop mapreduce and parse them into the desired output json\n",
    "    files to be used by the web_service. Currently this involves detail.json and overview.json.\n",
    "    :param streaming_output: the  streamed hadoop output.\n",
    "    :param tgtdir: location (in local filespace) where this data is being dumped to.\n",
    "    :param detail_data: CENT api container for the detail_data\n",
    "    :param overview_data: CENT api container for the overview_data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import json\n",
    "    # define local variables\n",
    "    models_built = 0\n",
    "    total_absolute_average_error = 0\n",
    "    total_mean_squared_error = 0\n",
    "\n",
    "    for line in streaming_output.split('\\n'):\n",
    "        try:\n",
    "            line = json.loads(sline)\n",
    "        except TypeError:  # skips non-json format\n",
    "            print line\n",
    "        #    pass\n",
    "\n",
    "        models_built += 1\n",
    "\n",
    "        state = line['state']\n",
    "        variable = line['variable']\n",
    "        coverage = line['coverage']\n",
    "        time_horizon = line['horizon']\n",
    "        point_forecast=[]\n",
    "        point_forecast.append(line['point_forecast']['value'])\n",
    "        point_forecast.append(line['point_forecast']['std'])\n",
    "\n",
    "        total_absolute_average_error += line['abs_average_error']\n",
    "        total_mean_squared_error += line['MSE']\n",
    "\n",
    "        # code to deal with forecast data\n",
    "        value_col = 'v'+str(time_horizon)\n",
    "        error_col = 'e'+str(time_horizon)\n",
    "        my_panel[state][coverage].loc[variable,value_col] = point_forecast[0]\n",
    "        my_panel[state][coverage].loc[variable,error_col] = point_forecast[1]\n",
    "\n",
    "        overview_data[time_horizon][\"metrics\"][variable][coverage][state] = line['overview_data']\n",
    "        detail_data[time_horizon][state][\"metrics\"][variable][coverage] = line['detail_data']\n",
    "\n",
    "\n",
    "    print '********************************************************************************************'\n",
    "    print total_absolute_average_error / float(models_built), total_mean_squared_error / float(models_built)\n",
    "    print models_built\n",
    "    print '********************************************************************************************'\n",
    "\n",
    "    # switch the ordering of the axes to match detail_data ordering\n",
    "    altered_panel = my_panel.swapaxes('items','major_axis')\n",
    "    for label in altered_panel.labels:\n",
    "        for item in altered_panel[label].items:\n",
    "            forecasted = altered_panel[label][item].apply(lambda x: make_interpolated_forecasts(x),axis=1)\n",
    "    #        altered_panel[label][item].loc[forecasted.index,'forecast']=r['forecast'].values\n",
    "    #        altered_panel[label][item].loc[forecasted.index,'std']=r['std'].values\n",
    "            ## now output the forecast to the horizon = 1 key = 'forecast'\n",
    "            for index_cov in forecasted.index:\n",
    "            #append a new dictionary level for 1st time_horizon\n",
    "                detail_data[1][label]['metrics'][item][index_cov]['forecast'] = {\n",
    "                    'values':forecasted.ix[index_cov]['forecast'],\n",
    "                    'std':forecasted.ix[index_cov]['std'] }\n",
    "\n",
    "    # augment the overview_data with the table info for topK and bottomK\n",
    "\n",
    "\n",
    "    # check that tgtdir ends in '/'; append if not\n",
    "    if not tgtdir.endswith('/'):\n",
    "        tgtdir += '/'\n",
    "\n",
    "    overview_file = tgtdir + 'overview.json'\n",
    "    detail_file = tgtdir + 'detail.json'\n",
    "    # check on existence of the directory and create it if missing\n",
    "    make_sure_path_exists(tgtdir)\n",
    "    with open(overview_file, 'w') as outfile:\n",
    "        json.dump(overview_data, outfile)\n",
    "\n",
    "    with open(detail_file, 'w') as outfile:\n",
    "        json.dump(detail_data, outfile)\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3,overview3,tp3 = initialize_CENT_api_3(coverages,variables_of_interest,states,[1,6,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_built = 0\n",
    "total_absolute_average_error = 0\n",
    "total_mean_squared_error = 0\n",
    "\n",
    "for sline in outA.split('\\n'):    \n",
    "    try:\n",
    "        line=json.loads(sline)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    state = line['state']\n",
    "    variable = line['variable']\n",
    "    coverage = line['coverage']\n",
    "    time_horizon = line['horizon']\n",
    "    point_forecast=[]\n",
    "    point_forecast.append(line['point_forecast']['value'])\n",
    "    point_forecast.append(line['point_forecast']['std'])\n",
    "    #point_forecast_std = line['point_forecast']['std']\n",
    "    #point_forecast[time_horizon]=line['detail_data']['point_forecast']\n",
    "\n",
    "    total_absolute_average_error += line['abs_average_error']\n",
    "    total_mean_squared_error += line['MSE']\n",
    "    #if state.startswith('I'):# == 'INDIANA':\n",
    "    #    print state, variable, coverage, time_horizon\n",
    "    print state,variable,coverage,time_horizon,point_forecast#,type(point_forecast)\n",
    "    \n",
    "    #tPanel['ARKANSAS']['BI'].loc['Severity','v1']=f1[0]\n",
    "    value_col = 'v'+str(time_horizon)\n",
    "    error_col = 'e'+str(time_horizon)\n",
    "    tp3[state][coverage].loc[variable,value_col] = point_forecast[0]\n",
    "    tp3[state][coverage].loc[variable,error_col] = point_forecast[1]\n",
    "#tPanel['ARKANSAS']['BI'].loc['Severity','v6']=f6[0]\n",
    "#tPanel['ARKANSAS']['BI'].loc['Severity','e6']=f6[1]\n",
    "#tPanel['ARKANSAS']['BI'].loc['Severity','v12']=f12[0]\n",
    "#tPanel['ARKANSAS']['BI'].loc['Severity','e12']=f12[1]\n",
    "#tPanel['ARKANSAS']['BI']\n",
    "    #tmpDF[state][coverage].ix[variable]\n",
    "    overview3[time_horizon][\"metrics\"][variable][coverage][state]=line['overview_data']\n",
    "    tline = line['detail_data']\n",
    "    detail3[time_horizon][state][\"metrics\"][variable][coverage]=tline#line['detail_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_interpolated_forecasts(x):\n",
    "    # if the input values are nan, just return an empty list\n",
    "    if sum(np.isnan(x)) == len(x) :\n",
    "        forecast_values = []\n",
    "        forecast_errors = []\n",
    "    else:\n",
    "        values = x[::2]\n",
    "        errors = x[1::2]\n",
    "        forecast_values = interpolate_CENT_predictions(values)\n",
    "        forecast_errors = interpolate_CENT_predictions(errors)\n",
    "    \n",
    "    return pd.Series(dict(forecast=forecast_values, std=forecast_errors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp3['DELAWARE']['PIP'].apply(lambda x: make_interpolated_forecasts(x),axis=1)#.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tp3['DELAWARE']['Property'].apply(lambda x: make_interpolated_forecasts(x),axis=1)#.ix[:].apply(lambda x: make_interpolated_forecasts(x),row=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp3['DELAWARE']['Property'].ix[:].apply(lambda x: make_interpolated_forecasts(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "up3 = tp3.swapaxes('labels','major_axis')\n",
    "print np.shape(up3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label in up3.labels:\n",
    "    for item in up3[label].items:\n",
    "        r = up3[label][item].apply(lambda x: make_interpolated_forecasts(x),axis=1)\n",
    "        up3[label][item].loc[r.index,'forecast']=r['forecast'].values\n",
    "        up3[label][item].loc[r.index,'std']=r['std'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "up3['Reported Count']['BI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tp3\n",
    "up3 = tp3.swapaxes('items','major_axis')\n",
    "print up3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now output the forecast to the horizon = 1 key = 'forecast'\n",
    "# reindex \n",
    "up3 = tp3.swapaxes('items','major_axis')\n",
    "print np.shape(up3)\n",
    "for label in up3.labels:\n",
    "    for item in up3[label].items:\n",
    "        r = up3[label][item].apply(lambda x: make_interpolated_forecasts(x),axis=1)\n",
    "        #up3[label][item].loc[r.index,'forecast']=r['forecast'].values\n",
    "        #up3[label][item].loc[r.index,'std']=r['std'].values\n",
    "        for index_cov in r.index:\n",
    "            #append a new dictionary level\n",
    "            detail3[1][label]['metrics'][item][index_cov]['forecast'] = {'values':r.ix[index_cov]['forecast'],'std':r.ix[index_cov]['std'] }\n",
    "#\n",
    "#detail3[1][label]['metrics']\n",
    "#detail4['ALABAMA']['metrics']['Severity']['Property']['forecast'] = {'values':[], 'std': []}#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3[1]['MICHIGAN']['metrics']['Pending Count']['PIP']['forecast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overview3[1]['metrics']['Pending Count']['PIP']['MICHIGAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(np.isnan(up3['MICHIGAN']['Pending Count'].ix['MPC'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1 = tp3['KANSAS']['BI'].apply(lambda x: make_interpolated_forecasts(x),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_array = tp3['KANSAS']['PIP'].ix['Severity'].values\n",
    "my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = pd.DataFrame.from_dict(detail2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ilDET = pd.DataFrame.from_dict(dd.ix['ILLINOIS'][1])\n",
    "ilDET.ix['Indemnity']['metrics']['Injury'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputfile = 'base_CENT_01_2015.csv'\n",
    "data = pd.read_csv(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = data.STATE.unique()\n",
    "coverages = data.COVERAGE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### function to initialize API data structure containers\n",
    "def initialize_CENT_api_containers(coverages, variables_of_interest, states, start_year=2007, start_month=0):\n",
    "    # overview_data is of the format overview_data[metric][variable][coverage][state]\n",
    "\n",
    "    overview_data = {\"startYear\": start_year,\n",
    "                     \"startMonth\": start_month,\n",
    "                     \"metrics\": {}}\n",
    "\n",
    "    for variable in variables_of_interest:\n",
    "        overview_data[\"metrics\"][variable] = {}\n",
    "        for coverage in coverages:\n",
    "            overview_data[\"metrics\"][variable][coverage] = {}\n",
    "            for state in states:\n",
    "                overview_data[\"metrics\"][variable][coverage][state] = []\n",
    "\n",
    "    # detail_data is of the format of detail_data[state][\"metrics\"][variable][coverage]\n",
    "    detail_data = {}\n",
    "    for state in states:\n",
    "        detail_data[state] = {\"startYear\": start_year,\n",
    "                              \"startMonth\": start_month,\n",
    "                              \"metrics\": {}}\n",
    "        for variable in variables_of_interest:\n",
    "            detail_data[state][\"metrics\"][variable] = {}\n",
    "            for coverage in coverages:\n",
    "                detail_data[state][\"metrics\"][variable][coverage] = {}\n",
    "\n",
    "    return (detail_data, overview_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3[1]['ALABAMA']['metrics']['Severity']['Property'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail4 = detail3[1]\n",
    "detail4['ALABAMA']['metrics']['Severity']['Property']['forecast'] = {'values':[], 'std': []}#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail4['ALABAMA']['metrics']['Severity']['Property']['forecast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(overview3[1]['metrics']['Pending Count']['Property']['ALABAMA'])\n",
    "plot(overview3[6]['metrics']['Pending Count']['Property']['ALABAMA'])\n",
    "plot(overview3[12]['metrics']['Pending Count']['Property']['ALABAMA'])\n",
    "plot(overview3[12]['metrics']['Severity']['BI']['ALABAMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3[1]['ALABAMA']['metrics']['Pending Count']['Property']['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index, value = max(enumerate(overview3[1]['metrics']['Severity']['UBI']['ALASKA']), key=operator.itemgetter(1))\n",
    "index,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in overview3[1]['metrics']['Severity']['MPC'].iteritems():\n",
    "    print key, max(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing with getting topK values from the dictonary\n",
    "* option 1 is that I only care about the most recent timeframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to get only most recent values:\n",
    "list_1 = []\n",
    "key_1 = []\n",
    "maxvalue = 0.0\n",
    "minvalue = 0.0\n",
    "for key,value in overview3[12]['metrics']['Severity']['BI'].iteritems():\n",
    "    last_value = value[-1]\n",
    "    if last_value > maxvalue:\n",
    "        maxvalue = last_value\n",
    "        maxkey = key\n",
    "    elif last_value < minvalue:\n",
    "        minvalue = last_value\n",
    "        minkey = key\n",
    "        \n",
    "print minkey, minvalue, '\\t||\\t', maxkey,maxvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_max_min_values(overview_dict,time_horizon,variable,coverage):\n",
    "    # to get only most recent values:\n",
    "    maxvalue = 0.0\n",
    "    minvalue = 0.0\n",
    "    for key,value in overview_dict[time_horizon]['metrics'][variable][coverage].iteritems():\n",
    "        try :\n",
    "            last_value = value[-1]\n",
    "            if last_value > maxvalue:\n",
    "                maxvalue = last_value\n",
    "                maxkey = key\n",
    "            elif last_value < minvalue:\n",
    "                minvalue = last_value\n",
    "                minkey = key\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "    print minkey, minvalue, '\\t||\\t', maxkey,maxvalue\n",
    "    return(minkey,minvalue,maxkey,maxvalue)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key,value in overview3[12]['metrics']['Severity']['BI'].iteritems():\n",
    "    print key, max(value),min(value),len(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_max_min_values(overview3,12,'Severity','MPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for variable in variables_of_interest:\n",
    "    for coverage in coverages:\n",
    "        print coverage, variable\n",
    "        get_max_min_values(overview3,1,variable,coverage)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(overview3[1]['metrics']['Overall']['WBI']['ARKANSAS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TOP K from a dictionary\n",
    "def add_value_to_dictionary(overview_dict,time_horizon,variable,coverage,joined_dictionary, index_to_add = -1):\n",
    "    # create a tuple out of the various keys\n",
    "\n",
    "    for key,value in overview_dict[time_horizon]['metrics'][variable][coverage].iteritems():\n",
    "        long_key = (time_horizon,variable,coverage,key)\n",
    "        try :\n",
    "            value_to_add = value[index_to_add]\n",
    "            joined_dictionary[long_key]=value_to_add\n",
    "        except IndexError:\n",
    "            print key, \" lacks values.\"\n",
    "            pass\n",
    "    return joined_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nd = get_last_value(overview3,1,'Indemnity','WBI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened_overview_dict = {}\n",
    "for variable in variables:\n",
    "    for coverage in coverages:\n",
    "        #print coverage, variable\n",
    "        flattened_overview_dict = add_value_to_dictionary(overview3,1,variable,coverage,flattened_overview_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(flattened_overview_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_top_bottom_K(flattened_dictionary, K=10):\n",
    "    sorted_dictionary = sorted(flattened_dictionary, key=flattened_dictionary.get)\n",
    "    my_column_list = ['horizon','Variable','Coverage','State','Value']\n",
    "    # create an empty dataframe\n",
    "    table_df = pd.DataFrame(data=np.zeros((0,len(my_column_list))), columns=my_column_list)\n",
    "    \n",
    "    table_size = len(table_df)\n",
    "    for key in sorted_dictionary[:K]:\n",
    "        key_list = [item for item in key]\n",
    "        key_list.append(flattened_dictionary[key])\n",
    "        table_df.loc[table_size]=key_list\n",
    "        table_size+=1\n",
    "        #print key_list\n",
    "    \n",
    "    for key in sorted_dictionary[-1*K:]:\n",
    "        key_list = [item for item in key]\n",
    "        key_list.append(flattened_dictionary[key])\n",
    "        table_df.loc[table_size]=key_list\n",
    "        table_size+=1\n",
    "        #print key_list\n",
    "    \n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_df.append(table_df,tmp2)\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp1 = flattened_overview_dict.keys()[0]\n",
    "tmp2 = [t for t in tmp1]\n",
    "tmp2.append(flattened_overview_dict[tmp1])\n",
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_df = return_top_bottom_K(flattened_overview_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overview3[1]['table']={}\n",
    "overview3[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overview3[1]['table'] = table_df[['Variable','Coverage','State','Value']].values.tolist()\n",
    "overview3[1]['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_df[['Variable','Coverage','State','Value']].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_overview = sorted(flattened_overview_dict, key=flattened_overview_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(0,len(nd)),nd.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(nd.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looking at missing value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "injson = '../../cent1/out2/hold.json'\n",
    "for line in open(injson):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level1 = pd.read_json(injson)\n",
    "len(level1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level2 = level1.loc['metrics',6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(level2['Overall']['MPC']['WEST VIRGINIA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
