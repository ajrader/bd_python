{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Add predictions \n",
    "* adjust the event horizon beyond 1 (want to use 6 and 12)\n",
    "* Auto identify max lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, every state has 11 coverage types. There are 5 main metrics for each coverage type. There are 69 months of data associated with each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd /home/kesj/work/claiment/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile= 'base2_CENT_02_2015.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import some of the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_norm(series):\n",
    "    return(series - series.mean())/series.std(ddof=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_cent_file(inputfile, x='STATE', y='COVERAGE', t='YEAR',\n",
    "                         important_variables=[], omit_column=[], hdfs_flag=False, create_overall_variable=False):\n",
    "    \"\"\"\n",
    "    A function that takes an input csv file of many timeseries data and processes it into the\n",
    "    format that we want for CENT analysis.\n",
    "\n",
    "    hdfs_flag indicates if the based file is in hdfs or not.\n",
    "\n",
    "    the input parameters:\n",
    "    data -- the flat 2d datashape\n",
    "    important_variables -- the column name to use for the 'items' -- default is 'STATE'\n",
    "    y -- the column name to use for the 'minor-axis' -- default is 'COVERAGE'\n",
    "    t -- the column name to use for the 'major-axis' <- where timeseres go; default is 'YEAR'\n",
    "    z -- the column title to use for 'labels' (these are the columns you can loop over)\n",
    "\n",
    "    :returns\n",
    "      my_grouped_df ( a grouped hierarchical data frame ready to be printed as key,value pairs\n",
    "      important_variables ( the list of metrics used in the calculation)\n",
    "      xvals (the list of the first major axis -- i.e. the states)\n",
    "      yvals ( the list of the minor axis -- i.e. the coverages)\n",
    "    \"\"\"\n",
    "\n",
    "    # if hdfs_flag:\n",
    "\n",
    "    data = pd.read_csv(inputfile)\n",
    "    # check format of a couple input formats. we want important_variables, omit_column to be lists\n",
    "\n",
    "    if len(omit_column) != 0:  # omit a list of columns if so given\n",
    "        data = data.drop(omit_column, axis=1, inplace=True)\n",
    "\n",
    "    print \"input dataframe has the dimensions of {0}\".format(np.shape(data))\n",
    "    print \"the column titles are: {0}\".format(data.columns)\n",
    "\n",
    "    # check that x,y,t,variables_of_interest are in columns\n",
    "    # create a set for the singletons:\n",
    "    s1 = set([x, y, t])\n",
    "\n",
    "    # create a set for all columns\n",
    "    scolumns = set(data.columns.values)\n",
    "    # test that s1 is subset of scolumns\n",
    "    sdiff1 = s1 - scolumns\n",
    "    if len(sdiff1) != 0:\n",
    "        # have to fix the s1 input files\n",
    "        print \"We have a problem because the columns you want to pivot on are not present\"\n",
    "        print sdiff1, \" is missing from \", scolumns\n",
    "        return 0\n",
    "    else:\n",
    "        # convert the year to a date-time object\n",
    "        data[t] = pd.to_datetime(data[t])\n",
    "\n",
    "    if len(important_variables) == 0:  # define these based upon the input data if not defined\n",
    "        important_variables = list(scolumns - s1)\n",
    "        print \"important variables are determined by the input data.\"\n",
    "        # print \"{0} are the important variables determined based upon the input data\".format(important_variables)\n",
    "\n",
    "\n",
    "        # option to generate an overall variable that is the normalized average of the other variables of interest\n",
    "    if create_overall_variable:\n",
    "        data['Overall'] = data.groupby([x, y]).transform(series_norm)[important_variables].sum(axis=1)\n",
    "        important_variables += ['Overall']\n",
    "\n",
    "    # generate list of unique values for the 3 input parameters: x,y,t\n",
    "    print s1\n",
    "\n",
    "    xvals = data[x].unique()\n",
    "    yvals = data[y].unique()\n",
    "    tvals = data[t].unique()\n",
    "\n",
    "    print len(xvals), len(yvals), len(tvals), len(important_variables)\n",
    "    # to do: check that all columns are used\n",
    "    # to do: reshape the 2d data frame into a multi-index, hierarchical df\n",
    "    my_grouped_data = data.groupby((x, y))  # set_index([x,y,t]\n",
    "    return (my_grouped_data, important_variables, xvals, yvals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def interpolate_CENT_predictions(input_list,round_digit=2):\n",
    "    \"\"\"Returns list of interpolated values, round to the number of places given\n",
    "       strictly assumes spacing of 1 month, 6 month and 12 month.\n",
    "    \"\"\"\n",
    "    interpolated_predictions=[]\n",
    "    forecast_1_month = input_list[0]\n",
    "    forecast_6_month = input_list[1]\n",
    "    forecast_12_month = input_list[2]\n",
    "    # Add 1 month prediction\n",
    "    interpolated_predictions.append(round(forecast_1_month,round_digit))\n",
    "\n",
    "    # Interpolate values between 1 month and 6 months\n",
    "    one_six_gap = (forecast_6_month-forecast_1_month)/5.0\n",
    "    for e in range(1,5):\n",
    "        interpolated_predictions.append(round(forecast_1_month+e*one_six_gap,round_digit))\n",
    "\n",
    "    # Add 6 month prediction\n",
    "    interpolated_predictions.append(round(forecast_6_month,round_digit))\n",
    "\n",
    "    # Interpolate values between 6 months and 12 months\n",
    "    six_twelve_gap = (forecast_12_month-forecast_6_month)/6.0\n",
    "    for e in range(1,6):\n",
    "        interpolated_predictions.append(round(forecast_6_month+e*six_twelve_gap,round_digit))\n",
    "\n",
    "    # Add 12 month prediction\n",
    "    interpolated_predictions.append(round(forecast_12_month,round_digit))\n",
    "    return interpolated_predictions\n",
    "\n",
    "def make_interpolated_forecasts(x):\n",
    "    \"\"\"\n",
    "    function to generate the interpolated values from a list of values and errors\n",
    "    :param x: list of 3 values and 3 errors in the format of [v1,e1,v2,e2,v3,e3]\n",
    "    :return: series of forecasted_values and forecasted_errors for the 12 months\n",
    "    assuming v1 is 1 month value, v2 is 6 month value and v3 is 12 month valaue\n",
    "    \"\"\"\n",
    "    # if the input values are nan, just return an empty list\n",
    "    if sum(np.isnan(x)) == len(x) :\n",
    "        forecast_values = []\n",
    "        forecast_errors = []\n",
    "    else:\n",
    "        values = x[::2]\n",
    "        errors = x[1::2]\n",
    "        forecast_values = interpolate_CENT_predictions(values)\n",
    "        forecast_errors = interpolate_CENT_predictions(errors)\n",
    "\n",
    "    return pd.Series(dict(forecast=forecast_values, std=forecast_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### function to initialize API data structure containers\n",
    "def initialize_CENT_api_containers(coverages, variables_of_interest, states, time_horizons, start_year=2007, start_month=0):\n",
    "    \"\"\"\n",
    "    Function for initializing the CENT api containers\n",
    "    :param coverages: list of possible coverages\n",
    "    :param variables_of_interest: list of variables of interest\n",
    "    :param states: list of states\n",
    "    :param time_horizons --> list of time horizons\n",
    "    :param start_year: starting year\n",
    "    :param start_month: starting month\n",
    "    :return: detail_data, overview_data, and panel\n",
    "    \"\"\"\n",
    "\n",
    "    # create a 4D panel data frame for the forecast data\n",
    "    forecast_dimension = []\n",
    "    for time in time_horizons:\n",
    "        forecast_dimension.append('v'+str(time))\n",
    "        forecast_dimension.append('e'+str(time))\n",
    "\n",
    "    my_panel = pd.Panel4D(labels=states, items=coverages,major_axis=variables_of_interest,minor_axis=forecast_dimension)\n",
    "\n",
    "    # overview_data is of the format overview_data[horizon][metric][variable][coverage][state]\n",
    "    overview_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        overview_data[horizon] = {\"startYear\": start_year,\n",
    "                                  \"startMonth\": start_month,\n",
    "                                  \"metrics\": {}}\n",
    "        for variable in variables_of_interest:\n",
    "            overview_data[horizon][\"metrics\"][variable] = {}\n",
    "            for coverage in coverages:\n",
    "                overview_data[horizon][\"metrics\"][variable][coverage] = {}\n",
    "                for state in states:\n",
    "                    overview_data[horizon][\"metrics\"][variable][coverage][state] = []\n",
    "\n",
    "    # detail_data is of the format of detail_data[horizon][state][\"metrics\"][variable][coverage]\n",
    "    detail_data = {}\n",
    "    for horizon in time_horizons:\n",
    "        detail_data[horizon] = {}\n",
    "        for state in states:\n",
    "            detail_data[horizon][state] = {\"startYear\": start_year,\n",
    "                              \"startMonth\": start_month,\n",
    "                              \"metrics\": {}}\n",
    "            for variable in variables_of_interest:\n",
    "                detail_data[horizon][state][\"metrics\"][variable] = {}\n",
    "                for coverage in coverages:\n",
    "                    detail_data[horizon][state][\"metrics\"][variable][coverage] = {}\n",
    "\n",
    "\n",
    "    return (detail_data, overview_data, my_panel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_models(variables_of_interest,grouped_df,my_time_horizons,my_panel,detail_data,overview_data):\n",
    "    # arrays for the errors\n",
    "    error_dict = {}\n",
    "    for time_horizon in my_time_horizons:\n",
    "        error_dict[time_horizon] = []\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    rnd_digits = defaultdict(int)\n",
    "    for v in variables_of_interest:\n",
    "        if v.endswith('Count'):\n",
    "            rnd_digits[v]=0\n",
    "        elif v.endswith('CNT'):\n",
    "            rnd_digits[v]=0\n",
    "        else:\n",
    "            rnd_digits[v]=2\n",
    "    # display progress\n",
    "    total_groups = len(grouped_df.groups)\n",
    "    ngrps=0\n",
    "    for g in grouped_df.groups:\n",
    "        state = g[0]\n",
    "        coverage = g[1]\n",
    "        focused_data = grouped_df.get_group(g)\n",
    "        ngrps+=1\n",
    "        print state, coverage,\"......\",ngrps,\" out of \",total_groups\n",
    "        for time_horizon in my_time_horizons:\n",
    "            X, y_data, x, undiff = time_series_to_cross_section(focused_data[variables_of_interest],\n",
    "                                                        forecast_horizon=time_horizon,\n",
    "                                                        max_fair_lags=13,\n",
    "                                                        seasonal_factor=12)\n",
    "            for variable in variables_of_interest:\n",
    "                # Account for time series with no variation\n",
    "                state_actuals = focused_data[variable]\n",
    "                std_of_actuals = state_actuals.std(ddof=1)\n",
    "                # code to deal with forecast data\n",
    "                value_col = 'v'+str(time_horizon)\n",
    "                error_col = 'e'+str(time_horizon)\n",
    "                if std_of_actuals == 0:\n",
    "                    overview_data[time_horizon][\"metrics\"][variable][coverage][state] = [0]*len(state_actuals)\n",
    "                    detail_data[time_horizon][state][\"metrics\"][variable][coverage] = {\"stddev\": 0,\n",
    "                                                                     \"actual\": list(state_actuals),\n",
    "                                                                     \"predicted\" : list(state_actuals)}\n",
    "                    my_panel[state][coverage].loc[variable,value_col] = 0#point_forecast[0]\n",
    "                    my_panel[state][coverage].loc[variable,error_col] = 0#point_forecast[1]                                                     \"predicted\": list(state_actuals)}\n",
    "                    continue\n",
    "            \n",
    "                # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "                final_model, variables_in_model = optimized_rf(X, y_data[variable],\n",
    "                                                       variable_importance_n_estimators=50,\n",
    "                                                       n_estimators_in_grid_search=20,\n",
    "                                                       number_of_important_variables_to_use_options=[6,8],#[6],#, 8, 10, 12, 15, 20],\n",
    "                                                       variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                       n_estimators_to_retrain_best_model=50,\n",
    "                                                       verbose=False, n_random_models_to_test=6,#3,\n",
    "                                                       charts=False, n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "                state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "                state_residuals = focused_data[variable] - state_predictions\n",
    "                data_consumed_for_model = sum(state_residuals == 0)\n",
    "                std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "                state_std_residuals = state_residuals/std_of_residuals\n",
    "                abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "                MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()         \n",
    "\n",
    "                #  point forecast for this variable and time horizon\n",
    "                forecast = model_forecast(x,variable, final_model, variables_in_model,undiff)\n",
    "                #convert list of values to 2 floating point digits or 0 for counts\n",
    "                rnd_size = rnd_digits[variable]\n",
    "                if rnd_size > 0 :\n",
    "                    state_actuals = [ round(elem,rnd_size) for elem in list(state_actuals)]\n",
    "                    state_predictions = [ round(elem,rnd_size) for elem in list(state_predictions)]\n",
    "                else:\n",
    "                    state_actuals = [ int(round(elem,0)) for elem in list(state_actuals)]\n",
    "                    state_predictions = [ int(round(elem,0)) for elem in list(state_predictions)]\n",
    "            \n",
    "                overview_values = [round(elem,2) for elem in list(state_std_residuals.values)]\n",
    "        \n",
    "                # Update primary data structures with model results\n",
    "                overview_data[time_horizon][\"metrics\"][variable][coverage][state] = overview_values\n",
    "                detail_data[time_horizon][state][\"metrics\"][variable][coverage] = {\"stddev\": round(std_of_residuals,5),\n",
    "                                                                 \"actual\": state_actuals,\n",
    "                                                                 \"predicted\": state_predictions}\n",
    "                MSE = round(MSE,5)\n",
    "                abs_average_error = round(abs_average_error,5)\n",
    "                my_panel[state][coverage].loc[variable,value_col] = round(forecast,5)\n",
    "                my_panel[state][coverage].loc[variable,error_col] = round(std_of_residuals,5)\n",
    "                error_dict[time_horizon].append([MSE,abs_average_error])\n",
    "                \n",
    "        #print '********************************************************************************************'\n",
    "    #print total_absolute_average_error / float(models_built), total_mean_squared_error / float(models_built)\n",
    "    #print models_built\n",
    "    #print '********************************************************************************************'\n",
    "    return(my_panel,detail_data,overview_data,error_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_overall_error(error_dict):\n",
    "    for time in error_dict.keys():\n",
    "        nmodels = len(error_dict[time])\n",
    "        total_mse = 0.0\n",
    "        total_abserr = 0.0\n",
    "        n=0\n",
    "        for model_error in error_dict[time]:\n",
    "            mse = model_error[0]\n",
    "            abserr = model_error[1]\n",
    "            if not np.isnan(mse) and  not np.isnan(abserr):\n",
    "                total_mse += mse\n",
    "                n+=1\n",
    "                total_abserr += abserr\n",
    "            \n",
    "            #if time == 1:\n",
    "            #    print mse, abserr, n\n",
    "        \n",
    "        print '********************************************************************************************'\n",
    "        print time, total_abserr / float(n), total_mse / float(n)\n",
    "        print n, nmodels\n",
    "        print '********************************************************************************************'    \n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CENT_rolling_12(df, columns_for_r12=[\"Reported Count\", \"Paid Count\", \"Pending Count\", \"Indemnity\", \"Severity\",\"CIF\",\"ALAE\",\"CWP\",\"OIE_CNT\",\"SUIT_CNT\"]):\n",
    "    vvv = df.copy()\n",
    "    result_list_of_df = []\n",
    "\n",
    "    for state, state_data in vvv.groupby(\"STATE\"):\n",
    "        for coverage, coverage_data in state_data.groupby(\"COVERAGE\"):\n",
    "            for metrics in columns_for_r12:\n",
    "                coverage_data[metrics] = pd.rolling_mean(coverage_data[metrics], window=12)\n",
    "            result_list_of_df.append(coverage_data)\n",
    "\n",
    "    vvv = pd.concat(result_list_of_df)\n",
    "    vvv.dropna(inplace=True)\n",
    "    return vvv\n",
    "\n",
    "#CENT_rolling_12(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOP K from a dictionary\n",
    "def add_value_to_dictionary(overview_dict,time_horizon,variable,coverage,joined_dictionary, index_to_add = -1):\n",
    "    \"\"\"\n",
    "    Function to convert the overview dictionary data into a flattened_dictionary with the keys joined as a tuple.\n",
    "    :param overview_dict: input dictionary of overview_data\n",
    "    :param time_horizon: specific time_horizon\n",
    "    :param variable: specific variable (of interest)\n",
    "    :param coverage: specific coverage\n",
    "    :param joined_dictionary: dictionary of flattened values of last data.\n",
    "    :param index_to_add: which value to add to this flattened dictionary; default is most recent value.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # if no input dictionary given, create one.\n",
    "    if joined_dictionary == None:\n",
    "        joined_dictionary = {}\n",
    "\n",
    "    for key,value in overview_dict[time_horizon]['metrics'][variable][coverage].iteritems():\n",
    "        long_key = (time_horizon,variable,coverage,key)\n",
    "        try :\n",
    "            value_to_add = value[index_to_add]\n",
    "            joined_dictionary[long_key]=value_to_add\n",
    "        except IndexError:\n",
    "            #print key, \" lacks values.\"\n",
    "            # skip over cases where there are no values.\n",
    "            pass\n",
    "    return joined_dictionary\n",
    "\n",
    "def return_top_bottom_K(flattened_dictionary, my_column_list = ['horizon','Variable','Coverage','State','Value'], K=10):\n",
    "    \"\"\"\n",
    "    Function to return the top and bottom K from a flattened dictionary\n",
    "    :param flattened_dictionary: flattened dictionary where the key is the 1st N-1 elements of my_column_list and the\n",
    "    value is the last one.\n",
    "    :param K: number of elements from top and bottom to return\n",
    "    :return: table_df, a pandas data frame of the topK and bottomK results.\n",
    "    \"\"\"\n",
    "    sorted_dictionary = sorted(flattened_dictionary, key=flattened_dictionary.get)\n",
    "\n",
    "    # create an empty dataframe\n",
    "    table_df = pd.DataFrame(data=np.zeros((0,len(my_column_list))), columns=my_column_list)\n",
    "\n",
    "    table_size = len(table_df)\n",
    "    for key in sorted_dictionary[:K]:\n",
    "        key_list = [item for item in key]\n",
    "        key_list.append(flattened_dictionary[key])\n",
    "        table_df.loc[table_size]=key_list\n",
    "        table_size+=1\n",
    "        #print key_list\n",
    "\n",
    "    for key in sorted_dictionary[-1*K:]:\n",
    "        key_list = [item for item in key]\n",
    "        key_list.append(flattened_dictionary[key])\n",
    "        table_df.loc[table_size]=key_list\n",
    "        table_size+=1\n",
    "        #print key_list\n",
    "\n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_up_results_serial(my_panel,detail_data,overview_data,time_horizon_list=[1,6,12]):\n",
    "    # switch the ordering of the axes to match detail_data ordering\n",
    "    altered_panel = my_panel.swapaxes('items','major_axis')\n",
    "    for label in altered_panel.labels:\n",
    "        for item in altered_panel[label].items:\n",
    "            forecasted = altered_panel[label][item].apply(lambda x: make_interpolated_forecasts(x),axis=1)\n",
    "    #        altered_panel[label][item].loc[forecasted.index,'forecast']=r['forecast'].values\n",
    "    #        altered_panel[label][item].loc[forecasted.index,'std']=r['std'].values\n",
    "            ## now output the forecast to the horizon = 1 key = 'forecast'\n",
    "            for index_cov in forecasted.index:\n",
    "            #append a new dictionary level for 1st time_horizon\n",
    "                detail_data[1][label]['metrics'][item][index_cov]['forecast'] = {\n",
    "                    'values':forecasted.ix[index_cov]['forecast'],\n",
    "                    'std':forecasted.ix[index_cov]['std'] }\n",
    "\n",
    "    # augment the overview_data with the table info for topK and bottomK\n",
    "    # create a table of the overview results\n",
    "    states = list(my_panel.labels)\n",
    "    coverages = list(my_panel.items)\n",
    "    importance_variables = list(my_panel.major_axis)\n",
    "\n",
    "    for time in time_horizon_list:\n",
    "        flattened_overview_dict = {}\n",
    "        for variable in importance_variables:\n",
    "            for coverage in coverages:\n",
    "                flattened_overview_dict = add_value_to_dictionary(overview_data, time, variable, coverage,\n",
    "                                                                  flattened_overview_dict)\n",
    "\n",
    "        table_df = return_top_bottom_K(flattened_overview_dict)\n",
    "        # sort by value;\n",
    "        table_df.sort(['Value'],inplace=True,ascending=False)\n",
    "        # append this information to overview_data\n",
    "        overview_data[time]['table'] = table_df[['State','Coverage','Variable','Value']].values.tolist()\n",
    "\n",
    "    # check that tgtdir ends in '/'; append if not\n",
    "    #if not tgtdir.endswith('/'):\n",
    "    #    tgtdir += '/'\n",
    "\n",
    "    #overview_file = tgtdir + 'overview.json'\n",
    "    #detail_file = tgtdir + 'detail.json'\n",
    "    overview_file = 'overview.json'#_',+fname+'.json'\n",
    "    detail_file = 'detail.json'#_'+fname+'.json'\n",
    "    \n",
    "    # check on existence of the directory and create it if missing\n",
    "    #make_sure_path_exists(tgtdir)\n",
    "    with open(overview_file, 'w') as outfile:\n",
    "        json.dump(overview_data, outfile)\n",
    "\n",
    "    with open(detail_file, 'w') as outfile:\n",
    "        json.dump(detail_data, outfile)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def number_of_nulls(series):\n",
    "    return series.isnull().sum()\n",
    "\n",
    "def are_the_last_n_values_null(series, n):\n",
    "    \"\"\"\n",
    "    series: A selection of a pandas dataframe\n",
    "    n: If the last n values are null, returns True\n",
    "    \"\"\"\n",
    "    number_of_null_values = number_of_nulls(series)\n",
    "    return number_of_null_values == series[-number_of_null_values:].isnull().sum()\n",
    "\n",
    "def n_of_missing_last_values(series):\n",
    "    number_of_null_values = number_of_nulls(series)\n",
    "    if number_of_null_values == 0:\n",
    "        return 0\n",
    "    while True:\n",
    "        if are_the_last_n_values_null(series, number_of_null_values):\n",
    "            return number_of_null_values\n",
    "        number_of_null_values -= 1\n",
    "        if number_of_null_values < 0:\n",
    "            return \"Error\"\n",
    "        \n",
    "def missing_last_values(df):\n",
    "    \"\"\"Returns a Series of each variable with\n",
    "    the number of missing values at the end\n",
    "    \"\"\"\n",
    "    missing_last_values_df = pd.Series()\n",
    "    for variable in df.columns:\n",
    "        missing_last_values_df[variable] = n_of_missing_last_values(df[variable])\n",
    "    return missing_last_values_df\n",
    "\n",
    "def make_lag(series, n_lag):\n",
    "    \"\"\"OLD VERSION. Returns a series with specified lag\"\"\"\n",
    "    original_index = series.index\n",
    "    series = series[:-n_lag]\n",
    "    series.index = original_index[n_lag:]\n",
    "    series = series.reindex(original_index)\n",
    "    return series\n",
    "\n",
    "# def make_lag(series, n_lag):\n",
    "#     \"\"\"Returns a series with specified lag\"\"\"\n",
    "#     # Update index\n",
    "#     max_index_value = series.index[-1]\n",
    "#     index_to_add = [\"t%d\"%(e+1) for e in xrange(n_lag)]\n",
    "#     new_index = np.concatenate([series.index, index_to_add])\n",
    "    \n",
    "#     # Update values\n",
    "#     values_to_add = [np.nan]*n_lag\n",
    "#     new_values = np.concatenate([values_to_add, series.values])\n",
    "    \n",
    "#     return pd.Series(new_values, new_index)\n",
    "\n",
    "def difference_data(df, diff=1):\n",
    "    \"\"\"\n",
    "    df: pandas df\n",
    "        Must be sorted with the most recent data at the bottom of the \n",
    "        dataframe\n",
    "        \n",
    "    diff: integer\n",
    "        Default of 1 implies first difference. If 2, it will be a second \n",
    "        difference\n",
    "    \"\"\"        \n",
    "    data_old = df.copy()\n",
    "    df = df[:-diff]\n",
    "    df.index = data_old.index[diff:]\n",
    "    df = df.reindex(data_old.index)\n",
    "    return data_old - df\n",
    "\n",
    "def time_series_to_cross_section(df, forecast_horizon=1, \n",
    "                                 difference=True, \n",
    "                                 max_fair_lags=4, seasonal_factor=None):\n",
    "    \"\"\"\n",
    "    df: pandas df or path to csv file\n",
    "        There should be no missing values within the series. Missing\n",
    "        values at the end or beginning of the series are okay.\n",
    "    \n",
    "    forecast_horizon: integer\n",
    "        Specifies how far ahead to make the forecast from the most\n",
    "        recent NON-NULL value.\n",
    "        \n",
    "    difference: boolean\n",
    "        If True, all the data is differenced at the forecast_horizon.\n",
    "        For example, if the forecast horizon is 6, then t0 will be\n",
    "        subtracted from t6. The reason for this is if we forecast the \n",
    "        simple difference (t1 - t0), we will not be able to\n",
    "        undifference it.\n",
    "        \n",
    "    max_fair_lags: integer\n",
    "        Will use up to max_fair_lags lags of data. If you enter 6 lags\n",
    "        and the last 4 data points of a series are missing, only\n",
    "        lag 6 will be included (lag 5 is only fair to predict the \n",
    "        most recent y value)\n",
    "        \n",
    "    seasonal_factor: integer\n",
    "        Adds a column to the dataset that is 0, 1, ... , \n",
    "        seasonal_factor-1 repeating.\n",
    "        \n",
    "    Returns:\n",
    "    output_df: pandas df\n",
    "        A fair design matrix to use to predict any of the columns of\n",
    "        the output_y_df\n",
    "        \n",
    "    output_y_df: pandas df\n",
    "        Contains a df with fair y values to use for every column in \n",
    "        the df. Simply select the column data and use it as y in any\n",
    "        ML algorithm paired with the output_df as X.\n",
    "        \n",
    "    x_to_predict: pandas series\n",
    "        Once you build your model and want to predict the next point,\n",
    "        use this data in the predict method of the model. This point \n",
    "        will correspond to the forecast horizon relative to the most \n",
    "        recent valid data point.\n",
    "        \n",
    "    undiff_x_predict_function: function\n",
    "        The predict series of the model will return the expected change\n",
    "        and not the absolute value of the prediction. Use this function\n",
    "        to undifference the prediction. The input is the differenced\n",
    "        prediction and the column name.\n",
    "    \"\"\"\n",
    "    # Create a df that specifies the most recent fair lag\n",
    "    # that can be used for each column\n",
    "    missing_last_values_df = missing_last_values(df)\n",
    "    most_recent_fair_lag = missing_last_values_df + forecast_horizon\n",
    "    \n",
    "    # Specify output dataframe\n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    # Add all fair lags to dataframe\n",
    "    for var in df.columns:\n",
    "        min_lag = most_recent_fair_lag[var]\n",
    "        # print var, min_lag, max_fair_lags\n",
    "        while min_lag <= max_fair_lags:\n",
    "            output_df[\"%s_lag%d\" % (var, min_lag)] = make_lag(df[var], \n",
    "                                                              min_lag)\n",
    "            min_lag += 1\n",
    "    \n",
    "    # Differenced design matrix\n",
    "    if difference:\n",
    "        output_df = difference_data(output_df, forecast_horizon)\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    output_df.dropna(inplace=True)\n",
    "\n",
    "    # Align df of y values so that it is fair for each variable\n",
    "    output_y_df = df.copy()\n",
    "    for var in df.columns:\n",
    "        recent_missing = missing_last_values_df[var]\n",
    "        if recent_missing > 0:\n",
    "            output_y_df[var] = np.concatenate([list([np.nan])*recent_missing, \n",
    "                                              output_y_df[var][:-recent_missing]])\n",
    "    \n",
    "    if difference:\n",
    "        undiff_output_y_df = output_y_df.copy()\n",
    "        output_y_df = difference_data(output_y_df, forecast_horizon)\n",
    "        output_y_df = output_y_df[forecast_horizon:]\n",
    "        \n",
    "    # Calculate number of rows to drop from output_y_df\n",
    "    rows_to_drop = output_y_df.shape[0] - output_df.shape[0] + forecast_horizon # Adjusts for t1-tn\n",
    "    output_y_df = output_y_df[rows_to_drop:]\n",
    "    \n",
    "    # Add seasonal factor\n",
    "    if seasonal_factor:\n",
    "        output_df[\"seasonality\"] = np.array(range(seasonal_factor)*(output_df.shape[0]/seasonal_factor+1))[:output_df.shape[0]]\n",
    "    \n",
    "    # Copy the last line as the x to predict\n",
    "    x_to_predict = pd.Series(output_df[-1:].values[0], index=output_df.columns)\n",
    "    \n",
    "    # Drop the last rows of output_df\n",
    "    output_df = output_df[:-forecast_horizon]\n",
    "    \n",
    "    # Make function that undifferences the data\n",
    "    def undifference(differenced_data, column_name, include_initial_points=False):\n",
    "        \"\"\"\n",
    "        Undifferences either the in-sample predictions or the \n",
    "        out-of-sample prediction.\n",
    "        \n",
    "        differenced_data: pandas dataframe or pandas series\n",
    "        If dataframe, the column_name must be the same between the original \n",
    "        dataset and the original data. If series, it doesn't matter\n",
    "        \n",
    "        column_name: string\n",
    "        The name of the column to undifference\n",
    "        \n",
    "        include_initial_points: Boolean, optional (default=False)\n",
    "        If True, would prepend the series with the actual values. These\n",
    "        values should not be considered to be fair predictions.\n",
    "        \n",
    "        Returns\n",
    "        Undifferenced series or point.\n",
    "        \"\"\"\n",
    "        original_series = df[column_name].values\n",
    "        \n",
    "        # If only a single data point is provided, return single \n",
    "        # undifferenced value\n",
    "        try:\n",
    "            differenced_data = float(differenced_data)\n",
    "        except TypeError:\n",
    "            pass\n",
    "        \n",
    "        if type(differenced_data) in [int, float] or len(differenced_data) == 1:\n",
    "            if type(differenced_data) == list:\n",
    "                differenced_data = differenced_data[0]\n",
    "            return original_series[-1] + differenced_data\n",
    "        \n",
    "        if len(differenced_data.shape) == 2:\n",
    "            differenced_data = differenced_data[column_name]\n",
    "        len_of_differenced_data = differenced_data.shape[0]\n",
    "        len_of_original_series = original_series.shape[0]\n",
    "        size_diff = len_of_original_series - len_of_differenced_data\n",
    "        \n",
    "        # Drop rows that were removed due to lags\n",
    "        new_series = original_series[(size_diff - forecast_horizon):-forecast_horizon]\n",
    "        \n",
    "        undifferenced_series = new_series + differenced_data\n",
    "        \n",
    "        if include_initial_points:\n",
    "            undifferenced_series = np.concatenate((original_series[:size_diff], undifferenced_series), \n",
    "                                                  axis=0)\n",
    "        return undifferenced_series\n",
    "    \n",
    "    return output_df, output_y_df, x_to_predict, undifference\n",
    "    \n",
    "    \n",
    "def sort_variables_by_importance(X, y, n_estimators=100, \n",
    "                                 max_features=\"auto\", chart=True,\n",
    "                                 random_state=42, n_jobs=1):\n",
    "    \"\"\"Returns an array of column names sorted by rf importance\"\"\"\n",
    "    model = RandomForestRegressor(n_estimators, \n",
    "                                  max_features=max_features, \n",
    "                                  random_state=random_state, \n",
    "                                  n_jobs=n_jobs)\n",
    "    model.fit(X, y)\n",
    "    important_variables = pd.Series(model.feature_importances_, \n",
    "                                    index=X.columns)\n",
    "    important_variables.sort()\n",
    "    if chart:\n",
    "        important_variables.plot(kind=\"barh\", figsize=(5,15))\n",
    "        plt.show()\n",
    "    important_variables.sort(ascending=False)\n",
    "    return list(important_variables.index)\n",
    "\n",
    "\n",
    "def oob_randomized_rf_gridsearch(X, y, n_estimators=50, \n",
    "                                 n_models_to_test=10, \n",
    "                                 top_models_to_print=3, \n",
    "                                 trees_in_final_model=\"auto\",\n",
    "                                 verbose=True, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs=1):\n",
    "    \"\"\"\n",
    "    Performs a simple randomized gridsearch using the oob_errors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas df\n",
    "    \n",
    "    y: pandas Series\n",
    "    \n",
    "    n_estimators: int, optional (default=50)\n",
    "    Number of trees in random forest\n",
    "    \n",
    "    n_models_to_test: int, optional (default=10)\n",
    "    Number of unique parameter combinations to test\n",
    "    \n",
    "    top_models_to_print: int, optional (default=3)\n",
    "    Prints the parameter list of the top models selected\n",
    "    \n",
    "    trees_in_final_model: int,  \"auto\", or None, optional (default=\"auto\")\n",
    "    After finding the optimal model, retrains model with this many\n",
    "    trees. If \"auto\", is equal to n_estimators * n_models_to_test/2.\n",
    "    If None, doesn't train final model. Returns trained model list \n",
    "    instead.\n",
    "    \n",
    "    verbose: Boolean, optional (default=True)\n",
    "    Prints final trained model stats if True\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    Final trained model\n",
    "    \"\"\"\n",
    "    max_features_options = [.1, \"sqrt\", .25, .5, .75, 1.0]\n",
    "    min_samples_split_options = [2, 4, 6]\n",
    "    min_samples_leaf_options = [1, 2, 3]\n",
    "    \n",
    "    max_trials = len(max_features_options)*len(min_samples_split_options)*len(min_samples_leaf_options)\n",
    "    if n_models_to_test >= max_trials:\n",
    "        print \"Too many trials, n_models_to_test set to\", max_trials - 1\n",
    "        n_models_to_test = max_trials - 1\n",
    "    \n",
    "    def get_parameters():\n",
    "        max_features = np.random.choice(max_features_options, 1, p=[.2, .1, .1, .1, .1, .4])[0]\n",
    "        if max_features != \"sqrt\":\n",
    "            max_features = float(max_features)\n",
    "        min_samples_split = np.random.choice(min_samples_split_options, 1, p=[.7, .2, .1])[0]\n",
    "        min_samples_leaf = np.random.choice(min_samples_leaf_options, 1, p=[.7, .2, .1])[0]\n",
    "        return max_features, min_samples_split, min_samples_leaf\n",
    "    \n",
    "    parameters_tested = []\n",
    "    model_results = []\n",
    "    for trial in xrange(n_models_to_test):\n",
    "        # Ensure only unique parameter combinations are tested\n",
    "        while True:\n",
    "            max_features, min_samples_split, min_samples_leaf = get_parameters()\n",
    "            if (max_features, min_samples_split, min_samples_leaf) not in parameters_tested:\n",
    "                parameters_tested += [(max_features, min_samples_split, min_samples_leaf)]\n",
    "                break\n",
    "\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, \n",
    "                                      min_samples_split=min_samples_split, \n",
    "                                      min_samples_leaf=min_samples_leaf, oob_score=True,\n",
    "                                      random_state=random_state, n_jobs=n_jobs)\n",
    "        try:\n",
    "            model.fit(X, y)\n",
    "            model_specs = zip([\"max_features\", \"min_samples_split\", \"min_samples_leaf\"], \n",
    "                              (max_features, min_samples_split, min_samples_leaf))\n",
    "            model_results += [(round(model.oob_score_,4), model_specs)]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    model_results.sort(reverse=True)\n",
    "    \n",
    "    if top_models_to_print:\n",
    "        pprint(model_results[:top_models_to_print])\n",
    "        \n",
    "    if trees_in_final_model:\n",
    "        if trees_in_final_model == \"auto\":\n",
    "            trees_in_final_model = int(n_estimators*n_models_to_test/2.0)\n",
    "            \n",
    "        top_model = model_results[0][1]\n",
    "        max_features, min_samples_split, min_samples_leaf = [e[1] for e in top_model]\n",
    "        model = RandomForestRegressor(n_estimators=trees_in_final_model, \n",
    "                                      max_features=max_features, \n",
    "                                      min_samples_split=min_samples_split, \n",
    "                                      min_samples_leaf=min_samples_leaf, \n",
    "                                      oob_score=True,\n",
    "                                      random_state=random_state, \n",
    "                                      n_jobs=n_jobs)\n",
    "        model.fit(X, y)\n",
    "        if verbose:\n",
    "            print \"\"\n",
    "            print \"Final model results\"\n",
    "            print (round(model.oob_score_,4), top_model)\n",
    "        return model\n",
    "    else:\n",
    "        return model_results\n",
    "    \n",
    "    return None\n",
    "  \n",
    "def optimized_rf(X, y, variable_importance_n_estimators=100, \n",
    "                 variable_importance_max_features_options=[\"sqrt\", .5, \"auto\"],\n",
    "                 number_of_important_variables_to_use_options=[8, 10, 12, 15],\n",
    "                 n_estimators_in_grid_search=50,\n",
    "                 n_estimators_to_retrain_best_model=100,\n",
    "                 n_random_models_to_test=15,\n",
    "                 verbose=True,\n",
    "                 charts=False,\n",
    "                 random_state=42,\n",
    "                 n_jobs=1):\n",
    "    \"\"\"\n",
    "    Auto optimize the RF oob R^2 using a randomized search\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Optimized model, variables_in_model\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=random_state)\n",
    "    best_model_score = -100\n",
    "    \n",
    "    for max_features in variable_importance_max_features_options:\n",
    "        if verbose:\n",
    "            print\n",
    "            print \"*\"*60\n",
    "            print \"Variable importance optimization max features:\", max_features\n",
    "            print \"*\"*60\n",
    "        important_variables = sort_variables_by_importance(X, y, \n",
    "                                                           n_estimators=variable_importance_n_estimators, \n",
    "                                                           max_features=max_features, \n",
    "                                                           chart=charts)\n",
    "\n",
    "        for n_important_variables_to_use in number_of_important_variables_to_use_options:\n",
    "            if n_important_variables_to_use > len(X.columns):\n",
    "                break\n",
    "            var_to_use = important_variables[:n_important_variables_to_use]\n",
    "            model = oob_randomized_rf_gridsearch(X[var_to_use], y, \n",
    "                                                 n_estimators=n_estimators_in_grid_search, \n",
    "                                                 n_models_to_test=n_random_models_to_test, \n",
    "                                                 top_models_to_print=None, \n",
    "                                                 trees_in_final_model=n_estimators_to_retrain_best_model,\n",
    "                                                 verbose=verbose, \n",
    "                                                 n_jobs=n_jobs)\n",
    "            if model.oob_score_ > best_model_score:\n",
    "                best_model = model\n",
    "                best_var_importance_max_features = max_features\n",
    "                variables_in_model = var_to_use\n",
    "                best_model_score = model.oob_score_\n",
    "    return best_model, variables_in_model\n",
    "\n",
    "def model_forecast(x_to_predict,variable, mdl,mdl_vars, undiff):\n",
    "    \"\"\"\n",
    "\n",
    "    :param x_to_predict: This is the x value for the given point in the future to predict\n",
    "    :param variable: the current variable (metric) of interest to forcast ahead\n",
    "    :param mdl: the optimized model returned for a given series and horizon\n",
    "    :param mdl_vars: the corresponding variables used in that model\n",
    "    :param undiff: the function used to transform from differenced to absolute values\n",
    "    :return: the point prediciton (forecast) corresponding to the associated future time point.\n",
    "    \"\"\"\n",
    "    raw_predict = mdl.predict(x_to_predict[mdl_vars])\n",
    "    forecast = undiff(raw_predict, variable, True)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data from CENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(infile)\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_df, variables, states, coverages = preprocess_cent_file(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking this file is present \n",
    "processed_df.get_group(('VERMONT','MPC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking this file is present \n",
    "processed_df.get_group(('VERMONT','PIP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_time_horizons = [1,6,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize api containers\n",
    "detail_data, overview_data, my_panel = initialize_CENT_api_containers(coverages, variables,\n",
    "                                                                          states, time_horizons=my_time_horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myp3,detail3,overview3,errortallies = build_models(variables,processed_df,my_time_horizons,my_panel,detail_data,overview_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#now clean up the results and make forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first check on the vermont data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for variable in variables:\n",
    "    print detail3[1]['VERMONT']['metrics'][variable]['MPC'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(detail3[1]['VERMONT']['metrics'][variable]['MPC']['actual'],detail3[1]['VERMONT']['metrics'][variable]['MPC']['predicted'])\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(detail3[1]['VERMONT']['metrics'][variables[-1]]['MPC']['actual'],detail3[1]['VERMONT']['metrics'][variables[-1]]['MPC']['predicted'],alpha=0.3,color='steelblue')\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_overall_error(errortallies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir serial_output_3\n",
    "%cd serial_output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_up_results_serial(myp3,detail3,overview3,[1,6,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAL with ROLLING 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd ../\n",
    "!mkdir serial_output_3_R12\n",
    "dd = pd.read_csv(infile)\n",
    "r12df = CENT_rolling_12(dd,variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a grouped Rolling12 data frame and pass through the same procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped12 = r12df.groupby(('STATE','COVERAGE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the data structures\n",
    "Rdetail_data, Roverview_data, Rmy_panel = initialize_CENT_api_containers(coverages, variables,\n",
    "                                                                          states, time_horizons=my_time_horizons)\n",
    "# generate the models\n",
    "Rmyp3,Rdetail3,Roverview3,Rerrortallies = build_models(variables,grouped12,my_time_horizons,Rmy_panel,Rdetail_data,Roverview_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate overall error\n",
    "calculate_overall_error(Rerrortallies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save output\n",
    "%cd serial_output_3_R12/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean-up the results\n",
    "clean_up_results_serial(Rmyp3,Rdetail3,Roverview3,[1,6,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_detail_data(detail,time_horizon,state,coverage,metric):\n",
    "    data = detail[time_horizon][state]['metrics'][metric][coverage]\n",
    "    start_month = detail[time_horizon][state]['startMonth']\n",
    "    start_year = detail[time_horizon][state]['startYear']\n",
    "    actual = data['actual']\n",
    "    predicted = data['predicted']\n",
    "    std = data['stddev']\n",
    "    nmonths = len(actual)\n",
    "    forecast_values = list(np.zeros((12,)))\n",
    "    forecast_errors = list(np.zeros((12,)))\n",
    "    if time_horizon == 1:\n",
    "        forecast_values = data['forecast']['values']\n",
    "        forecast_errors = data['forecast']['std']\n",
    "    #if r12_flag\n",
    "    return actual,predicted,std,forecast_values, forecast_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detail_to_data_frame(detail,time_horizon,state,coverage,metric,r12_flag=False):\n",
    "    data = detail[time_horizon][state]['metrics'][metric][coverage]\n",
    "    start_month = detail[time_horizon][state]['startMonth']\n",
    "    start_year = detail[time_horizon][state]['startYear']\n",
    "    actual = data['actual']\n",
    "    predicted = data['predicted']\n",
    "    std = data['stddev']\n",
    "    nmonths = len(actual)\n",
    "    forecast_values = list(np.zeros((12,)))\n",
    "    forecast_errors = list(np.zeros((12,)))\n",
    "    if time_horizon == 1:\n",
    "        forecast_values = data['forecast']['values']\n",
    "        forecast_errors = data['forecast']['std']\n",
    "        #nmonths+=12\n",
    "    \n",
    "    if r12_flag:\n",
    "        start_date = str(start_month+12)+'-'+str(start_year)\n",
    "        date_index = pd.date_range(start_date,freq='M',periods=nmonths)\n",
    "    else:\n",
    "        start_date = str(start_month+1)+'-'+str(start_year)\n",
    "        date_index = pd.date_range(start_date,freq='M',periods=nmonths)\n",
    "    \n",
    "    df = pd.DataFrame(index = date_index)\n",
    "    df['actual'] = actual\n",
    "    df['predicted'] = predicted\n",
    "    \n",
    "    return df,std,forecast_values,forecast_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_df.get_group(('VERMONT','MPC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3[1]['MISSOURI']['metrics']['Paid Count']['PIP']['forecast']#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,p,std, fv,fe = extract_detail_data(detail3,1,'MISSOURI','PIP','Paid Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(a,'k',lw=2)\n",
    "plt.errorbar(np.arange(0,len(p)),p,1.96*std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(a,'k')\n",
    "plt.errorbar(np.arange(0,len(p)),p,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(a,'k')\n",
    "1.96*std\n",
    "plt.fill_between(p-1.96*std,p+1.96*std,color='lightgreen',alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_df.get_group(('MISSOURI','PIP')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail3[1]['MISSOURI']['metrics']['ALAE']['BI'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_detail_data(detail3,1,'MISSOURI','BI','ALAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary for the rounding digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rounding_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "rounding_dict = defaultdict(int)\n",
    "for v in variables:\n",
    "    if v.endswith('Count'):\n",
    "        rounding_dict[v]=0\n",
    "    elif v.endswith('CNT'):\n",
    "        rounding_dict[v]=0\n",
    "    else:\n",
    "        rounding_dict[v]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vt_mpc = processed_df.get_group(('VERMONT','MPC'))\n",
    "np.shape(vt_mpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vt_mpc.SUIT_CNT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_models_for_group(processed_df.get_group(('MISSOURI','PIP')),'MISSOURI','PIP',variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_models_for_group(processed_df.get_group(('VERMONT','MPC')),'VERMONT','MPC',variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_models_for_group(df,state,coverage,variables_of_interest,rnd_digits=rounding_dict): \n",
    "    #,,my_time_horizons,my_panel,detail_data,overview_data):\n",
    "    my_time_horizons =[1,6,12]\n",
    "    print state, coverage, len(df)\n",
    "    for time_horizon in my_time_horizons:\n",
    "        X, y_data, x, undiff = time_series_to_cross_section(df[variables_of_interest],\n",
    "                                                        forecast_horizon=time_horizon,\n",
    "                                                        max_fair_lags=13,\n",
    "                                                        seasonal_factor=12)\n",
    "        for variable in variables_of_interest:\n",
    "            # Account for time series with no variation\n",
    "            state_actuals = df[variable]\n",
    "            std_of_actuals = state_actuals.std(ddof=1)\n",
    "            if std_of_actuals == 0:\n",
    "                print \"\\t\",time_horizon,variable, \" is zero\"\n",
    "                continue\n",
    "            \n",
    "            \"\"\"\n",
    "                # code to deal with forecast data\n",
    "                value_col = 'v'+str(time_horizon)\n",
    "                error_col = 'e'+str(time_horizon)\n",
    "                if std_of_actuals == 0:\n",
    "                    overview_data[time_horizon][\"metrics\"][variable][coverage][state] = [0]*len(state_actuals)\n",
    "                    detail_data[time_horizon][state][\"metrics\"][variable][coverage] = {\"stddev\": 0,\n",
    "                                                                     \"actual\": list(state_actuals),\n",
    "                                                                     \"predicted\" : list(state_actuals)}\n",
    "                    my_panel[state][coverage].loc[variable,value_col] = 0#point_forecast[0]\n",
    "                    my_panel[state][coverage].loc[variable,error_col] = 0#point_forecast[1]                                                     \"predicted\": list(state_actuals)}\n",
    "                    break\n",
    "            \"\"\"\n",
    "            # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "            final_model, variables_in_model = optimized_rf(X, y_data[variable],\n",
    "                                                       variable_importance_n_estimators=50,\n",
    "                                                       n_estimators_in_grid_search=20,\n",
    "                                                       number_of_important_variables_to_use_options=[6],#[6],#, 8, 10, 12, 15, 20],\n",
    "                                                       variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                       n_estimators_to_retrain_best_model=50,\n",
    "                                                       verbose=False, n_random_models_to_test=3,\n",
    "                                                       charts=False, n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "            state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "            state_residuals = df[variable] - state_predictions\n",
    "            data_consumed_for_model = sum(state_residuals == 0)\n",
    "            std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "            state_std_residuals = state_residuals/std_of_residuals\n",
    "            abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - df[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "            MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - df[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()         \n",
    "\n",
    "            #  point forecast for this variable and time horizon\n",
    "            forecast = model_forecast(x,variable, final_model, variables_in_model,undiff)\n",
    "            #convert list of values to 2 floating point digits or 0 for counts\n",
    "            rnd_size = rnd_digits[variable]\n",
    "            if rnd_size > 0 :\n",
    "                state_actuals = [ round(elem,rnd_size) for elem in list(state_actuals)]\n",
    "                state_predictions = [ round(elem,rnd_size) for elem in list(state_predictions)]\n",
    "            else:\n",
    "                state_actuals = [ int(round(elem,0)) for elem in list(state_actuals)]\n",
    "                state_predictions = [ int(round(elem,0)) for elem in list(state_predictions)]\n",
    "            \n",
    "            overview_values = [round(elem,2) for elem in list(state_std_residuals.values)]\n",
    "            \n",
    "            \n",
    "            #    # Update primary data structures with model results\n",
    "            #    overview_data[time_horizon][\"metrics\"][variable][coverage][state] = overview_values\n",
    "            #    detail_data[time_horizon][state][\"metrics\"][variable][coverage] = {\"stddev\": round(std_of_residuals,5),\n",
    "            #                                                     \"actual\": state_actuals,\n",
    "            #                                                     \"predicted\": state_predictions}\n",
    "            MSE = round(MSE,5)\n",
    "            abs_average_error = round(abs_average_error,5)\n",
    "            #    my_panel[state][coverage].loc[variable,value_col] = round(forecast,5)\n",
    "            #    my_panel[state][coverage].loc[variable,error_col] = round(std_of_residuals,5)\n",
    "            #    error_dict[time_horizon].append([MSE,abs_average_error])\n",
    "            print \"\\t\",time_horizon,variable,len(overview_values),MSE,abs_average_error\n",
    "        #print '********************************************************************************************'\n",
    "    #print total_absolute_average_error / float(models_built), total_mean_squared_error / float(models_built)\n",
    "    #print models_built\n",
    "    #print '********************************************************************************************'\n",
    "    \n",
    "    return #(my_panel,detail_data,overview_data,error_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = list(data.STATE.unique())\n",
    "coverages = list(data.COVERAGE.unique())\n",
    "len(states),len(coverages),len(variables_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_ALABI = processed_df.get_group(('ALABAMA','BI'))\n",
    "orig_ALABI = dd[(dd.STATE == 'ALABAMA') &(dd.COVERAGE == 'BI')]\n",
    "sum(processed_ALABI['CWP']==orig_ALABI['CWP']) == len(orig_ALABI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r12_ALABI = r12df[(r12df.STATE == 'ALABAMA') & (r12df.COVERAGE == 'BI')]\n",
    "processed_ALABI.CIF.plot()\n",
    "orig_ALABI.CIF.plot()\n",
    "r12_ALABI.CIF.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r12_ALABI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING The results\n",
    "* need to remove nans from json data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[(data.STATE == 'VERMONT')& (data.COVERAGE =='MPC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for variable in variables:\n",
    "    print variable\n",
    "    print detail3[1]['VERMONT']['metrics'][variable]['MPC'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_detail_data(detail3,1,'VERMONT','MPC','Paid Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_detail_data(detail3,1,'VERMONT','COLL','Indemnity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin to look at this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Roverview3[1]['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rdetail3[1]['GEORGIA']['metrics'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detail_to_data_frame(detail,time_horizon,state,coverage,metric,r12_flag =False):\n",
    "    data = detail[time_horizon][state]['metrics'][coverage][metric]\n",
    "    start_month = detail[time_horizon][state]['startMonth']\n",
    "    start_year = detail[time_horizon][state]['startYear']\n",
    "    actual = data['actual']\n",
    "    predicted = data['predicted']\n",
    "    std = data['actual']\n",
    "    nmonths = len(actual)\n",
    "    forecast_values = list(np.zeros((12,)))\n",
    "    forecast_errors = list(np.zeros((12,)))\n",
    "    if time_horizon == 1:\n",
    "        forecast_values = data['forecast']['values']\n",
    "        forecast_errors = data['forecast']['std']\n",
    "    #if r12_flag\n",
    "    return actual,predicted,std,forecast_values, forecast_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(\n",
    "len(Rdetail3[1]['GEORGIA']['metrics']['ALAE']['Property']['actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(Rdetail3[1]['GEORGIA']['startMonth']), Rdetail3[1]['GEORGIA']['startYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.date_range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_month = 0\n",
    "start_year = 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = str(start_month+12)+'-'+str(start_year)\n",
    "pd.date_range(start_date,freq='M',periods=87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "12*8+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use hierarchical grouping to get the state-coverage-combo grouped dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf = data.groupby(('STATE','COVERAGE'))\n",
    "len(gdf)\n",
    "#my_grouped_data = data.groupby((x, y))  # set_index([x,y,t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in order to run the below example, pull out a specific state,coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_case1 = gdf.get_group(('ALABAMA','BI'))\n",
    "test_case1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y_data, x, undiff = time_series_to_cross_section0(test_case1[variables_of_interest], \n",
    "                                                            forecast_horizon=1, \n",
    "                                                            max_fair_lags=2, \n",
    "                                                            seasonal_factor=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(test_case1[variables_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(y_data), np.shape(x),np.shape(undiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X6, y_data6, x6, undiff6 = time_series_to_cross_section0(test_case1[variables_of_interest], \n",
    "                                                            forecast_horizon=6, \n",
    "                                                            max_fair_lags=6, \n",
    "                                                            seasonal_factor=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X12, y_data12, x12, undiff12 = time_series_to_cross_section0(test_case1[variables_of_interest], \n",
    "                                                            forecast_horizon=12, \n",
    "                                                            max_fair_lags=12, \n",
    "                                                            seasonal_factor=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take a peak at some of the original timeseries here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_case1 = test_case1.copy()\n",
    "ts_case1.index = ts_case1.YEAR\n",
    "ts_case1.drop(['COVERAGE','STATE','YEAR'],axis=1,inplace=True)\n",
    "ts_case1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at getting A) extended forecast horizon and B) additional predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply the rf prediction to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction_for_ts(focused_data,variables_of_interest,X,y_data,x,undiff):\n",
    "    verbose=True\n",
    "    total_absolute_average_error = 0\n",
    "    total_mean_squared_error = 0\n",
    "    models_built = 0\n",
    "    variable_models_built = 0\n",
    "    for variable in variables_of_interest[:-2]:\n",
    "        # Housekeeping variables\n",
    "        models_built += 1\n",
    "        # Account for time series with no variation\n",
    "        state_actuals = focused_data[variable]\n",
    "        std_of_actuals = state_actuals.std(ddof=1)\n",
    "        if std_of_actuals == 0:\n",
    "            overview_data[\"metrics\"][variable][coverage][state] = [0]*len(state_actuals)\n",
    "            detail_data[state][\"metrics\"][variable][coverage] = {  \"stddev\": 0,\n",
    "                                                                       \"actual\": list(state_actuals),\n",
    "                                                                       \"predicted\": list(state_actuals)}\n",
    "            break\n",
    "                \n",
    "        variable_models_built += 1\n",
    "            \n",
    "        # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "        final_model, variables_in_model = optimized_rf(X, y_data[variable], \n",
    "                                                           variable_importance_n_estimators=20, \n",
    "                                                           n_estimators_in_grid_search=10,\n",
    "                                                           number_of_important_variables_to_use_options=[6],#, 8, 10, 12, 15, 20],\n",
    "                                                           variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                           n_estimators_to_retrain_best_model=10, \n",
    "                                                           verbose=False, n_random_models_to_test=1,\n",
    "                                                           charts=False, n_jobs=1)\n",
    "\n",
    "        state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "            \n",
    "        state_residuals = focused_data[variable] - state_predictions\n",
    "        data_consumed_for_model = sum(state_residuals == 0)\n",
    "        std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "        state_std_residuals = state_residuals/std_of_residuals\n",
    "        abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "        MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()\n",
    "        total_absolute_average_error += abs_average_error\n",
    "        total_mean_squared_error += MSE\n",
    "            \n",
    "        # Update primary data structures with model results\n",
    "        overview_data[\"metrics\"][variable][coverage][state] = list(state_std_residuals.values)\n",
    "        detail_data[state][\"metrics\"][variable][coverage] = {\"stddev\": std_of_residuals,\n",
    "                                                             \"actual\": list(state_actuals),\n",
    "                                                             \"predicted\": list(state_predictions.round(0).astype(int))\n",
    "                                                             }\n",
    "        \n",
    "        if verbose:\n",
    "            figsize(15,5)\n",
    "            title(\"%s %s: %s\" % (state, coverage, variable), fontsize=16)\n",
    "                # plot(state_predictions)\n",
    "            plot(state_actuals, \"k\")\n",
    "            plot(state_predictions + 1.96*std_of_residuals, \"0.5\")\n",
    "            LCL = state_predictions - 1.96*std_of_residuals\n",
    "            if sum(state_actuals < 0) == 0:\n",
    "                LCL[LCL<0] = 0\n",
    "            plot(LCL, \"0.5\")\n",
    "            show()\n",
    "\n",
    "        print \"OOB R^2: %f\" % final_model.oob_score_\n",
    "        print \"Absolute average error: %f\" % abs_average_error\n",
    "        print \"Mean squared error: %f\\n\" % MSE\n",
    "    \"\"\"\n",
    "        output = {'state': state,\n",
    "                  'variable': variable,\n",
    "                  'coverage': coverage,\n",
    "                  'overview_data': list(state_std_residuals.values),\n",
    "                  'detail_data': {\"stddev\": std_of_residuals,\n",
    "                                  \"actual\": list(state_actuals),\n",
    "                                  \"predicted\": list(state_predictions.round(0).astype(int))},\n",
    "                  'abs_average_error': abs_average_error,\n",
    "                  'MSE': MSE}\n",
    "\n",
    "        print json.dumps(output)\n",
    "        \"\"\"    \n",
    "    #print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)\n",
    "    return(state_predictions,final_model,variables_in_model,variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_prediction_for_ts(test_case1,variables_of_interest,X,y_data,x,undiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl.feature_importances_, mdl_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl6.feature_importances_, mdl_vars6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl12.feature_importances_, mdl_vars12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,mdl,mdl_vars,var = make_prediction_for_ts(test_case1,variables_of_interest, X,y_data,x,undiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[mdl_vars], var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_predict = mdl.apply(x[mdl_vars])\n",
    "\n",
    "undiff(raw_predict, var, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,mdl,mdl_vars,var = make_prediction_for_ts(test_case1,variables_of_interest, X,y_data,x,undiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print x, mdl_vars\n",
    "\n",
    "raw_predict = mdl.apply(x[mdl_vars])\n",
    "\n",
    "undiff(raw_predict, 'Indemnity', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print raw_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals_idemnity = [7823452.88,  7823439.88,  7823469.88,  7823439.88,  7823460.88,7823457.88,  7823453.88,  7823468.88,  7823466.88,  7823471.88]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_case1['Indemnity_predict'] = a\n",
    "#ts_case1.ix[len(ts_case1),'forecast'] = np.mean(vals_idemnity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_case1.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "x_date1 = datetime.date(2015,2,1)\n",
    "print x_date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_case1.Indemnity.plot(color='steelblue',label='actual')\n",
    "ts_case1.Indemnity_predict.plot(color='darkred',style=':',marker='s',label='prediction')\n",
    "\n",
    "#now add the vals_idemnity\n",
    "\n",
    "#for my_yval in vals_idemnity:\n",
    "#    plt.plot(x_date1,my_yval,'s',color='indianred',alpha=0.3)\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for my_yval in vals_idemnity:\n",
    "    plt.plot(x_date1,my_yval,'ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(a), test_case1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(test_case1.Overall, a,color='forestgreen',alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_case1['Overall_predict']=a\n",
    "ts_case1.Overall.plot()\n",
    "ts_case1.Overall_predict.plot(marker='o',style=':',color='darkred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### okay how to export the forecast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a6,mdl6,mdl_vars6,var6 = make_prediction_for_ts(test_case1,variables_of_interest, X6,y_data6,x6,undiff6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl_vars6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_model(x,variable, mdl,mdl_vars,undiff):\n",
    "    raw_predict = mdl.apply(x[mdl_vars])\n",
    "    forecast = undiff(raw_predict, variable, True)\n",
    "    return forecast,raw_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fore6,raw6 = apply_model(x6,var6,mdl6,mdl_vars6,undiff6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apply_model(x,var,mdl,mdl_vars,undiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fore6[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = datetime.date(2007,1,1)\n",
    "end_date = datetime.date(2015,1,1)\n",
    "years = matplotlib.dates.YearLocator()\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(ts_case1.Indemnity.values)\n",
    "ax.xaxis.set_major_locator(years)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a12,mdl12,mdl_vars12,var12 = make_prediction_for_ts(test_case1,variables_of_interest, X12,y_data12,x12,undiff12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fore,raw = apply_model(x,var,mdl,mdl_vars,undiff)\n",
    "fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fore12,raw12 = apply_model(x12,var12,mdl12,mdl_vars12,undiff12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fore12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dates = []\n",
    "for m in xrange(1,13):\n",
    "    my_dates.append(datetime.date(2014,m,1))\n",
    "\n",
    "my_dates.append(end_date)\n",
    "len(my_dates), my_dates\n",
    "yv1 = ts_case1.Indemnity[-13:].values\n",
    "#yv1\n",
    "yv2 = ts_case1.Indemnity_predict[-13:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_date2 = datetime.date(2015,7,1)\n",
    "x_date3 = datetime.date(2016,1,1)\n",
    "for my_yval in fore[0]:\n",
    "    plt.plot(x_date1,my_yval,'dk')\n",
    "    \n",
    "for y in fore6[0]:\n",
    "    plt.plot(x_date2,y,'dg')\n",
    "\n",
    "for y in fore12[0]:\n",
    "    plt.plot(x_date3,y,'dr')\n",
    "\n",
    "    \n",
    "plt.plot(my_dates,yv1,color='steelblue',marker='*')\n",
    "plt.plot(my_dates,yv2,'o:',color='deeppink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction_for_ts2(focused_data,variables_of_interest,X,y_data,x,undiff):\n",
    "    verbose=True\n",
    "    total_absolute_average_error = 0\n",
    "    total_mean_squared_error = 0\n",
    "    models_built = 0\n",
    "    variable_models_built = 0\n",
    "    max_forecast_horizon = 12 # the number of months to forecast\n",
    "    for variable in variables_of_interest[:-2]:\n",
    "        # Housekeeping variables\n",
    "        models_built += 1\n",
    "        # Account for time series with no variation\n",
    "        state_actuals = focused_data[variable]\n",
    "        std_of_actuals = state_actuals.std(ddof=1)\n",
    "        if std_of_actuals == 0:\n",
    "            overview_data[\"metrics\"][variable][coverage][state] = [0]*len(state_actuals)\n",
    "            # augment with forecast\n",
    "            # just use the last value of the state_actuals:\n",
    "            forecast_values = state_actuals[-1]*max_forecast_horizon\n",
    "            detail_data[state][\"metrics\"][variable][coverage] = {  \"stddev\": 0,\n",
    "                                                                       \"actual\": list(state_actuals),\n",
    "                                                                       \"predicted\": list(state_actuals),\n",
    "                                                                       \"forecast\": {\"values\":forecast_values,\n",
    "                                                                                    \"lower_cl\": forecast_values,\n",
    "                                                                                    \"upper_cl\": forecast_values}}\n",
    "            break\n",
    "                \n",
    "        variable_models_built += 1\n",
    "            \n",
    "        # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "        final_model, variables_in_model = optimized_rf(X, y_data[variable], \n",
    "                                                           variable_importance_n_estimators=20, \n",
    "                                                           n_estimators_in_grid_search=10,\n",
    "                                                           number_of_important_variables_to_use_options=[6],#, 8, 10, 12, 15, 20],\n",
    "                                                           variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                           n_estimators_to_retrain_best_model=10, \n",
    "                                                           verbose=False, n_random_models_to_test=1,\n",
    "                                                           charts=False, n_jobs=1)\n",
    "\n",
    "        state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "            \n",
    "        state_residuals = focused_data[variable] - state_predictions\n",
    "        data_consumed_for_model = sum(state_residuals == 0)\n",
    "        std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "        state_std_residuals = state_residuals/std_of_residuals\n",
    "        abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "        MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()\n",
    "        total_absolute_average_error += abs_average_error\n",
    "        total_mean_squared_error += MSE\n",
    "        # call a function to make forecast\n",
    "        \n",
    "        # call a function to interpolate \n",
    "        # Update primary data structures with model results\n",
    "        overview_data[\"metrics\"][variable][coverage][state] = list(state_std_residuals.values)\n",
    "        detail_data[state][\"metrics\"][variable][coverage] = {\"stddev\": std_of_residuals,\n",
    "                                                             \"actual\": list(state_actuals),\n",
    "                                                             \"predicted\": list(state_predictions.round(0).astype(int))}\n",
    "        \n",
    "        if verbose:\n",
    "            figsize(15,5)\n",
    "            title(\"%s %s: %s\" % (state, coverage, variable), fontsize=16)\n",
    "                # plot(state_predictions)\n",
    "            plot(state_actuals, \"k\")\n",
    "            plot(state_predictions + 1.96*std_of_residuals, \"0.5\")\n",
    "            LCL = state_predictions - 1.96*std_of_residuals\n",
    "            if sum(state_actuals < 0) == 0:\n",
    "                LCL[LCL<0] = 0\n",
    "            plot(LCL, \"0.5\")\n",
    "            show()\n",
    "\n",
    "        print \"OOB R^2: %f\" % final_model.oob_score_\n",
    "        print \"Absolute average error: %f\" % abs_average_error\n",
    "        print \"Mean squared error: %f\\n\" % MSE\n",
    "    \"\"\"\n",
    "        output = {'state': state,\n",
    "                  'variable': variable,\n",
    "                  'coverage': coverage,\n",
    "                  'overview_data': list(state_std_residuals.values),\n",
    "                  'detail_data': {\"stddev\": std_of_residuals,\n",
    "                                  \"actual\": list(state_actuals),\n",
    "                                  \"predicted\": list(state_predictions.round(0).astype(int))},\n",
    "                  'abs_average_error': abs_average_error,\n",
    "                  'MSE': MSE}\n",
    "\n",
    "        print json.dumps(output)\n",
    "        \"\"\"    \n",
    "    #print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)\n",
    "    return(state_predictions,final_model,variables_in_model,variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from mltimeseries import time_series_to_cross_section, optimized_rf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coverages, len(coverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alabama = data[data.STATE=='ALABAMA'].copy()\n",
    "print alabama.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alaBI = alabama[alabama.COVERAGE=='BI'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = pd.to_datetime(alaBI.YEAR)\n",
    "#plt.plot(alaBI.YEAR,alaBI.Severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "verbose=False\n",
    "\n",
    "models_built = 0\n",
    "total_absolute_average_error = 0\n",
    "total_mean_squared_error = 0\n",
    "\n",
    "for state in states:\n",
    "    coverage_group = state_data_group.get_group(state).groupby(\"COVERAGE\")\n",
    "    for coverage, focused_data in coverage_group:\n",
    "        ### Test code\n",
    "        if models_built >= 500:\n",
    "            break\n",
    "            \n",
    "        variable_models_built = 0\n",
    "        X, y_data, x, undiff = time_series_to_cross_section(focused_data[variables_of_interest], \n",
    "                                                            forecast_horizon=1, \n",
    "                                                            max_fair_lags=2, \n",
    "                                                            seasonal_factor=12)\n",
    "        for variable in variables_of_interest:\n",
    "            # Housekeeping variables\n",
    "            models_built += 1\n",
    "            \n",
    "            # Account for time series with no variation\n",
    "            state_actuals = focused_data[variable]\n",
    "            std_of_actuals = state_actuals.std(ddof=1)\n",
    "            if std_of_actuals == 0:\n",
    "                overview_data[\"metrics\"][variable][coverage][state] = [0]*len(state_actuals)\n",
    "                detail_data[state][\"metrics\"][variable][coverage] = {  \"stddev\": 0,\n",
    "                                                                       \"actual\": list(state_actuals),\n",
    "                                                                       \"predicted\": list(state_actuals)}\n",
    "                break\n",
    "                \n",
    "            variable_models_built += 1\n",
    "            \n",
    "            # Build custom model for dataset. This is the line of code that takes all the time.\n",
    "            final_model, variables_in_model = optimized_rf(X, y_data[variable], \n",
    "                                                           variable_importance_n_estimators=20, \n",
    "                                                           n_estimators_in_grid_search=10,\n",
    "                                                           number_of_important_variables_to_use_options=[6],#, 8, 10, 12, 15, 20],\n",
    "                                                           variable_importance_max_features_options=['sqrt'],#, 0.5, .75, 'auto'],\n",
    "                                                           n_estimators_to_retrain_best_model=10, \n",
    "                                                           verbose=False, n_random_models_to_test=1,\n",
    "                                                           charts=False, n_jobs=1)\n",
    "\n",
    "            state_predictions = undiff(final_model.oob_prediction_, variable, True)\n",
    "            \n",
    "            state_residuals = focused_data[variable] - state_predictions\n",
    "            data_consumed_for_model = sum(state_residuals == 0)\n",
    "            std_of_residuals = state_residuals[data_consumed_for_model:].std(ddof=1)\n",
    "            state_std_residuals = state_residuals/std_of_residuals\n",
    "            abs_average_error = abs((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals).mean()\n",
    "            MSE = (((undiff(final_model.oob_prediction_, variable, True)[data_consumed_for_model:] - focused_data[variable][data_consumed_for_model:])/std_of_actuals)**2).mean()\n",
    "            total_absolute_average_error += abs_average_error\n",
    "            total_mean_squared_error += MSE\n",
    "            \n",
    "            # Update primary data structures with model results\n",
    "            overview_data[\"metrics\"][variable][coverage][state] = list(state_std_residuals.values)\n",
    "            detail_data[state][\"metrics\"][variable][coverage] = {\"stddev\": std_of_residuals,\n",
    "                                                                 \"actual\": list(state_actuals),\n",
    "                                                                 \"predicted\": list(state_predictions.round(0).astype(int))}\n",
    "                        \n",
    "            if verbose:\n",
    "                figsize(15,5)\n",
    "                title(\"%s %s: %s\" % (state, coverage, variable), fontsize=16)\n",
    "                # plot(state_predictions)\n",
    "                plot(state_actuals, \"k\")\n",
    "                plot(state_predictions + 1.96*std_of_residuals, \"0.5\")\n",
    "                LCL = state_predictions - 1.96*std_of_residuals\n",
    "                if sum(state_actuals < 0) == 0:\n",
    "                    LCL[LCL<0] = 0\n",
    "                plot(LCL, \"0.5\")\n",
    "                show()\n",
    "\n",
    "                print \"OOB R^2: %f\" % final_model.oob_score_\n",
    "                print \"Absolute average error: %f\" % abs_average_error\n",
    "                print \"Mean squared error: %f\\n\" % MSE\n",
    "                \n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_fair_lags: 6 --> 13 (21:57:45) (Almost 22 hours to run)\n",
    "# variable_importance_n_estimators: 10 --> 100 \n",
    "# n_estimators_in_grid_search: 10 --> 50\n",
    "# number_of_important_variables_to_use_options: [8] --> [8, 10, 12, 15]\n",
    "# variable_importance_max_features_options: ['sqrt'] --> ['sqrt', 0.5, 'auto']\n",
    "# n_estimators_to_retrain_best_model: 10 --> 200\n",
    "# n_random_models_to_test: 1 --> 6\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_fair_lags: 6 --> 13 (5:30:45)\n",
    "# variable_importance_n_estimators: 10 --> 100 \n",
    "# n_estimators_in_grid_search: 10 --> 50\n",
    "# number_of_important_variables_to_use_options: [8] --> [8, 10, 12, 15]\n",
    "# variable_importance_max_features_options: ['sqrt'] --> ['sqrt', 0.5, 'auto']\n",
    "# n_estimators_to_retrain_best_model: 10 --> 200\n",
    "# n_random_models_to_test: 1 --> 6\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variable_importance_n_estimators: 10 --> 150 (8:51:47)\n",
    "# n_estimators_in_grid_search: 10 --> 75\n",
    "# number_of_important_variables_to_use_options: [8] --> [8, 10, 12, 15, 20]\n",
    "# variable_importance_max_features_options: ['sqrt'] --> ['sqrt', 0.5, 'auto']\n",
    "# n_estimators_to_retrain_best_model: 10 --> 200\n",
    "# n_random_models_to_test: 1 --> 8\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variable_importance_n_estimators: 10 --> 100 (2:30:10)\n",
    "# n_estimators_in_grid_search: 10 --> 50\n",
    "# number_of_important_variables_to_use_options: [8] --> default\n",
    "# variable_importance_max_features_options: ['sqrt'] --> ['sqrt', 0.5, 'auto']\n",
    "# n_estimators_to_retrain_best_model: 10 --> 100\n",
    "# n_random_models_to_test: 1 --> 4\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variable_importance_n_estimators: 10 --> 100 (5:15)\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_random_models_to_test: 1 --> 2; (2:30)\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base model (1:45)\n",
    "print total_absolute_average_error/float(models_built), total_mean_squared_error/float(models_built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('overview5.json', 'w') as outfile:\n",
    "    json.dump(overview_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('detail5.json', 'w') as outfile:\n",
    "    json.dump(detail_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = pl_retMX(dv[:],data,maine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_important(X, y, max_features):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import pandas as pd\n",
    "    #import RandomForestRegressor from sklearn.\n",
    "    #n_estimators=100, \n",
    "    #                             max_features=\"auto\", chart=True,\n",
    "    #                             random_state=42, n_jobs=1):\n",
    "    n_estimators = 100\n",
    "    #chart = False\n",
    "    random_state = 40\n",
    "    n_jobs=1\n",
    "    \"\"\"Returns an array of column names sorted by rf importance\"\"\"\n",
    "    model = RandomForestRegressor(n_estimators, \n",
    "                                  max_features=max_features, \n",
    "                                  random_state=random_state, \n",
    "                                  n_jobs=n_jobs)\n",
    "    model.fit(X, y)\n",
    "    important_variables = pd.Series(model.feature_importances_, \n",
    "                                    index=X.columns)\n",
    "    important_variables.sort()\n",
    "    \"\"\"\n",
    "    if chart:\n",
    "        important_variables.plot(kind=\"barh\", figsize=(5,15))\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "    important_variables.sort(ascending=False)\n",
    "    print len(important_variables.index)\n",
    "    return list(important_variables.index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
