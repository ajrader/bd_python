{
 "metadata": {
  "name": "",
  "signature": "sha256:dd15fc2b7f197cfe907d1b712528077e1520b303ff6376ce7e8f23dfc7acb270"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# A notebook to illustrate using the sqoop command from within the IPython Notebook\n",
      "Need to define the password file where I've stored the password for my access to the database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls '/user/kesj/config'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 1 items\r\n",
        "-rw-------   3 kesj kesj          9 2014-11-04 14:00 /user/kesj/config/.mypass\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setup parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nfile = '/user/kesj/config/.mypass'\n",
      "\n",
      "#alternatively can use relative path: nfile = 'config/e.pswd'\n",
      "# to use local File space you must prefice the file name with file:\n",
      "user = 'kesj'\n",
      "dbip = 'jdbc:db2://10.96.37.166:60100/FDW2P'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!sqoop list-tables --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file '/user/kesj/config/e.pswd'\n",
      "#!sqoop list-tables --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password {myPSWD}\n",
      "#!sqoop list-tables --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbList = !sqoop list-databases --connect {dbip} --username {user} --password-file {nfile}\n",
      "dbList=dbList[4:]\n",
      "print \"There are {0} databases (schemas) within this DB\".format(len(dbList))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 456 databases (schemas) within this DB\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[a for a in dbList if a.startswith('FDWATOMC')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['FDWATOMC',\n",
        " 'FDWATOMCACX',\n",
        " 'FDWATOMCAE',\n",
        " 'FDWATOMCAGC',\n",
        " 'FDWATOMCAIRS',\n",
        " 'FDWATOMCAM',\n",
        " 'FDWATOMCAR',\n",
        " 'FDWATOMCAR_MOK',\n",
        " 'FDWATOMCASR',\n",
        " 'FDWATOMCAUTO',\n",
        " 'FDWATOMCBANK',\n",
        " 'FDWATOMCCEN',\n",
        " 'FDWATOMCCI',\n",
        " 'FDWATOMCCLNT',\n",
        " 'FDWATOMCCPTVA',\n",
        " 'FDWATOMCECS',\n",
        " 'FDWATOMCECS_EXCP',\n",
        " 'FDWATOMCEPMAG',\n",
        " 'FDWATOMCESOM',\n",
        " 'FDWATOMCFC',\n",
        " 'FDWATOMCFE',\n",
        " 'FDWATOMCFSDYA',\n",
        " 'FDWATOMCHLTH',\n",
        " 'FDWATOMCIC',\n",
        " 'FDWATOMCITSM',\n",
        " 'FDWATOMCLEADS',\n",
        " 'FDWATOMCLIFE',\n",
        " 'FDWATOMCMFUND',\n",
        " 'FDWATOMCMTCH',\n",
        " 'FDWATOMCOCAP',\n",
        " 'FDWATOMCPC',\n",
        " 'FDWATOMCPCOX',\n",
        " 'FDWATOMCPCU',\n",
        " 'FDWATOMCPCUA',\n",
        " 'FDWATOMCPCUF',\n",
        " 'FDWATOMCSFPP',\n",
        " 'FDWATOMCSVI',\n",
        " 'FDWATOMCTPAR',\n",
        " 'FDWATOMCWBTD',\n",
        " 'FDWATOMCYA',\n",
        " 'FDWATOMC_MOK',\n",
        " 'FDWATOMC_T']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tblList = !sqoop list-tables --connect jdbc:db2://10.96.37.166:60100/FDW2P --username {user} --password-file {nfile}\n",
      "tblList = tblList[5:]\n",
      "print \"There are {0} tables in these schemas.\".format(len(tblList))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 17207 tables in these schemas.\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tblList"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Look for error in Y1573AAA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minID = 1403659351\n",
      "maxID = 3455806544"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hmil = 100000000\n",
      "maxID - minID"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l = linspace(minID+hmil,maxID,20)\n",
      "l, len(l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tgtbase = 'ehunt/Y1753AAA_DETL/'\n",
      "wclause0 = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' )'\n",
      "for i in xrange(0,3):\n",
      "    #if i == 0:\n",
      "    #if i > 0:\n",
      "    #    lmin = int(l[i-1])\n",
      "    lmax = int(l[i])+1\n",
      "    tgt = tgtbase+str(i)\n",
      "        #wclause = wclause0+' AND DETL_DIM_ID > '+str(lmin)+ 'AND DETL_DIM_ID < '+str(lmax)+'\"'\n",
      "    wclause = wclause0+' AND DETL_DIM_ID < '+str(lmax)+'\"'\n",
      "    print tgt,lmax,wclause\n",
      "    !sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\"\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )'\n",
      "tgt = '/data/discovery/vehrepat/staging/tmp/Y1753DAD'\n",
      "!sqoop import --connect {dbip} --table FDWATOMCAE.DETL --split-by \"DETL_DIM_ID\" --username {user} --password-file {nfile} --where {wclause}  --target-dir {tgt}  \n",
      "#--as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:09 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:10 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/12/17 13:46:10 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n",
        "14/12/17 13:46:11 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/50b6f3b7c2a81d4e69d4477deb82722e/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/12/17 13:46:13 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/50b6f3b7c2a81d4e69d4477deb82722e/FDWATOMCAE.DETL.jar\r\n",
        "14/12/17 13:46:13 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:13 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/12/17 13:46:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n",
        "14/12/17 13:46:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:13 INFO client.RMProxy: Connecting to ResourceManager at da74wbrmgr1.opr.statefarm.org/10.96.243.38:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:13 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 3527 for kesj on ha-hdfs:nameservice1\r\n",
        "14/12/17 13:46:13 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 3527 for kesj)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/17 13:46:15 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/12/17 13:46:15 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD' )  --target-dir /data/discovery/vehrepat/staging/tmp/Y1753DAD   )\r\n",
        "14/12/17 13:46:15 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/kesj/.staging/job_1416923118494_1066\r\n",
        "14/12/17 13:46:15 WARN security.UserGroupInformation: PriviledgedActionException as:kesj@OPR.STATEFARM.ORG (auth:KERBEROS) cause:java.io.IOException: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=END-OF-STATEMENT;_VEH_CD='Y1753DAD' );), DRIVER=4.15.113\r\n",
        "14/12/17 13:46:15 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=END-OF-STATEMENT;_VEH_CD='Y1753DAD' );), DRIVER=4.15.113\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DataDrivenDBInputFormat.getSplits(DataDrivenDBInputFormat.java:170)\r\n",
        "\tat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)\r\n",
        "\tat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)\r\n",
        "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)\r\n",
        "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1295)\r\n",
        "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1292)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\r\n",
        "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1292)\r\n",
        "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1313)\r\n",
        "\tat org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:186)\r\n",
        "\tat org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:159)\r\n",
        "\tat org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:247)\r\n",
        "\tat org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:614)\r\n",
        "\tat org.apache.sqoop.manager.Db2Manager.importTable(Db2Manager.java:65)\r\n",
        "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:413)\r\n",
        "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\r\n",
        "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\r\n",
        "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n",
        "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\r\n",
        "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\r\n",
        "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\r\n",
        "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=END-OF-STATEMENT;_VEH_CD='Y1753DAD' );), DRIVER=4.15.113\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:696)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:127)\r\n",
        "\tat com.ibm.db2.jcc.am.qo.c(qo.java:2770)\r\n",
        "\tat com.ibm.db2.jcc.am.qo.d(qo.java:2758)\r\n",
        "\tat com.ibm.db2.jcc.am.qo.a(qo.java:2191)\r\n",
        "\tat com.ibm.db2.jcc.t4.bb.h(bb.java:140)\r\n",
        "\tat com.ibm.db2.jcc.t4.bb.b(bb.java:40)\r\n",
        "\tat com.ibm.db2.jcc.t4.q.a(q.java:32)\r\n",
        "\tat com.ibm.db2.jcc.t4.rb.i(rb.java:135)\r\n",
        "\tat com.ibm.db2.jcc.am.qo.ib(qo.java:2160)\r\n",
        "\tat com.ibm.db2.jcc.am.qo.a(qo.java:3257)\r\n",
        "\tat com.ibm.db2.jcc.am.qo.a(qo.java:706)\r\n",
        "\tat com.ibm.db2.jcc.am.qo.executeQuery(qo.java:685)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DataDrivenDBInputFormat.getSplits(DataDrivenDBInputFormat.java:145)\r\n",
        "\t... 23 more\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )'\n",
      "wclause"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "'\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753DAD\\' )'"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = \"LOS_EST_DIM_ID IN (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753DAD\\')\"\n",
      "wclause"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "\"LOS_EST_DIM_ID IN (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD')\""
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect {dbip} --table FDWATOMCAE.DETL --split-by \"DETL_DIM_ID\" --username kesj --password-file config/.mypasswd --where {wclause}  --target-dir {tgt}  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/bin/sh: syntax error at line 1: `(' unexpected\r\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Narrow in on the 2nd segment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls ehunt/Y1753AAA_DETL/*"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ls: `ehunt/Y1753AAA_DETL/*': No such file or directory\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#wclause[-10:] = '553659352\"'\n",
      "wc = wclause[:-10]+'553659352\"'\n",
      "wc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/A --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wc = wclause[:-10]+'517673223\"' \n",
      "#1517673223\n",
      "wc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/B --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wc = wclause[:-10]+'535394808\"'#'517673224\"' 1\n",
      "#1517673223\n",
      "wc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/D --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wc = wclause[:-10]+'526534015\"'#'517673224\"' 1\n",
      "#1517673223\n",
      "print wc\n",
      "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/E --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wc = wclause[:-10]+'530964411\"'#'517673224\"' 1\n",
      "#1517673223\n",
      "print wc\n",
      "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/F --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wc = wclause[:-13]+'= 1533425449\"'\n",
      "#'517673224\"' 1\n",
      "#1517673223\n",
      "print wc\n",
      "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/P --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1533425430"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -rmdir ehunt/fdw1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Looking at differences in using the db2.jcc.charsetDecoderEncoder=3 option\n",
      "12.01.14\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID = 153342540\"'\n",
      "\n",
      "wc = wclause[:-29]+'\"'\n",
      "wc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753AAA\\' )\"'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pull the whole set from DETL\n",
      "tgt = 'e2/Y1753AAA_detl'\n",
      "!sqoop import --connect {dbip} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt}  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:18 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:19 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/12/01 08:54:19 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:20 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:20 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:21 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/671d3d10ad32fd6776cd82df618d7989/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/12/01 08:54:23 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/671d3d10ad32fd6776cd82df618d7989/FDWATOMCAE.DETL.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:23 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/12/01 08:54:23 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/12/01 08:54:23 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:23 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:54:25 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/12/01 08:54:25 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:55:32 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:55:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0360\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:55:33 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0360\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:55:33 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0360/\r\n",
        "14/12/01 08:55:33 INFO mapreduce.Job: Running job: job_1412635903408_0360\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:55:39 INFO mapreduce.Job: Job job_1412635903408_0360 running in uber mode : false\r\n",
        "14/12/01 08:55:39 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:56:17 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:56:20 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:56:26 INFO mapreduce.Job: Task Id : attempt_1412635903408_0360_m_000000_0, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 28976\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:56:27 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:56:28 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:56:41 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:57:03 INFO mapreduce.Job: Task Id : attempt_1412635903408_0360_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 104801\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:58:31 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:58:44 INFO mapreduce.Job: Task Id : attempt_1412635903408_0360_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 59249\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:58:45 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:59:34 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:59:35 INFO mapreduce.Job: Job job_1412635903408_0360 failed with state FAILED due to: Task failed task_1412635903408_0360_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:59:36 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=310662\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=9250261\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=723124\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=723124\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=723124\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=740478976\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=31582\r\n",
        "\t\tMap output records=31582\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=178\r\n",
        "\t\tCPU time spent (ms)=18190\r\n",
        "\t\tPhysical memory (bytes) snapshot=949301248\r\n",
        "\t\tVirtual memory (bytes) snapshot=4715098112\r\n",
        "\t\tTotal committed heap usage (bytes)=2070413312\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=9250261\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:59:36 INFO mapreduce.ImportJobBase: Transferred 8.8217 MB in 312.7424 seconds (28.8847 KB/sec)\r\n",
        "14/12/01 08:59:36 INFO mapreduce.ImportJobBase: Retrieved 31582 records.\r\n",
        "14/12/01 08:59:36 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbip, wc, tgt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "('jdbc:db2://10.96.37.166:60100/FDW2P',\n",
        " '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753AAA\\' )\"',\n",
        " 'e2/Y1753AAA_detl_decodeA')"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note that the 'general' "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat using -D db2.jcc.charsetDecoderEncoder=3 \n",
      "tgt = 'e2/Y1753AAA_detl_decodeA'\n",
      "!sqoop import -D db2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt} "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:04 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/12/01 09:28:04 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:06 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/23e9562944e8195086f76da64891868d/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/12/01 09:28:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/23e9562944e8195086f76da64891868d/FDWATOMCAE.DETL.jar\r\n",
        "14/12/01 09:28:08 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/12/01 09:28:08 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:08 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:10 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/12/01 09:28:10 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:37 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0362\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:37 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0362\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:37 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0362/\r\n",
        "14/12/01 09:28:37 INFO mapreduce.Job: Running job: job_1412635903408_0362\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:28:43 INFO mapreduce.Job: Job job_1412635903408_0362 running in uber mode : false\r\n",
        "14/12/01 09:28:43 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:29:02 INFO mapreduce.Job: Task Id : attempt_1412635903408_0362_m_000000_0, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 106898\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:29:06 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:29:12 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:29:13 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:29:23 INFO mapreduce.Job: Task Id : attempt_1412635903408_0362_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 23133\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:29:42 INFO mapreduce.Job: Task Id : attempt_1412635903408_0362_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 118018\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:30:01 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/12/01 09:30:01 INFO mapreduce.Job: Job job_1412635903408_0362 failed with state FAILED due to: Task failed task_1412635903408_0362_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:30:01 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=311100\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=9250261\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=201539\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=201539\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=201539\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=206375936\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=31582\r\n",
        "\t\tMap output records=31582\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=141\r\n",
        "\t\tCPU time spent (ms)=15770\r\n",
        "\t\tPhysical memory (bytes) snapshot=960954368\r\n",
        "\t\tVirtual memory (bytes) snapshot=4714942464\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=9250261\r\n",
        "14/12/01 09:30:01 INFO mapreduce.ImportJobBase: Transferred 8.8217 MB in 113.2291 seconds (79.7803 KB/sec)\r\n",
        "14/12/01 09:30:01 INFO mapreduce.ImportJobBase: Retrieved 31582 records.\r\n",
        "14/12/01 09:30:01 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -rmdir {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat using -Ddb2.jcc.charsetDecoderEncoder=3 \n",
      "tgt = 'e2/Y1753AAA_detl_decodeB'\n",
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt} "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:48 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/12/01 09:32:48 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:50 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/4c9fff8f8a575a34d5d25d153e158f59/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/12/01 09:32:52 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/4c9fff8f8a575a34d5d25d153e158f59/FDWATOMCAE.DETL.jar\r\n",
        "14/12/01 09:32:52 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/12/01 09:32:52 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:53 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:32:55 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/12/01 09:32:55 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:33:18 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:33:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0363\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:33:18 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0363\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:33:19 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0363/\r\n",
        "14/12/01 09:33:19 INFO mapreduce.Job: Running job: job_1412635903408_0363\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:33:25 INFO mapreduce.Job: Job job_1412635903408_0363 running in uber mode : false\r\n",
        "14/12/01 09:33:25 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:33:43 INFO mapreduce.Job: Task Id : attempt_1412635903408_0363_m_000000_0, Status : FAILED\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 302188\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:33:50 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:34:01 INFO mapreduce.Job: Task Id : attempt_1412635903408_0363_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 296655\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:34:14 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:34:21 INFO mapreduce.Job:  map 75% reduce 0%\r\n",
        "14/12/01 09:34:21 INFO mapreduce.Job: Task Id : attempt_1412635903408_0363_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 131039\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:34:22 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:34:24 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:34:41 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/12/01 09:34:41 INFO mapreduce.Job: Job job_1412635903408_0363 failed with state FAILED due to: Task failed task_1412635903408_0363_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 09:34:41 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=311100\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=9250261\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=238655\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=238655\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=238655\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=244382720\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=31582\r\n",
        "\t\tMap output records=31582\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=143\r\n",
        "\t\tCPU time spent (ms)=14910\r\n",
        "\t\tPhysical memory (bytes) snapshot=957784064\r\n",
        "\t\tVirtual memory (bytes) snapshot=4728446976\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=9250261\r\n",
        "14/12/01 09:34:41 INFO mapreduce.ImportJobBase: Transferred 8.8217 MB in 108.9452 seconds (82.9174 KB/sec)\r\n",
        "14/12/01 09:34:41 INFO mapreduce.ImportJobBase: Retrieved 31582 records.\r\n",
        "14/12/01 09:34:41 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbipMsg=dbip+\":retrieveMessagesFromServerOnGetMessage=true;\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbipMsg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "'jdbc:db2://10.96.37.166:60100/FDW2P:retrieveMessagesFromServerOnGetMessage=true;'"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "'\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753AAA\\' )\"'"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat using -D db2.jcc.charsetDecoderEncoder=3 \n",
      "tgt = 'e2/Y1753AAA_detl_decodeC'\n",
      "#org.apache.sqoop.manager.Db2Manager\n",
      "\n",
      "!sqoop import -D db2.jcc.charsetDecoderEncoder=3 --connection-manager org.apache.sqoop.manager.Db2Manager --connect {dbipMsg} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt} "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/04 10:14:31 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n",
        "--table or --query is required for import. (Or use sqoop import-all-tables.)\r\n",
        "Try --help for usage instructions.\r\n",
        "usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]\r\n",
        "\r\n",
        "Common arguments:\r\n",
        "   --connect <jdbc-uri>                         Specify JDBC connect\r\n",
        "                                                string\r\n",
        "   --connection-manager <class-name>            Specify connection manager\r\n",
        "                                                class name\r\n",
        "   --connection-param-file <properties-file>    Specify connection\r\n",
        "                                                parameters file\r\n",
        "   --driver <class-name>                        Manually specify JDBC\r\n",
        "                                                driver class to use\r\n",
        "   --hadoop-home <hdir>                         Override\r\n",
        "                                                $HADOOP_MAPRED_HOME_ARG\r\n",
        "   --hadoop-mapred-home <dir>                   Override\r\n",
        "                                                $HADOOP_MAPRED_HOME_ARG\r\n",
        "   --help                                       Print usage instructions\r\n",
        "-P                                              Read password from console\r\n",
        "   --password <password>                        Set authentication\r\n",
        "                                                password\r\n",
        "   --password-file <password-file>              Set authentication\r\n",
        "                                                password file path\r\n",
        "   --relaxed-isolation                          Use read-uncommitted\r\n",
        "                                                isolation for imports\r\n",
        "   --skip-dist-cache                            Skip copying jars to\r\n",
        "                                                distributed cache\r\n",
        "   --username <username>                        Set authentication\r\n",
        "                                                username\r\n",
        "   --verbose                                    Print more information\r\n",
        "                                                while working\r\n",
        "\r\n",
        "Import control arguments:\r\n",
        "   --append                                                   Imports data\r\n",
        "                                                              in append\r\n",
        "                                                              mode\r\n",
        "   --as-avrodatafile                                          Imports data\r\n",
        "                                                              to Avro data\r\n",
        "                                                              files\r\n",
        "   --as-sequencefile                                          Imports data\r\n",
        "                                                              to\r\n",
        "                                                              SequenceFile\r\n",
        "                                                              s\r\n",
        "   --as-textfile                                              Imports data\r\n",
        "                                                              as plain\r\n",
        "                                                              text\r\n",
        "                                                              (default)\r\n",
        "   --boundary-query <statement>                               Set boundary\r\n",
        "                                                              query for\r\n",
        "                                                              retrieving\r\n",
        "                                                              max and min\r\n",
        "                                                              value of the\r\n",
        "                                                              primary key\r\n",
        "   --columns <col,col,col...>                                 Columns to\r\n",
        "                                                              import from\r\n",
        "                                                              table\r\n",
        "   --compression-codec <codec>                                Compression\r\n",
        "                                                              codec to use\r\n",
        "                                                              for import\r\n",
        "   --delete-target-dir                                        Imports data\r\n",
        "                                                              in delete\r\n",
        "                                                              mode\r\n",
        "   --direct                                                   Use direct\r\n",
        "                                                              import fast\r\n",
        "                                                              path\r\n",
        "   --direct-split-size <n>                                    Split the\r\n",
        "                                                              input stream\r\n",
        "                                                              every 'n'\r\n",
        "                                                              bytes when\r\n",
        "                                                              importing in\r\n",
        "                                                              direct mode\r\n",
        "-e,--query <statement>                                        Import\r\n",
        "                                                              results of\r\n",
        "                                                              SQL\r\n",
        "                                                              'statement'\r\n",
        "   --fetch-size <n>                                           Set number\r\n",
        "                                                              'n' of rows\r\n",
        "                                                              to fetch\r\n",
        "                                                              from the\r\n",
        "                                                              database\r\n",
        "                                                              when more\r\n",
        "                                                              rows are\r\n",
        "                                                              needed\r\n",
        "   --inline-lob-limit <n>                                     Set the\r\n",
        "                                                              maximum size\r\n",
        "                                                              for an\r\n",
        "                                                              inline LOB\r\n",
        "-m,--num-mappers <n>                                          Use 'n' map\r\n",
        "                                                              tasks to\r\n",
        "                                                              import in\r\n",
        "                                                              parallel\r\n",
        "   --mapreduce-job-name <name>                                Set name for\r\n",
        "                                                              generated\r\n",
        "                                                              mapreduce\r\n",
        "                                                              job\r\n",
        "   --split-by <column-name>                                   Column of\r\n",
        "                                                              the table\r\n",
        "                                                              used to\r\n",
        "                                                              split work\r\n",
        "                                                              units\r\n",
        "   --table <table-name>                                       Table to\r\n",
        "                                                              read\r\n",
        "   --target-dir <dir>                                         HDFS plain\r\n",
        "                                                              table\r\n",
        "                                                              destination\r\n",
        "   --validate                                                 Validate the\r\n",
        "                                                              copy using\r\n",
        "                                                              the\r\n",
        "                                                              configured\r\n",
        "                                                              validator\r\n",
        "   --validation-failurehandler <validation-failurehandler>    Fully\r\n",
        "                                                              qualified\r\n",
        "                                                              class name\r\n",
        "                                                              for\r\n",
        "                                                              ValidationFa\r\n",
        "                                                              ilureHandler\r\n",
        "   --validation-threshold <validation-threshold>              Fully\r\n",
        "                                                              qualified\r\n",
        "                                                              class name\r\n",
        "                                                              for\r\n",
        "                                                              ValidationTh\r\n",
        "                                                              reshold\r\n",
        "   --validator <validator>                                    Fully\r\n",
        "                                                              qualified\r\n",
        "                                                              class name\r\n",
        "                                                              for the\r\n",
        "                                                              Validator\r\n",
        "   --warehouse-dir <dir>                                      HDFS parent\r\n",
        "                                                              for table\r\n",
        "                                                              destination\r\n",
        "   --where <where clause>                                     WHERE clause\r\n",
        "                                                              to use\r\n",
        "                                                              during\r\n",
        "                                                              import\r\n",
        "-z,--compress                                                 Enable\r\n",
        "                                                              compression\r\n",
        "\r\n",
        "Incremental import arguments:\r\n",
        "   --check-column <column>        Source column to check for incremental\r\n",
        "                                  change\r\n",
        "   --incremental <import-type>    Define an incremental import of type\r\n",
        "                                  'append' or 'lastmodified'\r\n",
        "   --last-value <value>           Last imported value in the incremental\r\n",
        "                                  check column\r\n",
        "\r\n",
        "Output line formatting arguments:\r\n",
        "   --enclosed-by <char>               Sets a required field enclosing\r\n",
        "                                      character\r\n",
        "   --escaped-by <char>                Sets the escape character\r\n",
        "   --fields-terminated-by <char>      Sets the field separator character\r\n",
        "   --lines-terminated-by <char>       Sets the end-of-line character\r\n",
        "   --mysql-delimiters                 Uses MySQL's default delimiter set:\r\n",
        "                                      fields: ,  lines: \\n  escaped-by: \\\r\n",
        "                                      optionally-enclosed-by: '\r\n",
        "   --optionally-enclosed-by <char>    Sets a field enclosing character\r\n",
        "\r\n",
        "Input parsing arguments:\r\n",
        "   --input-enclosed-by <char>               Sets a required field encloser\r\n",
        "   --input-escaped-by <char>                Sets the input escape\r\n",
        "                                            character\r\n",
        "   --input-fields-terminated-by <char>      Sets the input field separator\r\n",
        "   --input-lines-terminated-by <char>       Sets the input end-of-line\r\n",
        "                                            char\r\n",
        "   --input-optionally-enclosed-by <char>    Sets a field enclosing\r\n",
        "                                            character\r\n",
        "\r\n",
        "Hive arguments:\r\n",
        "   --create-hive-table                         Fail if the target hive\r\n",
        "                                               table exists\r\n",
        "   --hive-database <database-name>             Sets the database name to\r\n",
        "                                               use when importing to hive\r\n",
        "   --hive-delims-replacement <arg>             Replace Hive record \\0x01\r\n",
        "                                               and row delimiters (\\n\\r)\r\n",
        "                                               from imported string fields\r\n",
        "                                               with user-defined string\r\n",
        "   --hive-drop-import-delims                   Drop Hive record \\0x01 and\r\n",
        "                                               row delimiters (\\n\\r) from\r\n",
        "                                               imported string fields\r\n",
        "   --hive-home <dir>                           Override $HIVE_HOME\r\n",
        "   --hive-import                               Import tables into Hive\r\n",
        "                                               (Uses Hive's default\r\n",
        "                                               delimiters if none are\r\n",
        "                                               set.)\r\n",
        "   --hive-overwrite                            Overwrite existing data in\r\n",
        "                                               the Hive table\r\n",
        "   --hive-partition-key <partition-key>        Sets the partition key to\r\n",
        "                                               use when importing to hive\r\n",
        "   --hive-partition-value <partition-value>    Sets the partition value to\r\n",
        "                                               use when importing to hive\r\n",
        "   --hive-table <table-name>                   Sets the table name to use\r\n",
        "                                               when importing to hive\r\n",
        "   --map-column-hive <arg>                     Override mapping for\r\n",
        "                                               specific column to hive\r\n",
        "                                               types.\r\n",
        "\r\n",
        "HBase arguments:\r\n",
        "   --column-family <family>    Sets the target column family for the\r\n",
        "                               import\r\n",
        "   --hbase-bulkload            Enables HBase bulk loading\r\n",
        "   --hbase-create-table        If specified, create missing HBase tables\r\n",
        "   --hbase-row-key <col>       Specifies which input column to use as the\r\n",
        "                               row key\r\n",
        "   --hbase-table <table>       Import to <table> in HBase\r\n",
        "\r\n",
        "HCatalog arguments:\r\n",
        "   --hcatalog-database <arg>                   HCatalog database name\r\n",
        "   --hcatalog-home <hdir>                      Override $HCAT_HOME\r\n",
        "   --hcatalog-table <arg>                      HCatalog table name\r\n",
        "   --hive-home <dir>                           Override $HIVE_HOME\r\n",
        "   --hive-partition-key <partition-key>        Sets the partition key to\r\n",
        "                                               use when importing to hive\r\n",
        "   --hive-partition-value <partition-value>    Sets the partition value to\r\n",
        "                                               use when importing to hive\r\n",
        "   --map-column-hive <arg>                     Override mapping for\r\n",
        "                                               specific column to hive\r\n",
        "                                               types.\r\n",
        "\r\n",
        "HCatalog import specific options:\r\n",
        "   --create-hcatalog-table            Create HCatalog before import\r\n",
        "   --hcatalog-storage-stanza <arg>    HCatalog storage stanza for table\r\n",
        "                                      creation\r\n",
        "\r\n",
        "Accumulo arguments:\r\n",
        "   --accumulo-batch-size <size>          Batch size in bytes\r\n",
        "   --accumulo-column-family <family>     Sets the target column family for\r\n",
        "                                         the import\r\n",
        "   --accumulo-create-table               If specified, create missing\r\n",
        "                                         Accumulo tables\r\n",
        "   --accumulo-instance <instance>        Accumulo instance name.\r\n",
        "   --accumulo-max-latency <latency>      Max write latency in milliseconds\r\n",
        "   --accumulo-password <password>        Accumulo password.\r\n",
        "   --accumulo-row-key <col>              Specifies which input column to\r\n",
        "                                         use as the row key\r\n",
        "   --accumulo-table <table>              Import to <table> in Accumulo\r\n",
        "   --accumulo-user <user>                Accumulo user name.\r\n",
        "   --accumulo-visibility <vis>           Visibility token to be applied to\r\n",
        "                                         all rows imported\r\n",
        "   --accumulo-zookeepers <zookeepers>    Comma-separated list of\r\n",
        "                                         zookeepers (host:port)\r\n",
        "\r\n",
        "Code generation arguments:\r\n",
        "   --bindir <dir>                        Output directory for compiled\r\n",
        "                                         objects\r\n",
        "   --class-name <name>                   Sets the generated class name.\r\n",
        "                                         This overrides --package-name.\r\n",
        "                                         When combined with --jar-file,\r\n",
        "                                         sets the input class.\r\n",
        "   --input-null-non-string <null-str>    Input null non-string\r\n",
        "                                         representation\r\n",
        "   --input-null-string <null-str>        Input null string representation\r\n",
        "   --jar-file <file>                     Disable code generation; use\r\n",
        "                                         specified jar\r\n",
        "   --map-column-java <arg>               Override mapping for specific\r\n",
        "                                         columns to java types\r\n",
        "   --null-non-string <null-str>          Null non-string representation\r\n",
        "   --null-string <null-str>              Null string representation\r\n",
        "   --outdir <dir>                        Output directory for generated\r\n",
        "                                         code\r\n",
        "   --package-name <name>                 Put auto-generated classes in\r\n",
        "                                         this package\r\n",
        "\r\n",
        "Generic Hadoop command-line arguments:\r\n",
        "(must preceed any tool-specific arguments)\r\n",
        "Generic options supported are\r\n",
        "-conf <configuration file>     specify an application configuration file\r\n",
        "-D <property=value>            use value for given property\r\n",
        "-fs <local|namenode:port>      specify a namenode\r\n",
        "-jt <local|jobtracker:port>    specify a job tracker\r\n",
        "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\r\n",
        "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\r\n",
        "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\r\n",
        "\r\n",
        "The general command line syntax is\r\n",
        "bin/hadoop command [genericOptions] [commandOptions]\r\n",
        "\r\n",
        "\r\n",
        "At minimum, you must specify --connect and --table\r\n",
        "Arguments to mysqldump and other subprograms may be supplied\r\n",
        "after a '--' on the command line.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/bin/sh: --username: not found\r\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "'\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753AAA\\' ) AND DETL_DIM_ID = 153342540\"'"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID = 1533425450 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'jdbc:db2://10.96.37.166:60100/FDW2P'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# using --where clause # default\n",
      "tgt = 'e2/test1'\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wclause} --target-dir {tgt}  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:48 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/12/01 08:45:48 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:50 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/b184b1b26acb025535c69d940374cc19/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/12/01 08:45:52 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/b184b1b26acb025535c69d940374cc19/FDWATOMCAE.DETL.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:52 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/12/01 08:45:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/12/01 08:45:52 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:52 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:45:55 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/12/01 08:45:55 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID = 153342540 )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:46:23 INFO mapreduce.JobSubmitter: number of splits:1\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:46:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0359\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:46:24 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0359\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:46:24 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0359/\r\n",
        "14/12/01 08:46:24 INFO mapreduce.Job: Running job: job_1412635903408_0359\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:46:30 INFO mapreduce.Job: Job job_1412635903408_0359 running in uber mode : false\r\n",
        "14/12/01 08:46:30 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:46:37 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/12/01 08:46:37 INFO mapreduce.Job: Job job_1412635903408_0359 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/12/01 08:46:37 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=103574\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=119\r\n",
        "\t\tHDFS: Number of bytes written=0\r\n",
        "\t\tHDFS: Number of read operations=4\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=2\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=1\r\n",
        "\t\tOther local map tasks=1\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=4507\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=4507\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=4507\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=4615168\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=0\r\n",
        "\t\tMap output records=0\r\n",
        "\t\tInput split bytes=119\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=22\r\n",
        "\t\tCPU time spent (ms)=1310\r\n",
        "\t\tPhysical memory (bytes) snapshot=325427200\r\n",
        "\t\tVirtual memory (bytes) snapshot=1580871680\r\n",
        "\t\tTotal committed heap usage (bytes)=792199168\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=0\r\n",
        "14/12/01 08:46:37 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 45.2443 seconds (0 bytes/sec)\r\n",
        "14/12/01 08:46:37 INFO mapreduce.ImportJobBase: Retrieved 0 records.\r\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat using -D db2.jcc.charsetDecoderEncoder=3 and "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## running with updated jar file 11.03.14"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect {dbip} --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8192' )\" --target-dir e2/tmp1 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" \n",
      "            #--splity-by DETL_DIM_ID\n",
      "#    \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8192' ) AND timestamp(FDW_RPLC_TSTMP)='9999-12-31 23:59:59.999999'\"\n",
      "#Y1753AAA\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:37 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:38 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/03 11:53:38 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:40 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:40 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/293d0c0ae8fba95182eb72429f68c530/sf/datascience/vrp/avro/export/DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/03 11:53:42 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/293d0c0ae8fba95182eb72429f68c530/sf.datascience.vrp.avro.export.DETL.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:42 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/11/03 11:53:42 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/03 11:53:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:43 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:43 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:43 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-kesj/compile/293d0c0ae8fba95182eb72429f68c530/DETL.avsc\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:43 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:53:45 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/03 11:53:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8192' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:54:24 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:54:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0271\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:54:24 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0271\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:54:24 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0271/\r\n",
        "14/11/03 11:54:24 INFO mapreduce.Job: Running job: job_1412635903408_0271\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:54:30 INFO mapreduce.Job: Job job_1412635903408_0271 running in uber mode : false\r\n",
        "14/11/03 11:54:31 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:54:51 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:07 INFO mapreduce.Job: Job job_1412635903408_0271 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:07 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=440436\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=521\r\n",
        "\t\tHDFS: Number of bytes written=944940110\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=659413\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=659413\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=659413\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=675238912\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=3210697\r\n",
        "\t\tMap output records=3210697\r\n",
        "\t\tInput split bytes=521\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=2616\r\n",
        "\t\tCPU time spent (ms)=238000\r\n",
        "\t\tPhysical memory (bytes) snapshot=724185088\r\n",
        "\t\tVirtual memory (bytes) snapshot=6282031104\r\n",
        "\t\tTotal committed heap usage (bytes)=1993342976\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=944940110\r\n",
        "14/11/03 11:58:07 INFO mapreduce.ImportJobBase: Transferred 901.1651 MB in 263.649 seconds (3.418 MB/sec)\r\n",
        "14/11/03 11:58:07 INFO mapreduce.ImportJobBase: Retrieved 3210697 records.\r\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -rmdir e2/tmp2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "!sqoop import --connect {dbip} --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' )\" --target-dir e2/tmp2 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:55 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/03 11:58:55 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:57 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/dbf5af5000d7051e38d73945fcf68c01/sf/datascience/vrp/avro/export/DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/03 11:58:59 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/dbf5af5000d7051e38d73945fcf68c01/sf.datascience.vrp.avro.export.DETL.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:58:59 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/11/03 11:58:59 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/03 11:58:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:00 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-kesj/compile/dbf5af5000d7051e38d73945fcf68c01/DETL.avsc\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:00 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:03 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/03 11:59:03 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:25 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0272\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:26 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0272\r\n",
        "14/11/03 11:59:26 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0272/\r\n",
        "14/11/03 11:59:26 INFO mapreduce.Job: Running job: job_1412635903408_0272\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:32 INFO mapreduce.Job: Job job_1412635903408_0272 running in uber mode : false\r\n",
        "14/11/03 11:59:32 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:51 INFO mapreduce.Job: Task Id : attempt_1412635903408_0272_m_000000_0, Status : FAILED\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 141068\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 11:59:55 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:00 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:01 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:11 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/03 12:00:11 INFO mapreduce.Job: Task Id : attempt_1412635903408_0272_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 97652\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:12 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:29 INFO mapreduce.Job: Task Id : attempt_1412635903408_0272_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 313495\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:48 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:49 INFO mapreduce.Job: Job job_1412635903408_0272 failed with state FAILED due to: Task failed task_1412635903408_0272_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:00:49 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=330330\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=8947134\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=200490\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=200490\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=200490\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=205301760\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=30872\r\n",
        "\t\tMap output records=30872\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=153\r\n",
        "\t\tCPU time spent (ms)=18050\r\n",
        "\t\tPhysical memory (bytes) snapshot=982310912\r\n",
        "\t\tVirtual memory (bytes) snapshot=4742144000\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=8947134\r\n",
        "14/11/03 12:00:49 INFO mapreduce.ImportJobBase: Transferred 8.5327 MB in 109.2102 seconds (80.0057 KB/sec)\r\n",
        "14/11/03 12:00:49 INFO mapreduce.ImportJobBase: Retrieved 30872 records.\r\n",
        "14/11/03 12:00:49 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' )\" --target-dir e2/tmp3 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:36 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:37 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/03 12:06:37 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:39 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/bc8daca347c7a33d4f6078b30aa1062e/sf/datascience/vrp/avro/export/DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/03 12:06:41 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/bc8daca347c7a33d4f6078b30aa1062e/sf.datascience.vrp.avro.export.DETL.jar\r\n",
        "14/11/03 12:06:41 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:41 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/03 12:06:41 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:42 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-kesj/compile/bc8daca347c7a33d4f6078b30aa1062e/DETL.avsc\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:43 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:06:45 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/03 12:06:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:07 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0275\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:08 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0275\r\n",
        "14/11/03 12:07:08 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0275/\r\n",
        "14/11/03 12:07:08 INFO mapreduce.Job: Running job: job_1412635903408_0275\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:14 INFO mapreduce.Job: Job job_1412635903408_0275 running in uber mode : false\r\n",
        "14/11/03 12:07:14 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:33 INFO mapreduce.Job:  map 25% reduce 0%\r\n",
        "14/11/03 12:07:33 INFO mapreduce.Job: Task Id : attempt_1412635903408_0275_m_000000_0, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 8228\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:34 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:37 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:42 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:43 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:07:55 INFO mapreduce.Job: Task Id : attempt_1412635903408_0275_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 87333\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:08:14 INFO mapreduce.Job: Task Id : attempt_1412635903408_0275_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 66698\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:08:33 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:08:33 INFO mapreduce.Job: Job job_1412635903408_0275 failed with state FAILED due to: Task failed task_1412635903408_0275_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:08:33 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=330744\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=8947134\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=204361\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=204361\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=204361\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=209265664\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=30872\r\n",
        "\t\tMap output records=30872\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=144\r\n",
        "\t\tCPU time spent (ms)=17970\r\n",
        "\t\tPhysical memory (bytes) snapshot=987443200\r\n",
        "\t\tVirtual memory (bytes) snapshot=4739182592\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=8947134\r\n",
        "14/11/03 12:08:33 INFO mapreduce.ImportJobBase: Transferred 8.5327 MB in 111.1273 seconds (78.6255 KB/sec)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:08:33 INFO mapreduce.ImportJobBase: Retrieved 30872 records.\r\n",
        "14/11/03 12:08:33 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 -DcharacterEncoding=UTF8 -Dlocale=us_en --username kesj --password-file {nfile} --connect jdbc:db2://10.96.37.166:60100/FDW2P -table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' )\" --target-dir e2/tmp4 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:17:55 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:17:56 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/03 12:17:56 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:17:58 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:17:58 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:17:58 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/83503f793e97c0dda243d9a5241140ef/sf/datascience/vrp/avro/export/DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/03 12:18:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/83503f793e97c0dda243d9a5241140ef/sf.datascience.vrp.avro.export.DETL.jar\r\n",
        "14/11/03 12:18:00 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/03 12:18:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:00 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:03 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/03 12:18:03 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:24 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0276\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:25 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0276\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:25 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0276/\r\n",
        "14/11/03 12:18:25 INFO mapreduce.Job: Running job: job_1412635903408_0276\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:31 INFO mapreduce.Job: Job job_1412635903408_0276 running in uber mode : false\r\n",
        "14/11/03 12:18:31 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:50 INFO mapreduce.Job: Task Id : attempt_1412635903408_0276_m_000000_0, Status : FAILED\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 262160\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:54 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:18:59 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:19:00 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:19:08 INFO mapreduce.Job: Task Id : attempt_1412635903408_0276_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 268142\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:19:27 INFO mapreduce.Job: Task Id : attempt_1412635903408_0276_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat sf.datascience.vrp.avro.export.DETL.readFields(DETL.java:900)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 83748\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:19:47 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/03 12:19:47 INFO mapreduce.Job: Job job_1412635903408_0276 failed with state FAILED due to: Task failed task_1412635903408_0276_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/03 12:19:47 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=311925\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=9038381\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=197537\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=197537\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=197537\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=202277888\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=30872\r\n",
        "\t\tMap output records=30872\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=149\r\n",
        "\t\tCPU time spent (ms)=16060\r\n",
        "\t\tPhysical memory (bytes) snapshot=953507840\r\n",
        "\t\tVirtual memory (bytes) snapshot=4735176704\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=9038381\r\n",
        "14/11/03 12:19:47 INFO mapreduce.ImportJobBase: Transferred 8.6197 MB in 106.5284 seconds (82.8563 KB/sec)\r\n",
        "14/11/03 12:19:47 INFO mapreduce.ImportJobBase: Retrieved 30872 records.\r\n",
        "14/11/03 12:19:47 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now look at the list of ones that failed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aaaResults = !hadoop fs -ls ehunt/Y1753AAA_DETL/*/_SUCCESS\n",
      "print len(aaaResults)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aaaResults"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import  -DcharacterEncoding=utf8 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512' )\" --target-dir vrp_data/ARM8512_DETL -m 1 --class-name \"sf.datascience.vrp.avro.export.DETL\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listOfVNDRCDS = ['ARM8522', '', 'Y1753CAA', '910698', 'Y1753BAD', 'Y1753BAA',\n",
      "       'Y1753DAA', 'Y1753DAD', 'Y1753AAA', 'Y1753CAD', 'ARM8427',\n",
      "       'ARM8516', 'ARM8192', '911754', 'ARM8545', 'Y2114AER', 'ARM8512',\n",
      "       '910027', 'ARM8530', 'ARM8416', '910848']\n",
      "#for vcd in listOfVNDRCDS[3:]:\n",
      "#    print vcd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAA' )\" --target-dir vrp_data/Y1753DAA_DETL -m 1 --class-name \"sf.datascience.vrp.avro.export.DETL\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Repeat, pulling in parallel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls 'vrp_data/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAA' )\" --target-dir vrp_data/Y1753DAA_DETLp --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nDETLlines = {}\n",
      "for vcd in listOfVNDRCDS:\n",
      "    if vcd != '':\n",
      "        nDETLlines[vcd]=0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Try avoiding AVRO for one that works"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vndrCd = 'Y1753CAD'\n",
      "tgt = 'vrp/'+vndrCd+'_DETL'\n",
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'\"+vndrCd+\"'\"+' )\"'\n",
      "\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --table FDWATOMCAE.DETL --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --fields-terminated-by , --escaped-by \\\\ --enclosed-by '\\\"'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vndrCd = 'Y1753AAA'\n",
      "tgt = 'vrp/'+vndrCd+'_DETL'\n",
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'\"+vndrCd+\"'\"+' )\"'\n",
      "\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --table FDWATOMCAE.DETL --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --fields-terminated-by , --escaped-by \\\\ --enclosed-by '\\\"'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lVNDRCDS[:12]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8416' )\" --target-dir vrp_data/ARM8416_DETL --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls vrp_data/*_DETL/_SUCCESS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls vrp_data/Y2*_DETL/_SUCCESS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import -D db2.jcc.charsetDecoderEncoder=3 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8516' ) \" --target-dir vrp_data/ARM8516_DETL --split-by DETL_DIM_ID  --class-name \"sf.datascience.vrp.avro.export.DETL\" \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password {myPSWD} --as-avrodatafile --table FDWATOMCAE.LOS_EST --where \"WHERE VNDR_VEH_CD='ARM8522'\" --target-dir vrp_data/ARM8522_DETLLOS_EST --split-by \"\" --class-name \"sf.datascience.vrp.avro.export.LOS_EST\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for vcd in listOfVNDRCDS[3:]:\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 10.24.14\n",
      "# looking at pulling all of DETL records?? How big is this?\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --table FDWATOMCAE.DETL  --target-dir vrp/detl --split-by DETL_DIM_ID  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls vrp_data/ARM*"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pulling ARM8512\n",
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file {nfile} --table FDWATOMCAE.DETL  --target-dir vrp_data/ARM8512_DETL --split-by DETL_DIM_ID   --as-avrodatafile --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512')\"  --class-name \"sf.datascience.vrp.avro.export.DETL\" -m 16 \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL  --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512') and DETL_DIM_ID AND \\$CONDITIONS\" --target-dir vrp/detl --split-by DETL_DIM_ID  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Look at excluding the column that caused that error?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clisting = '\"DETL_DIM_ID , LINE_PRT_STTS_CD , LINE_LBR_STTS_CD ,LINE_NUM ,PARNT_LINE_NUM ,LINE_INCLD_IND ,LINE_SUPP_NUM ,MSG_CD ,VNDR_REFR_CD ,LINE_ADJST_APLY_CD ,LBR_OPRTN_CD ,PRT_TYPE_CD ,LBR_TYPE_CD ,MTRL_TYPE_CD ,LINE_ADJST_TYPE_CD ,LINE_ADJST_PCT ,LINE_ADJST_AMT ,OEM_PRT_NUM ,NON_OEM_PRT_NUM ,EST_PRT_NUM ,PRT_CLAS_CD ,SUPLR_REFR_ID ,PRT_QTY_CNT ,LBR_AMT ,PRICE_AMT ,DBASE_PRICE_AMT ,PRT_PRICE_INCLD_IND ,PRT_PRICE_JDGT_IND ,LBR_NOTE_IND ,AFMRKT_PRT_USE_CD ,CRTFY_PRT_IND ,GLSS_PRT_IND ,PRT_TAX_IND ,LBR_TAX_IND ,PAINT_HR_CNT ,PAINT_HR_DBASE_CNT ,LBR_HR_CNT ,LBR_HR_CALC_CNT ,LBR_HR_DBASE_CNT ,LBR_INCLD_IND ,LBR_HR_JDGT_IND ,PAINT_HR_JDGT_IND ,LBR_TYPE_JDGT_IND ,LBR_OPRTN_JDGT_IND ,PAINT_STG_IND ,PAINT_TONE_IND ,PAINT_INCLD_IND ,UNIQ_LINE_NUM ,LINE_ITEM_CTGRY_CD ,ALT_PRT_IND ,DBASE_LBR_TYPE_CD ,LBR_ADJST_HR_CNT ,OTH_CHRG_UOM_CD ,OTH_CHRG_PRICE_INCLD_IND ,LINE_ADJST_TXBL_IND ,MANL_LINE_IND ,AUTOM_ENT_IND ,CLM_ID ,VNDR_CD ,VER_NUM ,DATA_CNTXT_CD ,FDW_RPLC_IND ,FDW_INSRT_TSTMP ,FDW_RPLC_TSTMP ,EST_VER_NUM ,SRC_INSRT_TSTMP ,LOS_EST_BUSN_ID ,LOS_EST_DIM_ID \"'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clisting"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listOfVNDRCDS = sort(listOfVNDRCDS)\n",
      "lVNDRCDS = listOfVNDRCDS[1:]\n",
      "lVNDRCDS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL  --target-dir vrp_data/arm8512_DETL --split-by DETL_DIM_ID   --as-avrodatafile --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512')\" --class-name \"sf.datascience.vrp.avro.export.DETL\" --columns {clisting} "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for vndrCd in lVNDRCDS:\n",
      "    tgt = 'vrpdata/'+vndrCd+'_DETL'\n",
      "    wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'\"+vndrCd+\"'\"+' )\"'\n",
      "    #print wclause\n",
      "    #print vndrCd,tgt\n",
      "    #!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=910027')\" --target-dir {tgt} --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" \n",
      "    !sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID -m 8 --class-name \"sf.datascience.vrp.avro.export.DETL\" --columns {clisting}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls vrpdata/*DETL/_SUCCESS\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Get all the agent info"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table V23.AGENTAFO_V  --target-dir data/agentafo -m 1 --as-avrodatafile "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table V23.AUTO_MONTHLY_SS_V  --target-dir data/autoMonthlySS -m 1 --as-avrodatafile "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v23tbls = ['AGENTAFO_V','AUTO_MONTHLY_SS_V',\n",
      "           'AUTO_LOS_YTD_SNAPSHOT_V','AUTO_PREM_YTD_SNAPSHOT_V'\n",
      "           ,'FIRE_PREM_YTD_SNAPSHOT_V','FIRE_PREMIUM_MONTHLY_V']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table V23.AUTO_PREM_YTD_SNAPSHOT_V  --target-dir data/autoPREMytdSS -m 1 --as-avrodatafile "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## try to pull all of the toyota camrys from 2007\n",
      "1. get LOS_EST tables --> restrict to 'active' ones\n",
      "2. get DETL tables -->"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tgt = 'toyota/camry2007/losest'\n",
      "today = '2014-10-31'\n",
      "wclause0 = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' )'\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.LOS_EST --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID -m 8 --class-name \"sf.datascience.vrp.avro.export.DETL\" --columns {clisting}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "'jdbc:db2://10.96.37.166:60100/FDW2P'"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### try importing as a hive table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )\"'\n",
      "\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID --hive-import --hive-table Y1753DADdetl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:39 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:41 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override\r\n",
        "14/11/04 10:05:41 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:41 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/04 10:05:41 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:43 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/8e2fbc324b5fe7e2a2bd0d5bf96f5d37/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:45 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/8e2fbc324b5fe7e2a2bd0d5bf96f5d37/FDWATOMCAE.DETL.jar\r\n",
        "14/11/04 10:05:45 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/11/04 10:05:45 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/04 10:05:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:45 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:45 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:05:47 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/04 10:05:47 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:26 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0279\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:26 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0279\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:26 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0279/\r\n",
        "14/11/04 10:06:26 INFO mapreduce.Job: Running job: job_1412635903408_0279\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:32 INFO mapreduce.Job: Job job_1412635903408_0279 running in uber mode : false\r\n",
        "14/11/04 10:06:32 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:52 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:56 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:06:59 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:03 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:20 INFO mapreduce.Job: Job job_1412635903408_0279 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:20 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=414212\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=521\r\n",
        "\t\tHDFS: Number of bytes written=20965374\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=171509\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=171509\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=171509\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=175625216\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=71588\r\n",
        "\t\tMap output records=71588\r\n",
        "\t\tInput split bytes=521\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=253\r\n",
        "\t\tCPU time spent (ms)=23470\r\n",
        "\t\tPhysical memory (bytes) snapshot=1225986048\r\n",
        "\t\tVirtual memory (bytes) snapshot=6286651392\r\n",
        "\t\tTotal committed heap usage (bytes)=2544893952\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=20965374\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:20 INFO mapreduce.ImportJobBase: Transferred 19.9941 MB in 95.09 seconds (215.3117 KB/sec)\r\n",
        "14/11/04 10:07:20 INFO mapreduce.ImportJobBase: Retrieved 71588 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column LINE_ADJST_PCT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column LINE_ADJST_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column PRT_QTY_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column LBR_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column PRICE_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column DBASE_PRICE_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column PAINT_HR_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column PAINT_HR_DBASE_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column LBR_HR_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column LBR_HR_CALC_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column LBR_HR_DBASE_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column LBR_ADJST_HR_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column FDW_INSRT_TSTMP had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column FDW_RPLC_TSTMP had to be cast to a less precise type in Hive\r\n",
        "14/11/04 10:07:21 WARN hive.TableDefWriter: Column SRC_INSRT_TSTMP had to be cast to a less precise type in Hive\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:21 INFO hive.HiveImport: Loading uploaded data into Hive\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:23 INFO hive.HiveImport: 14/11/04 10:07:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\r\n",
        "14/11/04 10:07:23 INFO hive.HiveImport: 14/11/04 10:07:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize\r\n",
        "14/11/04 10:07:23 INFO hive.HiveImport: 14/11/04 10:07:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize\r\n",
        "14/11/04 10:07:23 INFO hive.HiveImport: 14/11/04 10:07:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack\r\n",
        "14/11/04 10:07:23 INFO hive.HiveImport: 14/11/04 10:07:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node\r\n",
        "14/11/04 10:07:23 INFO hive.HiveImport: 14/11/04 10:07:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\r\n",
        "14/11/04 10:07:23 INFO hive.HiveImport: 14/11/04 10:07:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:23 INFO hive.HiveImport: \r\n",
        "14/11/04 10:07:23 INFO hive.HiveImport: Logging initialized using configuration in file:/opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/etc/hive/conf.dist/hive-log4j.properties\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:25 INFO hive.HiveImport: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table Y1753DADdetl\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:960)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1163)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 24 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.lang.reflect.InvocationTargetException\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 29 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true, username = APP. Terminating connection pool. Original Exception: ------\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: ------\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: NestedThrowables:\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true, username = APP. Terminating connection pool. Original Exception: ------\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: ------\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:781)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 34 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true, username = APP. Terminating connection pool. Original Exception: ------\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: ------\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:312)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 63 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 76 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 10:07:25 INFO hive.HiveImport: \t... 87 more\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:26 INFO hive.HiveImport: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 10:07:26 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Hive exited with status 1\r\n",
        "\tat org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:385)\r\n",
        "\tat org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:335)\r\n",
        "\tat org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:239)\r\n",
        "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:425)\r\n",
        "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\r\n",
        "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\r\n",
        "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n",
        "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\r\n",
        "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\r\n",
        "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\r\n",
        "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+'\"'\n",
      "\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.LOS_EST --where {wclause} --split-by LOS_EST_DIM_ID --hive-import --hive-table Y1753DADdetl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VEH_NDR_VEH_CD='Y1753DAD' )\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )\"'\n",
      "\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID --null-string '\\\\N' --null-non-string '\\\\N' --hive-import\n",
      "            #--hive-import --hive-table Y1753AAA.DETL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:27 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override\r\n",
        "14/11/04 11:54:27 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:27 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/04 11:54:27 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:29 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/cc352e2eacba240d11ca2fa32829de24/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/04 11:54:31 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/cc352e2eacba240d11ca2fa32829de24/FDWATOMCAE.DETL.jar\r\n",
        "14/11/04 11:54:31 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/11/04 11:54:31 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/04 11:54:31 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:31 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:54:33 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/04 11:54:33 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:55:12 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:55:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0283\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:55:12 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0283\r\n",
        "14/11/04 11:55:12 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0283/\r\n",
        "14/11/04 11:55:12 INFO mapreduce.Job: Running job: job_1412635903408_0283\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:55:19 INFO mapreduce.Job: Job job_1412635903408_0283 running in uber mode : false\r\n",
        "14/11/04 11:55:19 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:55:38 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:55:42 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:55:48 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:04 INFO mapreduce.Job: Job job_1412635903408_0283 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:04 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=414212\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=521\r\n",
        "\t\tHDFS: Number of bytes written=20965374\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=168942\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=168942\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=168942\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=172996608\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=71588\r\n",
        "\t\tMap output records=71588\r\n",
        "\t\tInput split bytes=521\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=261\r\n",
        "\t\tCPU time spent (ms)=23540\r\n",
        "\t\tPhysical memory (bytes) snapshot=1227841536\r\n",
        "\t\tVirtual memory (bytes) snapshot=6296117248\r\n",
        "\t\tTotal committed heap usage (bytes)=2545418240\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=20965374\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:04 INFO mapreduce.ImportJobBase: Transferred 19.9941 MB in 93.276 seconds (219.4992 KB/sec)\r\n",
        "14/11/04 11:56:04 INFO mapreduce.ImportJobBase: Retrieved 71588 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column LINE_ADJST_PCT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column LINE_ADJST_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column PRT_QTY_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column LBR_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column PRICE_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column DBASE_PRICE_AMT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column PAINT_HR_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column PAINT_HR_DBASE_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column LBR_HR_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column LBR_HR_CALC_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column LBR_HR_DBASE_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column LBR_ADJST_HR_CNT had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column FDW_INSRT_TSTMP had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column FDW_RPLC_TSTMP had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 WARN hive.TableDefWriter: Column SRC_INSRT_TSTMP had to be cast to a less precise type in Hive\r\n",
        "14/11/04 11:56:05 INFO hive.HiveImport: Loading uploaded data into Hive\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:07 INFO hive.HiveImport: 14/11/04 11:56:07 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\r\n",
        "14/11/04 11:56:07 INFO hive.HiveImport: 14/11/04 11:56:07 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize\r\n",
        "14/11/04 11:56:07 INFO hive.HiveImport: 14/11/04 11:56:07 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize\r\n",
        "14/11/04 11:56:07 INFO hive.HiveImport: 14/11/04 11:56:07 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack\r\n",
        "14/11/04 11:56:07 INFO hive.HiveImport: 14/11/04 11:56:07 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node\r\n",
        "14/11/04 11:56:07 INFO hive.HiveImport: 14/11/04 11:56:07 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\r\n",
        "14/11/04 11:56:07 INFO hive.HiveImport: 14/11/04 11:56:07 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:07 INFO hive.HiveImport: \r\n",
        "14/11/04 11:56:07 INFO hive.HiveImport: Logging initialized using configuration in file:/opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/etc/hive/conf.dist/hive-log4j.properties\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:10 INFO hive.HiveImport: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table DETL\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:960)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1163)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 24 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.lang.reflect.InvocationTargetException\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 29 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true, username = APP. Terminating connection pool. Original Exception: ------\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hiv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "e.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: ------\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: NestedThrowables:\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true, username = APP. Terminating connection pool. Original Exception: ------\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: ------\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:781)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 34 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true, username = APP. Terminating connection pool. Original Exception: ------\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:307)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:336)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:245)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:420)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:446)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:333)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:293)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4085)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1161)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2407)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2418)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:952)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:907)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9016)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8325)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:352)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:995)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1038)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:921)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:357)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:465)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:746)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: ------\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:312)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.maybeInit(BoneCPDataSource.java:150)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:112)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:479)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:304)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 63 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection30.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:571)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.sql.DriverManager.getConnection(DriverManager.java:187)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:254)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:305)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 76 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Failed to create database '/var/lib/hive/metastore/metastore_db', see the next exception for details.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 90 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: java.sql.SQLException: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: Caused by: ERROR XBM0H: Directory /var/lib/hive/metastore/metastore_db cannot be created.\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService$9.run(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\r\n",
        "14/11/04 11:56:10 INFO hive.HiveImport: \t... 87 more\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:10 INFO hive.HiveImport: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 11:56:10 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Hive exited with status 1\r\n",
        "\tat org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:385)\r\n",
        "\tat org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:335)\r\n",
        "\tat org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:239)\r\n",
        "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:425)\r\n",
        "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\r\n",
        "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\r\n",
        "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n",
        "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\r\n",
        "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\r\n",
        "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\r\n",
        "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls FDWATOMCAE.DETL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 5 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-04 11:56 FDWATOMCAE.DETL/_SUCCESS\r\n",
        "-rw-r--r--   3 kesj kesj    5415928 2014-11-04 11:56 FDWATOMCAE.DETL/part-m-00000\r\n",
        "-rw-r--r--   3 kesj kesj    2565784 2014-11-04 11:56 FDWATOMCAE.DETL/part-m-00001\r\n",
        "-rw-r--r--   3 kesj kesj    3246472 2014-11-04 11:56 FDWATOMCAE.DETL/part-m-00002\r\n",
        "-rw-r--r--   3 kesj kesj    9737190 2014-11-04 11:56 FDWATOMCAE.DETL/part-m-00003\r\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )\"'\n",
      "\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID --hive-import --hive-table=Y1753DADdetl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vvcd = \"'Y1753DAD'\"\n",
      "tgt = 'vrp/data/y1753dad/detl2'\n",
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+' )\"'\n",
      "#print wclause\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID  --fields-terminated-by \"\\t\"  --target-dir {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:28 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/04 16:52:28 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:29 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:29 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:29 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/57233ea15a176ca58e3230506b750c58/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/04 16:52:32 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/57233ea15a176ca58e3230506b750c58/FDWATOMCAE.DETL.jar\r\n",
        "14/11/04 16:52:32 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/11/04 16:52:32 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/04 16:52:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:32 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:34 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/04 16:52:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:56 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0288\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:56 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0288\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:52:56 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0288/\r\n",
        "14/11/04 16:52:56 INFO mapreduce.Job: Running job: job_1412635903408_0288\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:53:02 INFO mapreduce.Job: Job job_1412635903408_0288 running in uber mode : false\r\n",
        "14/11/04 16:53:02 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:53:22 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:53:25 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:53:31 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:53:32 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:53:47 INFO mapreduce.Job: Job job_1412635903408_0288 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/04 16:53:47 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=414244\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=521\r\n",
        "\t\tHDFS: Number of bytes written=20965374\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=168926\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=168926\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=168926\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=172980224\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=71588\r\n",
        "\t\tMap output records=71588\r\n",
        "\t\tInput split bytes=521\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=240\r\n",
        "\t\tCPU time spent (ms)=23390\r\n",
        "\t\tPhysical memory (bytes) snapshot=1331175424\r\n",
        "\t\tVirtual memory (bytes) snapshot=6288891904\r\n",
        "\t\tTotal committed heap usage (bytes)=2663383040\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=20965374\r\n",
        "14/11/04 16:53:47 INFO mapreduce.ImportJobBase: Transferred 19.9941 MB in 75.2326 seconds (272.1425 KB/sec)\r\n",
        "14/11/04 16:53:47 INFO mapreduce.ImportJobBase: Retrieved 71588 records.\r\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vvcd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "\"'Y1753DAD'\""
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### now load the LOS_EST file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vvcd = \"'Y1753CAA'\"\n",
      "tgt = 'vrp/data/y1753caa/los_est'\n",
      "\n",
      "wclause = '\" VNDR_VEH_CD='+vvcd+' \"'\n",
      "print wclause\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.LOS_EST --where {wclause} --split-by LOS_EST_DIM_ID  --fields-terminated-by \"\\t\" --target-dir {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\" VNDR_VEH_CD='Y1753CAA' \"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:19 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:20 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:03:20 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:22 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:22 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:22 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/b08a8c6e0d9df4185f083a7bb0bd4981/FDWATOMCAE_LOS_EST.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:03:25 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/b08a8c6e0d9df4185f083a7bb0bd4981/FDWATOMCAE.LOS_EST.jar\r\n",
        "14/11/05 10:03:25 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.LOS_EST\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:25 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:03:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:25 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:28 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:03:28 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(LOS_EST_DIM_ID), MAX(LOS_EST_DIM_ID) FROM FDWATOMCAE.LOS_EST WHERE (  VNDR_VEH_CD='Y1753CAA'  )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:41 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0305\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:41 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0305\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:41 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0305/\r\n",
        "14/11/05 10:03:41 INFO mapreduce.Job: Running job: job_1412635903408_0305\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:03:48 INFO mapreduce.Job: Job job_1412635903408_0305 running in uber mode : false\r\n",
        "14/11/05 10:03:48 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:04:01 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:04:02 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:04:09 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:04:10 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 10:04:10 INFO mapreduce.Job: Job job_1412635903408_0305 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:04:10 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=419256\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=530\r\n",
        "\t\tHDFS: Number of bytes written=1915512\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=80804\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=80804\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=80804\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=82743296\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=2265\r\n",
        "\t\tMap output records=2265\r\n",
        "\t\tInput split bytes=530\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=93\r\n",
        "\t\tCPU time spent (ms)=11840\r\n",
        "\t\tPhysical memory (bytes) snapshot=1284149248\r\n",
        "\t\tVirtual memory (bytes) snapshot=6347042816\r\n",
        "\t\tTotal committed heap usage (bytes)=2879913984\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=1915512\r\n",
        "14/11/05 10:04:10 INFO mapreduce.ImportJobBase: Transferred 1.8268 MB in 45.0314 seconds (41.5403 KB/sec)\r\n",
        "14/11/05 10:04:10 INFO mapreduce.ImportJobBase: Retrieved 2265 records.\r\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Grab all the tables corresponding to a given VNDR_VEH_CD\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vvc = 'ARM8521'\n",
      "vvc = 'Y1753DAD'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vvc.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "y1753dad\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tgt = 'vrp/data/'+vvc.lower()+'/'+tbl\n",
      "print tgt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vrp/data/Y1753DAD\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vvc = 'Y1753DAD'\n",
      "vvcd = \"'\"+vvc+\"'\" # place single quotes around the vvc\n",
      "tbl = 'TTL'\n",
      "tgt = 'vrp/data/'+vvc.lower()+'/'+tbl.lower()\n",
      "#wclause = '\" VNDR_VEH_CD='+vvcd+' \"'\n",
      "wclause = '\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
      "print wclause\n",
      "#print vvc\n",
      "#print tgt\n",
      "spltkey = 'TTL_DIM_ID'\n",
      "tblname = 'FDWATOMCAE.'+tbl\n",
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:04:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:01 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 09:05:01 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/84dcea9901e29f96e04fd85a466ae9d1/FDWATOMCAE_TTL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 09:05:04 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/84dcea9901e29f96e04fd85a466ae9d1/FDWATOMCAE.TTL.jar\r\n",
        "14/11/05 09:05:04 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.TTL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 09:05:04 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:04 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:08 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 09:05:08 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(TTL_DIM_ID), MAX(TTL_DIM_ID) FROM FDWATOMCAE.TTL WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:26 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0293\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:27 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0293\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:27 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0293/\r\n",
        "14/11/05 09:05:27 INFO mapreduce.Job: Running job: job_1412635903408_0293\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:33 INFO mapreduce.Job: Job job_1412635903408_0293 running in uber mode : false\r\n",
        "14/11/05 09:05:33 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:54 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:55 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 09:05:55 INFO mapreduce.Job: Job job_1412635903408_0293 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 09:05:55 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411804\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=510\r\n",
        "\t\tHDFS: Number of bytes written=4215127\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=77638\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=77638\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=77638\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=79501312\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=24736\r\n",
        "\t\tMap output records=24736\r\n",
        "\t\tInput split bytes=510\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=111\r\n",
        "\t\tCPU time spent (ms)=16160\r\n",
        "\t\tPhysical memory (bytes) snapshot=1247358976\r\n",
        "\t\tVirtual memory (bytes) snapshot=6315945984\r\n",
        "\t\tTotal committed heap usage (bytes)=2591031296\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=4215127\r\n",
        "14/11/05 09:05:55 INFO mapreduce.ImportJobBase: Transferred 4.0199 MB in 51.0347 seconds (80.6576 KB/sec)\r\n",
        "14/11/05 09:05:55 INFO mapreduce.ImportJobBase: Retrieved 24736 records.\r\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### get the AUTO_EST_SUM for this case"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vvcd = \"'Y1753CAA'\"\n",
      "tgt = 'vrp/data/y1753caa/auto_est_sum'\n",
      "wclause = '\" LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
      "\n",
      "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.AUTO_EST_SUM --where {wclause} --split-by AUTO_EST_SUM_DIM_ID  --fields-terminated-by \"\\t\" --target-dir {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:45 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:47 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 12:48:47 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:48 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/5f625a97e3148b1ac3acc05cfd08317f/FDWATOMCAE_AUTO_EST_SUM.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 12:48:51 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/5f625a97e3148b1ac3acc05cfd08317f/FDWATOMCAE.AUTO_EST_SUM.jar\r\n",
        "14/11/05 12:48:51 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.AUTO_EST_SUM\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:51 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 12:48:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:51 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:51 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:48:54 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 12:48:54 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(AUTO_EST_SUM_DIM_ID), MAX(AUTO_EST_SUM_DIM_ID) FROM FDWATOMCAE.AUTO_EST_SUM WHERE (  LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753CAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:04 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0328\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:05 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0328\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:05 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0328/\r\n",
        "14/11/05 12:49:05 INFO mapreduce.Job: Running job: job_1412635903408_0328\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:11 INFO mapreduce.Job: Job job_1412635903408_0328 running in uber mode : false\r\n",
        "14/11/05 12:49:11 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:27 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:28 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:29 INFO mapreduce.Job: Job job_1412635903408_0328 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 12:49:29 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=425080\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=577\r\n",
        "\t\tHDFS: Number of bytes written=5133157\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=56994\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=56994\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=56994\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=58361856\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=4023\r\n",
        "\t\tMap output records=4023\r\n",
        "\t\tInput split bytes=577\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=142\r\n",
        "\t\tCPU time spent (ms)=15090\r\n",
        "\t\tPhysical memory (bytes) snapshot=1245134848\r\n",
        "\t\tVirtual memory (bytes) snapshot=6275936256\r\n",
        "\t\tTotal committed heap usage (bytes)=2591031296\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=5133157\r\n",
        "14/11/05 12:49:29 INFO mapreduce.ImportJobBase: Transferred 4.8954 MB in 37.8349 seconds (132.4926 KB/sec)\r\n",
        "14/11/05 12:49:29 INFO mapreduce.ImportJobBase: Retrieved 4023 records.\r\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbip, user, nfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "('jdbc:db2://10.96.37.166:60100/FDW2P', 'kesj', '/user/kesj/config/e.pswd')"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "schema = 'FDWATOMCAE.'\n",
      "tblList = ['LOS_EST','DETL','AUTO_EST_SUM','LBR_NOTE','EST_PARTY',\n",
      "           'NON_OEM','OPT','MSG','TAX','TTL','RATE']\n",
      "           #'EST_PARTY_DETL_RLTN'\n",
      "def pullForVVC(vvc,user,pfile,dbip,baseTarget,schema,tblList,sep='\"\\t\"'):\n",
      "    # define a splitKey dictionary\n",
      "    splitKey = {}\n",
      "    splitKey['LOS_EST']='LOS_EST_DIM_ID'\n",
      "    splitKey['DETL']='DETL_DIM_ID'\n",
      "    splitKey['LBR_NOTE']='LBR_NOTE_DIM_ID'\n",
      "    splitKey['EST_PARTY']='EST_PARTY_DIM_ID'\n",
      "    splitKey['TAX']='TAX_DIM_ID'\n",
      "    splitKey['NON_OEM'] ='NON_OEM_DIM_ID'\n",
      "    splitKey['OPT']='OPT_DIM_ID'\n",
      "    splitKey['AUTO_EST_SECT']='AUTO_EST_SECT_DIM_ID'\n",
      "    splitKey['AUTO_EST_SUM']='AUTO_EST_SUM_DIM_ID'\n",
      "    splitKey['AUTO_EST_TEAM']='AUTO_EST_TEAM_DIM_ID'\n",
      "    splitKey['AUTO_EST_USER']='AUTO_EST_USER_DIM_ID'\n",
      "    splitKey['AUTO_RPR_FAC']='AUTO_RPR_FAC_DIM_ID'\n",
      "    splitKey['MSG']='MSG_DIM_ID'\n",
      "    splitKey['RATE']='RATE_DIM_ID'\n",
      "    splitKey['TTL']='TTL_DIM_ID'\n",
      "    \n",
      "    vvcd = \"'\"+vvc+\"'\" # place single quotes around the vvc\n",
      "    for tbl in tblList:\n",
      "        tgt = baseTarget+vvc.lower()+'/'+tbl.lower()\n",
      "        print tbl, tgt\n",
      "        if tbl == 'LOS_EST':\n",
      "            wclause = '\" VNDR_VEH_CD='+vvcd+' \"'\n",
      "        elif tbl == 'AUTO_EST_SUM':\n",
      "            # uses LOS_EST_BUSN_ID\n",
      "            wclause = '\" LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
      "        else:\n",
      "            wclause = '\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
      "        print wclause\n",
      "        skey = splitKey[tbl]\n",
      "        tblname = schema+tbl\n",
      "        !sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username {user} --password-file {pfile} --table {tblname} --where {wclause} --split-by {skey}  --fields-terminated-by {sep} --target-dir {tgt}\n",
      "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pullForVVC('910848','kesj',nfile,dbip,'vrp/data/','FDWATOMCAE.',tblList,sep='\"\\t\"')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LOS_EST vrp/data/910848/los_est\n",
        "\" VNDR_VEH_CD='910848' \"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:22 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:20:22 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:23 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/69403685e42ca8432aaf471118800cd5/FDWATOMCAE_LOS_EST.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:20:26 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/69403685e42ca8432aaf471118800cd5/FDWATOMCAE.LOS_EST.jar\r\n",
        "14/11/05 10:20:26 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.LOS_EST\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:26 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:20:26 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:27 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:29 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:20:29 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(LOS_EST_DIM_ID), MAX(LOS_EST_DIM_ID) FROM FDWATOMCAE.LOS_EST WHERE (  VNDR_VEH_CD='910848'  )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:39 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0306\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:39 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0306\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:39 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0306/\r\n",
        "14/11/05 10:20:39 INFO mapreduce.Job: Running job: job_1412635903408_0306\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:45 INFO mapreduce.Job: Job job_1412635903408_0306 running in uber mode : false\r\n",
        "14/11/05 10:20:45 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:56 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:20:57 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:10 INFO mapreduce.Job: Job job_1412635903408_0306 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:10 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=419792\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=530\r\n",
        "\t\tHDFS: Number of bytes written=38580987\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=90295\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=90295\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=90295\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=92462080\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=43219\r\n",
        "\t\tMap output records=43219\r\n",
        "\t\tInput split bytes=530\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=306\r\n",
        "\t\tCPU time spent (ms)=26090\r\n",
        "\t\tPhysical memory (bytes) snapshot=1335885824\r\n",
        "\t\tVirtual memory (bytes) snapshot=6332583936\r\n",
        "\t\tTotal committed heap usage (bytes)=2811756544\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=38580987\r\n",
        "14/11/05 10:21:10 INFO mapreduce.ImportJobBase: Transferred 36.7937 MB in 43.4681 seconds (866.7668 KB/sec)\r\n",
        "14/11/05 10:21:10 INFO mapreduce.ImportJobBase: Retrieved 43219 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DETL vrp/data/910848/detl\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:13 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:21:13 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:15 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/8578f6fd4ec58878d61ca2aaba28616b/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:21:17 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/8578f6fd4ec58878d61ca2aaba28616b/FDWATOMCAE.DETL.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:17 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n",
        "14/11/05 10:21:17 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:21:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:17 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:20 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:21:20 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:57 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0307\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:57 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0307\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:21:57 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0307/\r\n",
        "14/11/05 10:21:57 INFO mapreduce.Job: Running job: job_1412635903408_0307\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:22:02 INFO mapreduce.Job: Job job_1412635903408_0307 running in uber mode : false\r\n",
        "14/11/05 10:22:02 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:22:23 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:22:24 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:42 INFO mapreduce.Job: Job job_1412635903408_0307 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:42 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=414776\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=521\r\n",
        "\t\tHDFS: Number of bytes written=433302306\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=316350\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=316350\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=316350\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=323942400\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=1405637\r\n",
        "\t\tMap output records=1405637\r\n",
        "\t\tInput split bytes=521\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=1218\r\n",
        "\t\tCPU time spent (ms)=75020\r\n",
        "\t\tPhysical memory (bytes) snapshot=840904704\r\n",
        "\t\tVirtual memory (bytes) snapshot=6281097216\r\n",
        "\t\tTotal committed heap usage (bytes)=2112880640\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=433302306\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:42 INFO mapreduce.ImportJobBase: Transferred 413.2293 MB in 145.2657 seconds (2.8446 MB/sec)\r\n",
        "14/11/05 10:23:42 INFO mapreduce.ImportJobBase: Retrieved 1405637 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AUTO_EST_SUM vrp/data/910848/auto_est_sum\n",
        "\" LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:45 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:23:45 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:47 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/9bad8c2191b66b54fc7a3bba974b0ecc/FDWATOMCAE_AUTO_EST_SUM.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:23:50 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/9bad8c2191b66b54fc7a3bba974b0ecc/FDWATOMCAE.AUTO_EST_SUM.jar\r\n",
        "14/11/05 10:23:50 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.AUTO_EST_SUM\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:50 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:23:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:50 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:23:52 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:23:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(AUTO_EST_SUM_DIM_ID), MAX(AUTO_EST_SUM_DIM_ID) FROM FDWATOMCAE.AUTO_EST_SUM WHERE (  LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:01 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0308\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:01 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0308\r\n",
        "14/11/05 10:24:01 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0308/\r\n",
        "14/11/05 10:24:01 INFO mapreduce.Job: Running job: job_1412635903408_0308\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:08 INFO mapreduce.Job: Job job_1412635903408_0308 running in uber mode : false\r\n",
        "14/11/05 10:24:08 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:27 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:28 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:31 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:40 INFO mapreduce.Job: Job job_1412635903408_0308 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:40 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=425632\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=577\r\n",
        "\t\tHDFS: Number of bytes written=114230203\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=104588\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=104588\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=104588\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=107098112\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=89875\r\n",
        "\t\tMap output records=89875\r\n",
        "\t\tInput split bytes=577\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=540\r\n",
        "\t\tCPU time spent (ms)=38000\r\n",
        "\t\tPhysical memory (bytes) snapshot=1417449472\r\n",
        "\t\tVirtual memory (bytes) snapshot=6296133632\r\n",
        "\t\tTotal committed heap usage (bytes)=2686976000\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=114230203\r\n",
        "14/11/05 10:24:40 INFO mapreduce.ImportJobBase: Transferred 108.9384 MB in 50.0787 seconds (2.1753 MB/sec)\r\n",
        "14/11/05 10:24:40 INFO mapreduce.ImportJobBase: Retrieved 89875 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LBR_NOTE vrp/data/910848/lbr_note\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:42 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:43 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:24:43 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LBR_NOTE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LBR_NOTE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/2cb2ec027cf662b911e1140d0e6b2758/FDWATOMCAE_LBR_NOTE.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:24:46 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/2cb2ec027cf662b911e1140d0e6b2758/FDWATOMCAE.LBR_NOTE.jar\r\n",
        "14/11/05 10:24:46 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.LBR_NOTE\r\n",
        "14/11/05 10:24:46 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:24:46 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LBR_NOTE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:47 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:49 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:24:49 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(LBR_NOTE_DIM_ID), MAX(LBR_NOTE_DIM_ID) FROM FDWATOMCAE.LBR_NOTE WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:24:59 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0309\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:00 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0309\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:00 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0309/\r\n",
        "14/11/05 10:25:00 INFO mapreduce.Job: Running job: job_1412635903408_0309\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:06 INFO mapreduce.Job: Job job_1412635903408_0309 running in uber mode : false\r\n",
        "14/11/05 10:25:06 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:25 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:26 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:30 INFO mapreduce.Job: Job job_1412635903408_0309 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:30 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411504\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=545\r\n",
        "\t\tHDFS: Number of bytes written=44504916\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=80928\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=80928\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=80928\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=82870272\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=309144\r\n",
        "\t\tMap output records=309144\r\n",
        "\t\tInput split bytes=545\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=330\r\n",
        "\t\tCPU time spent (ms)=28090\r\n",
        "\t\tPhysical memory (bytes) snapshot=1343614976\r\n",
        "\t\tVirtual memory (bytes) snapshot=6342238208\r\n",
        "\t\tTotal committed heap usage (bytes)=2783444992\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=44504916\r\n",
        "14/11/05 10:25:30 INFO mapreduce.ImportJobBase: Transferred 42.4432 MB in 43.9769 seconds (988.2873 KB/sec)\r\n",
        "14/11/05 10:25:30 INFO mapreduce.ImportJobBase: Retrieved 309144 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "EST_PARTY vrp/data/910848/est_party\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:33 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:25:33 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:35 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.EST_PARTY AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:35 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.EST_PARTY AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:35 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/6fd94aede8a976ee5053e0e547ad2976/FDWATOMCAE_EST_PARTY.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:25:37 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/6fd94aede8a976ee5053e0e547ad2976/FDWATOMCAE.EST_PARTY.jar\r\n",
        "14/11/05 10:25:37 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.EST_PARTY\r\n",
        "14/11/05 10:25:37 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:25:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.EST_PARTY AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:37 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:39 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:25:39 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(EST_PARTY_DIM_ID), MAX(EST_PARTY_DIM_ID) FROM FDWATOMCAE.EST_PARTY WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:52 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0310\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:52 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0310\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:53 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0310/\r\n",
        "14/11/05 10:25:53 INFO mapreduce.Job: Running job: job_1412635903408_0310\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:25:59 INFO mapreduce.Job: Job job_1412635903408_0310 running in uber mode : false\r\n",
        "14/11/05 10:25:59 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:18 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:19 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:26 INFO mapreduce.Job: Job job_1412635903408_0310 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:26 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=412412\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=553\r\n",
        "\t\tHDFS: Number of bytes written=52710096\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=88567\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=88567\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=88567\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=90692608\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=256745\r\n",
        "\t\tMap output records=256745\r\n",
        "\t\tInput split bytes=553\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=326\r\n",
        "\t\tCPU time spent (ms)=29830\r\n",
        "\t\tPhysical memory (bytes) snapshot=1316020224\r\n",
        "\t\tVirtual memory (bytes) snapshot=6311436288\r\n",
        "\t\tTotal committed heap usage (bytes)=2793406464\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=52710096\r\n",
        "14/11/05 10:26:26 INFO mapreduce.ImportJobBase: Transferred 50.2683 MB in 49.165 seconds (1.0224 MB/sec)\r\n",
        "14/11/05 10:26:26 INFO mapreduce.ImportJobBase: Retrieved 256745 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NON_OEM vrp/data/910848/non_oem\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:28 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:29 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:26:29 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:31 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.NON_OEM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:31 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.NON_OEM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:31 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/60bad454c0bbd3579846be3e00118e73/FDWATOMCAE_NON_OEM.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:26:33 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/60bad454c0bbd3579846be3e00118e73/FDWATOMCAE.NON_OEM.jar\r\n",
        "14/11/05 10:26:33 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.NON_OEM\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:26:33 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.NON_OEM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:33 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:35 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:26:35 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(NON_OEM_DIM_ID), MAX(NON_OEM_DIM_ID) FROM FDWATOMCAE.NON_OEM WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:43 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0311\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:43 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0311\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:43 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0311/\r\n",
        "14/11/05 10:26:43 INFO mapreduce.Job: Running job: job_1412635903408_0311\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:26:50 INFO mapreduce.Job: Job job_1412635903408_0311 running in uber mode : false\r\n",
        "14/11/05 10:26:50 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:06 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:07 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:08 INFO mapreduce.Job: Job job_1412635903408_0311 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:08 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411924\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=520\r\n",
        "\t\tHDFS: Number of bytes written=2790814\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=57694\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=57694\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=57694\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=59078656\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=16065\r\n",
        "\t\tMap output records=16065\r\n",
        "\t\tInput split bytes=520\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=110\r\n",
        "\t\tCPU time spent (ms)=14350\r\n",
        "\t\tPhysical memory (bytes) snapshot=1329782784\r\n",
        "\t\tVirtual memory (bytes) snapshot=6342213632\r\n",
        "\t\tTotal committed heap usage (bytes)=2879913984\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=2790814\r\n",
        "14/11/05 10:27:08 INFO mapreduce.ImportJobBase: Transferred 2.6615 MB in 35.1988 seconds (77.429 KB/sec)\r\n",
        "14/11/05 10:27:08 INFO mapreduce.ImportJobBase: Retrieved 16065 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OPT vrp/data/910848/opt\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:10 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:11 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:27:11 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:12 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.OPT AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.OPT AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:13 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/1fc65094b69f61324bc23dcb0c0508f7/FDWATOMCAE_OPT.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:27:14 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/1fc65094b69f61324bc23dcb0c0508f7/FDWATOMCAE.OPT.jar\r\n",
        "14/11/05 10:27:14 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.OPT\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:14 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:27:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.OPT AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:14 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:15 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:17 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:27:17 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(OPT_DIM_ID), MAX(OPT_DIM_ID) FROM FDWATOMCAE.OPT WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:32 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0312\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:33 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0312\r\n",
        "14/11/05 10:27:33 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0312/\r\n",
        "14/11/05 10:27:33 INFO mapreduce.Job: Running job: job_1412635903408_0312\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:39 INFO mapreduce.Job: Job job_1412635903408_0312 running in uber mode : false\r\n",
        "14/11/05 10:27:39 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:58 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:27:59 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:27 INFO mapreduce.Job: Job job_1412635903408_0312 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:28 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411412\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=512\r\n",
        "\t\tHDFS: Number of bytes written=176328661\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=147083\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=147083\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=147083\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=150612992\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=1120248\r\n",
        "\t\tMap output records=1120248\r\n",
        "\t\tInput split bytes=512\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=602\r\n",
        "\t\tCPU time spent (ms)=42530\r\n",
        "\t\tPhysical memory (bytes) snapshot=1147805696\r\n",
        "\t\tVirtual memory (bytes) snapshot=6323695616\r\n",
        "\t\tTotal committed heap usage (bytes)=2462056448\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=176328661\r\n",
        "14/11/05 10:28:28 INFO mapreduce.ImportJobBase: Transferred 168.1601 MB in 73.2322 seconds (2.2963 MB/sec)\r\n",
        "14/11/05 10:28:28 INFO mapreduce.ImportJobBase: Retrieved 1120248 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MSG vrp/data/910848/msg\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:29 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:31 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:28:31 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.MSG AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.MSG AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:32 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/6aef20d76e83545f457b6d1a64e42bf4/FDWATOMCAE_MSG.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:28:34 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/6aef20d76e83545f457b6d1a64e42bf4/FDWATOMCAE.MSG.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:34 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.MSG\r\n",
        "14/11/05 10:28:34 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:28:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.MSG AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:34 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:34 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:28:37 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:28:37 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(MSG_DIM_ID), MAX(MSG_DIM_ID) FROM FDWATOMCAE.MSG WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:00 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0313\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:00 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0313\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:00 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0313/\r\n",
        "14/11/05 10:29:00 INFO mapreduce.Job: Running job: job_1412635903408_0313\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:07 INFO mapreduce.Job: Job job_1412635903408_0313 running in uber mode : false\r\n",
        "14/11/05 10:29:07 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:26 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:27 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:30 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:44 INFO mapreduce.Job: Job job_1412635903408_0313 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:44 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411368\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=512\r\n",
        "\t\tHDFS: Number of bytes written=123519395\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=131142\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=131142\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=131142\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=134289408\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=690770\r\n",
        "\t\tMap output records=690770\r\n",
        "\t\tInput split bytes=512\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=480\r\n",
        "\t\tCPU time spent (ms)=34590\r\n",
        "\t\tPhysical memory (bytes) snapshot=1158529024\r\n",
        "\t\tVirtual memory (bytes) snapshot=6276141056\r\n",
        "\t\tTotal committed heap usage (bytes)=2400714752\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=123519395\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:44 INFO mapreduce.ImportJobBase: Transferred 117.7973 MB in 69.8699 seconds (1.686 MB/sec)\r\n",
        "14/11/05 10:29:44 INFO mapreduce.ImportJobBase: Retrieved 690770 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TAX vrp/data/910848/tax\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:46 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:47 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:29:47 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TAX AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TAX AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:49 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/fa885b479d0cd12dd5305143f01209de/FDWATOMCAE_TAX.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:29:50 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/fa885b479d0cd12dd5305143f01209de/FDWATOMCAE.TAX.jar\r\n",
        "14/11/05 10:29:50 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.TAX\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:50 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:29:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TAX AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:50 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:29:53 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:29:53 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(TAX_DIM_ID), MAX(TAX_DIM_ID) FROM FDWATOMCAE.TAX WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:05 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0314\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:05 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0314\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:05 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0314/\r\n",
        "14/11/05 10:30:05 INFO mapreduce.Job: Running job: job_1412635903408_0314\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:11 INFO mapreduce.Job: Job job_1412635903408_0314 running in uber mode : false\r\n",
        "14/11/05 10:30:11 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:30 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:31 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:34 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:50 INFO mapreduce.Job: Job job_1412635903408_0314 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:50 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411852\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=506\r\n",
        "\t\tHDFS: Number of bytes written=131951889\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=128474\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=128474\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=128474\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=131557376\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=761365\r\n",
        "\t\tMap output records=761365\r\n",
        "\t\tInput split bytes=506\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=561\r\n",
        "\t\tCPU time spent (ms)=42800\r\n",
        "\t\tPhysical memory (bytes) snapshot=1358958592\r\n",
        "\t\tVirtual memory (bytes) snapshot=6327431168\r\n",
        "\t\tTotal committed heap usage (bytes)=2761424896\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=131951889\r\n",
        "14/11/05 10:30:50 INFO mapreduce.ImportJobBase: Transferred 125.8391 MB in 59.4665 seconds (2.1161 MB/sec)\r\n",
        "14/11/05 10:30:50 INFO mapreduce.ImportJobBase: Retrieved 761365 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TTL vrp/data/910848/ttl\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:53 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:30:53 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:54 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:54 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:54 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/a4f8268fa355a7bb3f137c302b5e17a8/FDWATOMCAE_TTL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:30:56 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/a4f8268fa355a7bb3f137c302b5e17a8/FDWATOMCAE.TTL.jar\r\n",
        "14/11/05 10:30:56 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.TTL\r\n",
        "14/11/05 10:30:56 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:30:56 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:56 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:56 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:30:59 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:30:59 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(TTL_DIM_ID), MAX(TTL_DIM_ID) FROM FDWATOMCAE.TTL WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:31:13 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:31:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0315\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:31:13 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0315\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:31:13 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0315/\r\n",
        "14/11/05 10:31:13 INFO mapreduce.Job: Running job: job_1412635903408_0315\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:31:19 INFO mapreduce.Job: Job job_1412635903408_0315 running in uber mode : false\r\n",
        "14/11/05 10:31:19 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:31:38 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:31:39 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:01 INFO mapreduce.Job: Job job_1412635903408_0315 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:01 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411788\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=510\r\n",
        "\t\tHDFS: Number of bytes written=130681992\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=133504\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=133504\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=133504\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=136708096\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=771940\r\n",
        "\t\tMap output records=771940\r\n",
        "\t\tInput split bytes=510\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=543\r\n",
        "\t\tCPU time spent (ms)=42780\r\n",
        "\t\tPhysical memory (bytes) snapshot=1188003840\r\n",
        "\t\tVirtual memory (bytes) snapshot=6309662720\r\n",
        "\t\tTotal committed heap usage (bytes)=2435842048\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=130681992\r\n",
        "14/11/05 10:32:01 INFO mapreduce.ImportJobBase: Transferred 124.6281 MB in 64.7216 seconds (1.9256 MB/sec)\r\n",
        "14/11/05 10:32:01 INFO mapreduce.ImportJobBase: Retrieved 771940 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RATE vrp/data/910848/rate\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:04 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:32:04 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.RATE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.RATE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:05 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/0eba3de6bfa1c89c73b820776ae7c471/FDWATOMCAE_RATE.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:32:07 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/0eba3de6bfa1c89c73b820776ae7c471/FDWATOMCAE.RATE.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:07 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.RATE\r\n",
        "14/11/05 10:32:07 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:32:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.RATE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:07 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:10 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:32:10 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(RATE_DIM_ID), MAX(RATE_DIM_ID) FROM FDWATOMCAE.RATE WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='910848') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:24 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0316\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:25 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0316\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:25 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0316/\r\n",
        "14/11/05 10:32:25 INFO mapreduce.Job: Running job: job_1412635903408_0316\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:31 INFO mapreduce.Job: Job job_1412635903408_0316 running in uber mode : false\r\n",
        "14/11/05 10:32:31 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:50 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:32:51 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:33:22 INFO mapreduce.Job: Job job_1412635903408_0316 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:33:22 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411994\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=518\r\n",
        "\t\tHDFS: Number of bytes written=197298767\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=160733\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=160733\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=160733\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=164590592\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=1020798\r\n",
        "\t\tMap output records=1020798\r\n",
        "\t\tInput split bytes=518\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=655\r\n",
        "\t\tCPU time spent (ms)=49890\r\n",
        "\t\tPhysical memory (bytes) snapshot=1132371968\r\n",
        "\t\tVirtual memory (bytes) snapshot=6381346816\r\n",
        "\t\tTotal committed heap usage (bytes)=2555379712\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=197298767\r\n",
        "14/11/05 10:33:22 INFO mapreduce.ImportJobBase: Transferred 188.1588 MB in 75.1905 seconds (2.5024 MB/sec)\r\n",
        "14/11/05 10:33:22 INFO mapreduce.ImportJobBase: Retrieved 1020798 records.\r\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -ls vrp/data/910848/*/_S*"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 1 items\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:24 vrp/data/910848/auto_est_sum/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:23 vrp/data/910848/detl/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:26 vrp/data/910848/est_party/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:25 vrp/data/910848/lbr_note/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:21 vrp/data/910848/los_est/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:29 vrp/data/910848/msg/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:27 vrp/data/910848/non_oem/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:28 vrp/data/910848/opt/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:33 vrp/data/910848/rate/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:30 vrp/data/910848/tax/_SUCCESS\r\n",
        "Found 1 items\r\n",
        "-rw-r--r--   3 kesj kesj          0 2014-11-05 10:31 vrp/data/910848/ttl/_SUCCESS\r\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# need to fix for AUTO_EST_SUM and LOS_EST\n",
      "also possibly for lbr_note and non_oem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pullForVVC('Y1753AAA','kesj',nfile,dbip,'vrp/data/','FDWATOMCAE.',tblList,sep='\"\\t\"')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LOS_EST vrp/data/y1753aaa/los_est\n",
        "\" VNDR_VEH_CD='Y1753AAA' \"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:09 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:52:09 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:11 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/c5686ddfc9ea96fc935606c6e40b8b12/FDWATOMCAE_LOS_EST.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:52:13 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/c5686ddfc9ea96fc935606c6e40b8b12/FDWATOMCAE.LOS_EST.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:13 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.LOS_EST\r\n",
        "14/11/05 10:52:13 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:52:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LOS_EST AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:14 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:16 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:52:16 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(LOS_EST_DIM_ID), MAX(LOS_EST_DIM_ID) FROM FDWATOMCAE.LOS_EST WHERE (  VNDR_VEH_CD='Y1753AAA'  )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:28 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0317\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:28 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0317\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:28 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0317/\r\n",
        "14/11/05 10:52:28 INFO mapreduce.Job: Running job: job_1412635903408_0317\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:34 INFO mapreduce.Job: Job job_1412635903408_0317 running in uber mode : false\r\n",
        "14/11/05 10:52:34 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:49 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:50 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:57 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:59 INFO mapreduce.Job: Job job_1412635903408_0317 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:52:59 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=419808\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=530\r\n",
        "\t\tHDFS: Number of bytes written=1054293\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=81085\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=81085\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=81085\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=83031040\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=1255\r\n",
        "\t\tMap output records=1255\r\n",
        "\t\tInput split bytes=530\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=125\r\n",
        "\t\tCPU time spent (ms)=9210\r\n",
        "\t\tPhysical memory (bytes) snapshot=1182490624\r\n",
        "\t\tVirtual memory (bytes) snapshot=6285926400\r\n",
        "\t\tTotal committed heap usage (bytes)=2691694592\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=1054293\r\n",
        "14/11/05 10:52:59 INFO mapreduce.ImportJobBase: Transferred 1.0055 MB in 45.3457 seconds (22.7052 KB/sec)\r\n",
        "14/11/05 10:52:59 INFO mapreduce.ImportJobBase: Retrieved 1255 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DETL vrp/data/y1753aaa/detl\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:01 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:02 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:53:02 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:04 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/d6c3b361b13efd044749e3ad954e778c/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:53:06 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/d6c3b361b13efd044749e3ad954e778c/FDWATOMCAE.DETL.jar\r\n",
        "14/11/05 10:53:06 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:06 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:53:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:06 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:08 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:53:08 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:46 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0318\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:47 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0318\r\n",
        "14/11/05 10:53:47 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0318/\r\n",
        "14/11/05 10:53:47 INFO mapreduce.Job: Running job: job_1412635903408_0318\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:53:53 INFO mapreduce.Job: Job job_1412635903408_0318 running in uber mode : false\r\n",
        "14/11/05 10:53:53 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:54:12 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:54:13 INFO mapreduce.Job: Task Id : attempt_1412635903408_0318_m_000000_0, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 155004\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:54:14 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:54:19 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:54:22 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:54:35 INFO mapreduce.Job: Task Id : attempt_1412635903408_0318_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 84799\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:54:53 INFO mapreduce.Job: Task Id : attempt_1412635903408_0318_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 118187\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:14 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 10:55:14 INFO mapreduce.Job: Job job_1412635903408_0318 failed with state FAILED due to: Task failed task_1412635903408_0318_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:14 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=311094\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=9038381\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=209430\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=209430\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=209430\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=214456320\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=30872\r\n",
        "\t\tMap output records=30872\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=145\r\n",
        "\t\tCPU time spent (ms)=16260\r\n",
        "\t\tPhysical memory (bytes) snapshot=941785088\r\n",
        "\t\tVirtual memory (bytes) snapshot=4725211136\r\n",
        "\t\tTotal committed heap usage (bytes)=2069364736\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=9038381\r\n",
        "14/11/05 10:55:14 INFO mapreduce.ImportJobBase: Transferred 8.6197 MB in 128.663 seconds (68.602 KB/sec)\r\n",
        "14/11/05 10:55:14 INFO mapreduce.ImportJobBase: Retrieved 30872 records.\r\n",
        "14/11/05 10:55:14 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AUTO_EST_SUM vrp/data/y1753aaa/auto_est_sum\n",
        "\" LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:17 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:55:17 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:19 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/b3a42e1e7b02db9034a6cd98f019fa59/FDWATOMCAE_AUTO_EST_SUM.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:55:22 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/b3a42e1e7b02db9034a6cd98f019fa59/FDWATOMCAE.AUTO_EST_SUM.jar\r\n",
        "14/11/05 10:55:22 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.AUTO_EST_SUM\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:22 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:55:22 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SUM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:22 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:22 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:25 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:55:25 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(AUTO_EST_SUM_DIM_ID), MAX(AUTO_EST_SUM_DIM_ID) FROM FDWATOMCAE.AUTO_EST_SUM WHERE (  LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:33 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0319\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:33 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0319\r\n",
        "14/11/05 10:55:33 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0319/\r\n",
        "14/11/05 10:55:33 INFO mapreduce.Job: Running job: job_1412635903408_0319\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:39 INFO mapreduce.Job: Job job_1412635903408_0319 running in uber mode : false\r\n",
        "14/11/05 10:55:39 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:55 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:56 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:57 INFO mapreduce.Job: Job job_1412635903408_0319 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:57 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=425648\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=577\r\n",
        "\t\tHDFS: Number of bytes written=2821609\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=55532\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=55532\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=55532\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=56864768\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=2218\r\n",
        "\t\tMap output records=2218\r\n",
        "\t\tInput split bytes=577\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=122\r\n",
        "\t\tCPU time spent (ms)=13100\r\n",
        "\t\tPhysical memory (bytes) snapshot=1317597184\r\n",
        "\t\tVirtual memory (bytes) snapshot=6342971392\r\n",
        "\t\tTotal committed heap usage (bytes)=2879913984\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=2821609\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:57 INFO mapreduce.ImportJobBase: Transferred 2.6909 MB in 34.7402 seconds (79.3166 KB/sec)\r\n",
        "14/11/05 10:55:57 INFO mapreduce.ImportJobBase: Retrieved 2218 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LBR_NOTE vrp/data/y1753aaa/lbr_note\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:55:58 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:00 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:56:00 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LBR_NOTE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LBR_NOTE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:01 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/c9776ec1c1d72901b353a42807ed5511/FDWATOMCAE_LBR_NOTE.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:56:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/c9776ec1c1d72901b353a42807ed5511/FDWATOMCAE.LBR_NOTE.jar\r\n",
        "14/11/05 10:56:03 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.LBR_NOTE\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:03 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:56:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.LBR_NOTE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:03 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:06 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:56:06 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(LBR_NOTE_DIM_ID), MAX(LBR_NOTE_DIM_ID) FROM FDWATOMCAE.LBR_NOTE WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:15 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0320\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:16 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0320\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:16 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0320/\r\n",
        "14/11/05 10:56:16 INFO mapreduce.Job: Running job: job_1412635903408_0320\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:22 INFO mapreduce.Job: Job job_1412635903408_0320 running in uber mode : false\r\n",
        "14/11/05 10:56:22 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:38 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:39 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 10:56:39 INFO mapreduce.Job: Job job_1412635903408_0320 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:39 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411520\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=545\r\n",
        "\t\tHDFS: Number of bytes written=972101\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=57699\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=57699\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=57699\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=59083776\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=5756\r\n",
        "\t\tMap output records=5756\r\n",
        "\t\tInput split bytes=545\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=115\r\n",
        "\t\tCPU time spent (ms)=9210\r\n",
        "\t\tPhysical memory (bytes) snapshot=1218158592\r\n",
        "\t\tVirtual memory (bytes) snapshot=6272217088\r\n",
        "\t\tTotal committed heap usage (bytes)=2591031296\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=972101\r\n",
        "14/11/05 10:56:39 INFO mapreduce.ImportJobBase: Transferred 949.3174 KB in 35.8583 seconds (26.4741 KB/sec)\r\n",
        "14/11/05 10:56:39 INFO mapreduce.ImportJobBase: Retrieved 5756 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "EST_PARTY vrp/data/y1753aaa/est_party\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:41 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:42 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:56:42 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:43 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.EST_PARTY AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:44 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.EST_PARTY AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:44 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/8b7702f68a968ceba7b4d8dbc701ce8d/FDWATOMCAE_EST_PARTY.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:56:45 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/8b7702f68a968ceba7b4d8dbc701ce8d/FDWATOMCAE.EST_PARTY.jar\r\n",
        "14/11/05 10:56:45 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.EST_PARTY\r\n",
        "14/11/05 10:56:45 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:56:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.EST_PARTY AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:45 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:46 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:56:48 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:56:48 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(EST_PARTY_DIM_ID), MAX(EST_PARTY_DIM_ID) FROM FDWATOMCAE.EST_PARTY WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:01 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0321\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:01 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0321\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:01 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0321/\r\n",
        "14/11/05 10:57:01 INFO mapreduce.Job: Running job: job_1412635903408_0321\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:07 INFO mapreduce.Job: Job job_1412635903408_0321 running in uber mode : false\r\n",
        "14/11/05 10:57:07 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:26 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:27 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 10:57:27 INFO mapreduce.Job: Job job_1412635903408_0321 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:28 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=412428\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=553\r\n",
        "\t\tHDFS: Number of bytes written=1288850\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=68333\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=68333\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=68333\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=69972992\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=6656\r\n",
        "\t\tMap output records=6656\r\n",
        "\t\tInput split bytes=553\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=115\r\n",
        "\t\tCPU time spent (ms)=9930\r\n",
        "\t\tPhysical memory (bytes) snapshot=1220501504\r\n",
        "\t\tVirtual memory (bytes) snapshot=6323814400\r\n",
        "\t\tTotal committed heap usage (bytes)=2591031296\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=1288850\r\n",
        "14/11/05 10:57:28 INFO mapreduce.ImportJobBase: Transferred 1.2291 MB in 42.0899 seconds (29.9037 KB/sec)\r\n",
        "14/11/05 10:57:28 INFO mapreduce.ImportJobBase: Retrieved 6656 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NON_OEM vrp/data/y1753aaa/non_oem\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:29 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:30 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:57:30 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.NON_OEM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.NON_OEM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:32 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/7f6130b0bd20a1bd8df97b015488efdb/FDWATOMCAE_NON_OEM.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:57:34 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/7f6130b0bd20a1bd8df97b015488efdb/FDWATOMCAE.NON_OEM.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:34 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.NON_OEM\r\n",
        "14/11/05 10:57:34 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:57:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.NON_OEM AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:34 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:34 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:36 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:57:36 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(NON_OEM_DIM_ID), MAX(NON_OEM_DIM_ID) FROM FDWATOMCAE.NON_OEM WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:44 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0322\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:45 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0322\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:45 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0322/\r\n",
        "14/11/05 10:57:45 INFO mapreduce.Job: Running job: job_1412635903408_0322\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:57:51 INFO mapreduce.Job: Job job_1412635903408_0322 running in uber mode : false\r\n",
        "14/11/05 10:57:51 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:05 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:06 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:07 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:08 INFO mapreduce.Job: Job job_1412635903408_0322 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:08 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411940\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=524\r\n",
        "\t\tHDFS: Number of bytes written=113562\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=53488\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=53488\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=53488\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=54771712\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=652\r\n",
        "\t\tMap output records=652\r\n",
        "\t\tInput split bytes=524\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=113\r\n",
        "\t\tCPU time spent (ms)=6680\r\n",
        "\t\tPhysical memory (bytes) snapshot=1189789696\r\n",
        "\t\tVirtual memory (bytes) snapshot=6280998912\r\n",
        "\t\tTotal committed heap usage (bytes)=2591031296\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=113562\r\n",
        "14/11/05 10:58:08 INFO mapreduce.ImportJobBase: Transferred 110.9004 KB in 34.4748 seconds (3.2169 KB/sec)\r\n",
        "14/11/05 10:58:08 INFO mapreduce.ImportJobBase: Retrieved 652 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OPT vrp/data/y1753aaa/opt\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:10 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:11 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:58:11 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.OPT AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.OPT AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:13 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/7d01ea64457fac23bfe4b32a3748e7a6/FDWATOMCAE_OPT.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:58:15 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/7d01ea64457fac23bfe4b32a3748e7a6/FDWATOMCAE.OPT.jar\r\n",
        "14/11/05 10:58:15 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.OPT\r\n",
        "14/11/05 10:58:15 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:58:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.OPT AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:15 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:17 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:58:17 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(OPT_DIM_ID), MAX(OPT_DIM_ID) FROM FDWATOMCAE.OPT WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:33 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0323\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:33 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0323\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:33 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0323/\r\n",
        "14/11/05 10:58:33 INFO mapreduce.Job: Running job: job_1412635903408_0323\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:58:40 INFO mapreduce.Job: Job job_1412635903408_0323 running in uber mode : false\r\n",
        "14/11/05 10:58:40 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:00 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:03 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 10:59:03 INFO mapreduce.Job: Job job_1412635903408_0323 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:03 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411428\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=512\r\n",
        "\t\tHDFS: Number of bytes written=3563781\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=83741\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=83741\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=83741\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=85750784\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=22672\r\n",
        "\t\tMap output records=22672\r\n",
        "\t\tInput split bytes=512\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=152\r\n",
        "\t\tCPU time spent (ms)=13880\r\n",
        "\t\tPhysical memory (bytes) snapshot=1215795200\r\n",
        "\t\tVirtual memory (bytes) snapshot=6279454720\r\n",
        "\t\tTotal committed heap usage (bytes)=2691694592\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=3563781\r\n",
        "14/11/05 10:59:03 INFO mapreduce.ImportJobBase: Transferred 3.3987 MB in 47.7946 seconds (72.817 KB/sec)\r\n",
        "14/11/05 10:59:03 INFO mapreduce.ImportJobBase: Retrieved 22672 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MSG vrp/data/y1753aaa/msg\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:06 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 10:59:06 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.MSG AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.MSG AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/8c7d52ba2ab9fc0a2721ccb7be4c4c34/FDWATOMCAE_MSG.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 10:59:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/8c7d52ba2ab9fc0a2721ccb7be4c4c34/FDWATOMCAE.MSG.jar\r\n",
        "14/11/05 10:59:09 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.MSG\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:09 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 10:59:09 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.MSG AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:09 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:09 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:11 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 10:59:11 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(MSG_DIM_ID), MAX(MSG_DIM_ID) FROM FDWATOMCAE.MSG WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:31 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0324\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:32 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0324\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:32 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0324/\r\n",
        "14/11/05 10:59:32 INFO mapreduce.Job: Running job: job_1412635903408_0324\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 10:59:38 INFO mapreduce.Job: Job job_1412635903408_0324 running in uber mode : false\r\n",
        "14/11/05 10:59:38 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:01 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:03 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:04 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 11:00:04 INFO mapreduce.Job: Job job_1412635903408_0324 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:04 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411384\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=512\r\n",
        "\t\tHDFS: Number of bytes written=3557123\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=94577\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=94577\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=94577\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=96846848\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=18790\r\n",
        "\t\tMap output records=18790\r\n",
        "\t\tInput split bytes=512\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=118\r\n",
        "\t\tCPU time spent (ms)=13360\r\n",
        "\t\tPhysical memory (bytes) snapshot=1309245440\r\n",
        "\t\tVirtual memory (bytes) snapshot=6361735168\r\n",
        "\t\tTotal committed heap usage (bytes)=2879913984\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=3557123\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:04 INFO mapreduce.ImportJobBase: Transferred 3.3923 MB in 55.132 seconds (63.008 KB/sec)\r\n",
        "14/11/05 11:00:04 INFO mapreduce.ImportJobBase: Retrieved 18790 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TAX vrp/data/y1753aaa/tax\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:07 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 11:00:07 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:09 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TAX AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:09 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TAX AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/f6356f758dc91d0576be1823a2335653/FDWATOMCAE_TAX.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 11:00:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/f6356f758dc91d0576be1823a2335653/FDWATOMCAE.TAX.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:11 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.TAX\r\n",
        "14/11/05 11:00:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 11:00:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TAX AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:11 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:13 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 11:00:13 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(TAX_DIM_ID), MAX(TAX_DIM_ID) FROM FDWATOMCAE.TAX WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:25 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0325\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:25 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0325\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:25 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0325/\r\n",
        "14/11/05 11:00:25 INFO mapreduce.Job: Running job: job_1412635903408_0325\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:31 INFO mapreduce.Job: Job job_1412635903408_0325 running in uber mode : false\r\n",
        "14/11/05 11:00:31 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:50 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:00:51 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:22 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 11:01:22 INFO mapreduce.Job: Job job_1412635903408_0325 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:22 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411579\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=506\r\n",
        "\t\tHDFS: Number of bytes written=1660772\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=96570\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=96570\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=96570\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=98887680\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=9615\r\n",
        "\t\tMap output records=9615\r\n",
        "\t\tInput split bytes=506\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=98\r\n",
        "\t\tCPU time spent (ms)=11870\r\n",
        "\t\tPhysical memory (bytes) snapshot=1294520320\r\n",
        "\t\tVirtual memory (bytes) snapshot=6310940672\r\n",
        "\t\tTotal committed heap usage (bytes)=2879913984\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=1660772\r\n",
        "14/11/05 11:01:22 INFO mapreduce.ImportJobBase: Transferred 1.5838 MB in 71.2527 seconds (22.7619 KB/sec)\r\n",
        "14/11/05 11:01:22 INFO mapreduce.ImportJobBase: Retrieved 9615 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TTL vrp/data/y1753aaa/ttl\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:25 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 11:01:25 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:27 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:27 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/d1afd1a6000d83cad7940eb0bca7c48f/FDWATOMCAE_TTL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 11:01:28 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/d1afd1a6000d83cad7940eb0bca7c48f/FDWATOMCAE.TTL.jar\r\n",
        "14/11/05 11:01:28 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.TTL\r\n",
        "14/11/05 11:01:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 11:01:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.TTL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:29 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:31 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 11:01:31 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(TTL_DIM_ID), MAX(TTL_DIM_ID) FROM FDWATOMCAE.TTL WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:45 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0326\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:45 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0326\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:45 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0326/\r\n",
        "14/11/05 11:01:45 INFO mapreduce.Job: Running job: job_1412635903408_0326\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:01:52 INFO mapreduce.Job: Job job_1412635903408_0326 running in uber mode : false\r\n",
        "14/11/05 11:01:52 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:13 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:14 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/05 11:02:14 INFO mapreduce.Job: Job job_1412635903408_0326 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:14 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411804\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=510\r\n",
        "\t\tHDFS: Number of bytes written=3068336\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=78535\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=78535\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=78535\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=80419840\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=18301\r\n",
        "\t\tMap output records=18301\r\n",
        "\t\tInput split bytes=510\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=102\r\n",
        "\t\tCPU time spent (ms)=14040\r\n",
        "\t\tPhysical memory (bytes) snapshot=1240502272\r\n",
        "\t\tVirtual memory (bytes) snapshot=6321332224\r\n",
        "\t\tTotal committed heap usage (bytes)=2591031296\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=3068336\r\n",
        "14/11/05 11:02:14 INFO mapreduce.ImportJobBase: Transferred 2.9262 MB in 45.4471 seconds (65.932 KB/sec)\r\n",
        "14/11/05 11:02:14 INFO mapreduce.ImportJobBase: Retrieved 18301 records.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RATE vrp/data/y1753aaa/rate\n",
        "\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\"\n",
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:17 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/05 11:02:17 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:18 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.RATE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:18 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.RATE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:18 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/6dad9b19e7437cbbed737477d8348789/FDWATOMCAE_RATE.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/05 11:02:20 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/6dad9b19e7437cbbed737477d8348789/FDWATOMCAE.RATE.jar\r\n",
        "14/11/05 11:02:20 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.RATE\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:20 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/05 11:02:20 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.RATE AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:20 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:23 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/05 11:02:23 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(RATE_DIM_ID), MAX(RATE_DIM_ID) FROM FDWATOMCAE.RATE WHERE (  LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:37 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0327\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:37 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0327\r\n",
        "14/11/05 11:02:37 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0327/\r\n",
        "14/11/05 11:02:37 INFO mapreduce.Job: Running job: job_1412635903408_0327\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:02:43 INFO mapreduce.Job: Job job_1412635903408_0327 running in uber mode : false\r\n",
        "14/11/05 11:02:43 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:03:05 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:03:06 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:03:07 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:03:08 INFO mapreduce.Job: Job job_1412635903408_0327 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/05 11:03:08 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=412280\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=518\r\n",
        "\t\tHDFS: Number of bytes written=3007388\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=84545\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=84545\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=84545\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=86574080\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=16023\r\n",
        "\t\tMap output records=16023\r\n",
        "\t\tInput split bytes=518\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=123\r\n",
        "\t\tCPU time spent (ms)=14660\r\n",
        "\t\tPhysical memory (bytes) snapshot=1328181248\r\n",
        "\t\tVirtual memory (bytes) snapshot=6304653312\r\n",
        "\t\tTotal committed heap usage (bytes)=2879913984\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=3007388\r\n",
        "14/11/05 11:03:08 INFO mapreduce.ImportJobBase: Transferred 2.8681 MB in 47.7955 seconds (61.4473 KB/sec)\r\n",
        "14/11/05 11:03:08 INFO mapreduce.ImportJobBase: Retrieved 16023 records.\r\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID = 1533425450 ) with ur\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID = 153342540\"' \n",
      "wclause"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "'\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753AAA\\' ) AND DETL_DIM_ID = 153342540\"'"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID <= 153342540\"' \n",
      "tgt = 'vrpTemp1'\n",
      "spltkey='DETL_DIM_ID'\n",
      "tblname='FDWATOMCAE.DETL'\n",
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:01 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:03 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/10 10:02:03 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:05 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/1d395726038844b47498635dd7c35f11/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/10 10:02:07 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/1d395726038844b47498635dd7c35f11/FDWATOMCAE.DETL.jar\r\n",
        "14/11/10 10:02:07 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:07 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/10 10:02:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:07 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:10 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/10 10:02:10 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID <= 153342540 )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:49 INFO mapreduce.JobSubmitter: number of splits:1\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0330\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:49 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0330\r\n",
        "14/11/10 10:02:49 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0330/\r\n",
        "14/11/10 10:02:49 INFO mapreduce.Job: Running job: job_1412635903408_0330\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:02:56 INFO mapreduce.Job: Job job_1412635903408_0330 running in uber mode : false\r\n",
        "14/11/10 10:02:56 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:03:03 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:03:03 INFO mapreduce.Job: Job job_1412635903408_0330 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:03:03 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=103716\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=119\r\n",
        "\t\tHDFS: Number of bytes written=0\r\n",
        "\t\tHDFS: Number of read operations=4\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=2\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=1\r\n",
        "\t\tOther local map tasks=1\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=5035\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=5035\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=5035\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=5155840\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=0\r\n",
        "\t\tMap output records=0\r\n",
        "\t\tInput split bytes=119\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=29\r\n",
        "\t\tCPU time spent (ms)=1010\r\n",
        "\t\tPhysical memory (bytes) snapshot=269639680\r\n",
        "\t\tVirtual memory (bytes) snapshot=1565073408\r\n",
        "\t\tTotal committed heap usage (bytes)=503316480\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 10:03:03 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 55.7687 seconds (0 bytes/sec)\r\n",
        "14/11/10 10:03:03 INFO mapreduce.ImportJobBase: Retrieved 0 records.\r\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540\"' \n",
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' )\"' \n",
      "tgt = 'vrpTempY17533'\n",
      "spltkey='DETL_DIM_ID'\n",
      "tblname='FDWATOMCAE.DETL'\n",
      "sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt} --verbose >"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:17 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/10 11:22:17 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:18 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:18 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:18 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/7a3db212db177de8c542cf593821d8b6/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/10 11:22:21 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/7a3db212db177de8c542cf593821d8b6/FDWATOMCAE.DETL.jar\r\n",
        "14/11/10 11:22:21 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:21 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/10 11:22:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:21 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:21 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:23 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/10 11:22:23 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) )\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:45 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0338\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:46 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0338\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:46 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0338/\r\n",
        "14/11/10 11:22:46 INFO mapreduce.Job: Running job: job_1412635903408_0338\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:22:51 INFO mapreduce.Job: Job job_1412635903408_0338 running in uber mode : false\r\n",
        "14/11/10 11:22:51 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:11 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:11 INFO mapreduce.Job: Task Id : attempt_1412635903408_0338_m_000000_0, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 20641\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:12 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:14 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:20 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:21 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:30 INFO mapreduce.Job: Task Id : attempt_1412635903408_0338_m_000000_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 56610\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:23:48 INFO mapreduce.Job: Task Id : attempt_1412635903408_0338_m_000000_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 37193\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:24:08 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/10 11:24:08 INFO mapreduce.Job: Job job_1412635903408_0338 failed with state FAILED due to: Task failed task_1412635903408_0338_m_000000\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 11:24:08 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=311067\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=391\r\n",
        "\t\tHDFS: Number of bytes written=9058992\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=189444\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=189444\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=189444\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=193990656\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=30940\r\n",
        "\t\tMap output records=30940\r\n",
        "\t\tInput split bytes=391\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=146\r\n",
        "\t\tCPU time spent (ms)=15220\r\n",
        "\t\tPhysical memory (bytes) snapshot=959918080\r\n",
        "\t\tVirtual memory (bytes) snapshot=4718555136\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=9058992\r\n",
        "14/11/10 11:24:08 INFO mapreduce.ImportJobBase: Transferred 8.6393 MB in 106.9596 seconds (82.7104 KB/sec)\r\n",
        "14/11/10 11:24:08 INFO mapreduce.ImportJobBase: Retrieved 30940 records.\r\n",
        "14/11/10 11:24:08 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -rmdir {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rmdir: `vrptemp1ne': No such file or directory\r\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tgt = 'vrptemp1ne'\n",
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342500\"' #153342540\"' \n",
      "qclause = '\"Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA' ) AND DETL_DIM_ID < 1533425400 )\"+' and \\$CONDITIONS\"'\n",
      "#\n",
      "\n",
      "#select * from (LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540) with ur\"' \n",
      "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} -m 1  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --query {qclause}  --fields-terminated-by \"\\t\" --target-dir {tgt} --split-by {spltkey}\n",
      "#--split-by {spltkey}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:01:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:01 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:01 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/10 13:02:01 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:02 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID < 1533425400 ) and  (1 = 0) \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:02 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID < 1533425400 ) and  (1 = 0) \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/6453a020423d66610e78e2ff6ebb99c4/QueryResult.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/10 13:02:04 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/6453a020423d66610e78e2ff6ebb99c4/QueryResult.jar\r\n",
        "14/11/10 13:02:04 INFO mapreduce.ImportJobBase: Beginning query import.\r\n",
        "14/11/10 13:02:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:05 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:07 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/10 13:02:07 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM (Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID < 1533425400 ) and  (1 = 1) ) AS t1\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:29 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0343\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:29 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0343\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:29 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0343/\r\n",
        "14/11/10 13:02:29 INFO mapreduce.Job: Running job: job_1412635903408_0343\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:36 INFO mapreduce.Job: Job job_1412635903408_0343 running in uber mode : false\r\n",
        "14/11/10 13:02:36 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:02:59 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:03:08 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:03:20 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:03:26 INFO mapreduce.Job:  map 100% reduce 0%\r\n",
        "14/11/10 13:03:26 INFO mapreduce.Job: Job job_1412635903408_0343 completed successfully\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:03:26 INFO mapreduce.Job: Counters: 30\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=411372\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=521\r\n",
        "\t\tHDFS: Number of bytes written=1756987\r\n",
        "\t\tHDFS: Number of read operations=16\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=8\r\n",
        "\tJob Counters \r\n",
        "\t\tLaunched map tasks=4\r\n",
        "\t\tOther local map tasks=4\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=188599\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=188599\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=188599\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=193125376\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=6099\r\n",
        "\t\tMap output records=6099\r\n",
        "\t\tInput split bytes=521\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=108\r\n",
        "\t\tCPU time spent (ms)=11820\r\n",
        "\t\tPhysical memory (bytes) snapshot=1197625344\r\n",
        "\t\tVirtual memory (bytes) snapshot=6277672960\r\n",
        "\t\tTotal committed heap usage (bytes)=2591031296\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=1756987\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:03:26 INFO mapreduce.ImportJobBase: Transferred 1.6756 MB in 81.5949 seconds (21.0284 KB/sec)\r\n",
        "14/11/10 13:03:26 INFO mapreduce.ImportJobBase: Retrieved 6099 records.\r\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tgt = 'vrptemp1le'\n",
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342500\"' #153342540\"' \n",
      "qclause = '\"Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 )\"+' and \\$CONDITIONS\"'\n",
      "#\n",
      "\n",
      "#select * from (LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540) with ur\"' \n",
      "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} -m 1  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 -DcharacterEncoding=UTF8 --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --query {qclause}  --fields-terminated-by \"\\t\" --target-dir {tgt} --split-by {spltkey}\n",
      "#--split-by {spltkey}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\r\n",
        "14/11/10 13:30:07 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/10 13:30:07 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:09 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 ) and  (1 = 0) \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:09 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 ) and  (1 = 0) \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/3f746881c27bea82765ea8cef942b957/QueryResult.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/10 13:30:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/3f746881c27bea82765ea8cef942b957/QueryResult.jar\r\n",
        "14/11/10 13:30:11 INFO mapreduce.ImportJobBase: Beginning query import.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/10 13:30:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:11 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:13 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/10 13:30:13 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM (Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 ) and  (1 = 1) ) AS t1\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:35 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0346\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:36 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0346\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:36 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0346/\r\n",
        "14/11/10 13:30:36 INFO mapreduce.Job: Running job: job_1412635903408_0346\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:30:42 INFO mapreduce.Job: Job job_1412635903408_0346 running in uber mode : false\r\n",
        "14/11/10 13:30:42 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:31:05 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:31:14 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:31:25 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:31:26 INFO mapreduce.Job: Task Id : attempt_1412635903408_0346_m_000003_0, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat QueryResult.readFields(QueryResult.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 22507\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:31:27 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:31:28 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:31:46 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:32:00 INFO mapreduce.Job: Task Id : attempt_1412635903408_0346_m_000003_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat QueryResult.readFields(QueryResult.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 26445\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:32:01 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:32:21 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:32:34 INFO mapreduce.Job: Task Id : attempt_1412635903408_0346_m_000003_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat QueryResult.readFields(QueryResult.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 272851\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:32:35 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:32:54 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:33:09 INFO mapreduce.Job: Job job_1412635903408_0346 failed with state FAILED due to: Task failed task_1412635903408_0346_m_000003\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/10 13:33:09 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=308922\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=390\r\n",
        "\t\tHDFS: Number of bytes written=1127129\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=271602\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=271602\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=271602\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=278120448\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=3917\r\n",
        "\t\tMap output records=3917\r\n",
        "\t\tInput split bytes=390\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=73\r\n",
        "\t\tCPU time spent (ms)=8920\r\n",
        "\t\tPhysical memory (bytes) snapshot=937545728\r\n",
        "\t\tVirtual memory (bytes) snapshot=4729262080\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=1127129\r\n",
        "14/11/10 13:33:09 INFO mapreduce.ImportJobBase: Transferred 1.0749 MB in 177.9505 seconds (6.1855 KB/sec)\r\n",
        "14/11/10 13:33:09 INFO mapreduce.ImportJobBase: Retrieved 3917 records.\r\n",
        "14/11/10 13:33:09 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!hadoop fs -rm -r  {tgt}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.\r\n",
        "Moved: 'hdfs://ac00h1pname02.opr.statefarm.org:8020/user/kesj/vrptemp1le' to trash at: hdfs://ac00h1pname02.opr.statefarm.org:8020/user/kesj/.Trash/Current\r\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tgt = 'vrptemp1le'\n",
      "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342500\"' #153342540\"' \n",
      "qclause = '\"Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 )\"+' and \\$CONDITIONS\"'\n",
      "#\n",
      "\n",
      "#select * from (LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540) with ur\"' \n",
      "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} -m 1  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
      "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --query {qclause}  --fields-terminated-by \"\\t\" --target-dir {tgt} --split-by {spltkey}\n",
      "#--split-by {spltkey}\n",
      "#-DcharacterEncoding=cp1252"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\r\n",
        "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.0.0\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:55 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:55 INFO manager.SqlManager: Using default fetchSize of 1000\r\n",
        "14/11/13 09:14:55 INFO tool.CodeGenTool: Beginning code generation\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:57 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 ) and  (1 = 0) \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:57 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 ) and  (1 = 0) \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:57 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Note: /tmp/sqoop-kesj/compile/2d6972d0edad8a99f538605d75fcfc6f/QueryResult.java uses or overrides a deprecated API.\r\n",
        "Note: Recompile with -Xlint:deprecation for details.\r\n",
        "14/11/13 09:14:59 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/2d6972d0edad8a99f538605d75fcfc6f/QueryResult.jar\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:59 INFO mapreduce.ImportJobBase: Beginning query import.\r\n",
        "14/11/13 09:14:59 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\r\n",
        "14/11/13 09:14:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:14:59 INFO client.RMProxy: Connecting to ResourceManager at ac00h1pjtkr01.opr.statefarm.org/10.36.219.119:8032\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:15:02 INFO db.DBInputFormat: Using read commited transaction isolation\r\n",
        "14/11/13 09:15:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM (Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 ) and  (1 = 1) ) AS t1\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:15:40 INFO mapreduce.JobSubmitter: number of splits:4\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:15:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1412635903408_0355\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:15:40 INFO impl.YarnClientImpl: Submitted application application_1412635903408_0355\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:15:40 INFO mapreduce.Job: The url to track the job: http://ac00h1pjtkr01.opr.statefarm.org:8088/proxy/application_1412635903408_0355/\r\n",
        "14/11/13 09:15:40 INFO mapreduce.Job: Running job: job_1412635903408_0355\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:15:47 INFO mapreduce.Job: Job job_1412635903408_0355 running in uber mode : false\r\n",
        "14/11/13 09:15:47 INFO mapreduce.Job:  map 0% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:16:10 INFO mapreduce.Job:  map 25% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:16:16 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:16:27 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:16:34 INFO mapreduce.Job: Task Id : attempt_1412635903408_0355_m_000003_0, Status : FAILED\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat QueryResult.readFields(QueryResult.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 8855\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:16:35 INFO mapreduce.Job:  map 50% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:16:36 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:16:54 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:17:07 INFO mapreduce.Job: Task Id : attempt_1412635903408_0355_m_000003_1, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat QueryResult.readFields(QueryResult.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 254004\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:17:08 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:17:27 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:17:40 INFO mapreduce.Job: Task Id : attempt_1412635903408_0355_m_000003_2, Status : FAILED\r\n",
        "Error: java.io.IOException: SQLException in nextKeyValue\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:279)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\r\n",
        "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n",
        "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n",
        "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n",
        "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\r\n",
        "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\r\n",
        "\tat java.security.AccessController.doPrivileged(Native Method)\r\n",
        "\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n",
        "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n",
        "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\r\n",
        "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\r\n",
        "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\r\n",
        "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\r\n",
        "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\r\n",
        "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\r\n",
        "\tat QueryResult.readFields(QueryResult.java:899)\r\n",
        "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:246)\r\n",
        "\t... 12 more\r\n",
        "Caused by: java.nio.charset.MalformedInputException: Input length = 233358\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\r\n",
        "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\r\n",
        "\t... 20 more\r\n",
        "Caused by: sun.io.MalformedInputException\r\n",
        "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\r\n",
        "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\r\n",
        "\t... 21 more\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:17:41 INFO mapreduce.Job:  map 75% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:18:04 INFO mapreduce.Job:  map 100% reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:18:17 INFO mapreduce.Job: Job job_1412635903408_0355 failed with state FAILED due to: Task failed task_1412635903408_0355_m_000003\r\n",
        "Job failed as tasks failed. failedMaps:1 failedReduces:0\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14/11/13 09:18:17 INFO mapreduce.Job: Counters: 31\r\n",
        "\tFile System Counters\r\n",
        "\t\tFILE: Number of bytes read=0\r\n",
        "\t\tFILE: Number of bytes written=308535\r\n",
        "\t\tFILE: Number of read operations=0\r\n",
        "\t\tFILE: Number of large read operations=0\r\n",
        "\t\tFILE: Number of write operations=0\r\n",
        "\t\tHDFS: Number of bytes read=390\r\n",
        "\t\tHDFS: Number of bytes written=1127129\r\n",
        "\t\tHDFS: Number of read operations=12\r\n",
        "\t\tHDFS: Number of large read operations=0\r\n",
        "\t\tHDFS: Number of write operations=6\r\n",
        "\tJob Counters \r\n",
        "\t\tFailed map tasks=4\r\n",
        "\t\tLaunched map tasks=7\r\n",
        "\t\tOther local map tasks=7\r\n",
        "\t\tTotal time spent by all maps in occupied slots (ms)=284266\r\n",
        "\t\tTotal time spent by all reduces in occupied slots (ms)=0\r\n",
        "\t\tTotal time spent by all map tasks (ms)=284266\r\n",
        "\t\tTotal vcore-seconds taken by all map tasks=284266\r\n",
        "\t\tTotal megabyte-seconds taken by all map tasks=291088384\r\n",
        "\tMap-Reduce Framework\r\n",
        "\t\tMap input records=3917\r\n",
        "\t\tMap output records=3917\r\n",
        "\t\tInput split bytes=390\r\n",
        "\t\tSpilled Records=0\r\n",
        "\t\tFailed Shuffles=0\r\n",
        "\t\tMerged Map outputs=0\r\n",
        "\t\tGC time elapsed (ms)=72\r\n",
        "\t\tCPU time spent (ms)=8980\r\n",
        "\t\tPhysical memory (bytes) snapshot=932315136\r\n",
        "\t\tVirtual memory (bytes) snapshot=4714430464\r\n",
        "\t\tTotal committed heap usage (bytes)=2087714816\r\n",
        "\tFile Input Format Counters \r\n",
        "\t\tBytes Read=0\r\n",
        "\tFile Output Format Counters \r\n",
        "\t\tBytes Written=1127129\r\n",
        "14/11/13 09:18:17 INFO mapreduce.ImportJobBase: Transferred 1.0749 MB in 197.3585 seconds (5.5772 KB/sec)\r\n",
        "14/11/13 09:18:17 INFO mapreduce.ImportJobBase: Retrieved 3917 records.\r\n",
        "14/11/13 09:18:17 ERROR tool.ImportTool: Error during import: Import job failed!\r\n"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}