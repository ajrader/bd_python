{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook to illustrate using the sqoop command from within the IPython Notebook\n",
    "Need to define the password file where I've stored the password for my access to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rwxrwxr-x+  3 kesj sasusers         10 2015-01-22 15:51 /user/kesj/config/.myp\r\n",
      "-rwxrwxr-x+  3 kesj sasusers          8 2014-12-17 13:57 /user/kesj/config/.vpasswd\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls '/user/kesj/config'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfile = '/user/kesj/config/.myp'\n",
    "\n",
    "#alternatively can use relative path: nfile = 'config/e.pswd'\n",
    "# to use local File space you must preface the file name with file:\n",
    "user = 'kesj'\n",
    "dbip = 'jdbc:db2://10.96.37.166:60100/FDW2P'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at what data is already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/data/discovery/vehrepat/staging/vrp/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls '/data/discovery/vehrepat/staging/vrp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try pulling all of AUTO_EST_SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdbc:db2://10.96.37.166:60100/FDW2P'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 506 databases (schemas) within this DB\n"
     ]
    }
   ],
   "source": [
    "dbList = !sqoop list-databases --connect {dbip} --username {user} --password-file {nfile}\n",
    "dbList=dbList[4:]\n",
    "print \"There are {0} databases (schemas) within this DB\".format(len(dbList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACTDM4_PCA',\n",
       " 'ACTDM4_PCAETL',\n",
       " 'ACTDM4_PCM',\n",
       " 'ACTDM4_PRL',\n",
       " 'ACTDM4_RU99',\n",
       " 'ACTDM4_SYSCAT',\n",
       " 'ACTDM4_SYSIBMADM',\n",
       " 'ACTDM4_VR1',\n",
       " 'AQN',\n",
       " 'AQN3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FDWATOMC',\n",
       " 'FDWATOMCAAP',\n",
       " 'FDWATOMCACX',\n",
       " 'FDWATOMCAE',\n",
       " 'FDWATOMCAGC',\n",
       " 'FDWATOMCAIRS',\n",
       " 'FDWATOMCAM',\n",
       " 'FDWATOMCAR',\n",
       " 'FDWATOMCAR_MOK',\n",
       " 'FDWATOMCASR',\n",
       " 'FDWATOMCAUTO',\n",
       " 'FDWATOMCBANK',\n",
       " 'FDWATOMCBEVNT',\n",
       " 'FDWATOMCCEN',\n",
       " 'FDWATOMCCI',\n",
       " 'FDWATOMCCLNT',\n",
       " 'FDWATOMCCLNTCMPA',\n",
       " 'FDWATOMCCMPA',\n",
       " 'FDWATOMCCPTVA',\n",
       " 'FDWATOMCECS',\n",
       " 'FDWATOMCECS_EXCP',\n",
       " 'FDWATOMCEPMAG',\n",
       " 'FDWATOMCESOM',\n",
       " 'FDWATOMCFC',\n",
       " 'FDWATOMCFE',\n",
       " 'FDWATOMCFSDYA',\n",
       " 'FDWATOMCHLTH',\n",
       " 'FDWATOMCIC',\n",
       " 'FDWATOMCIRP',\n",
       " 'FDWATOMCITSM',\n",
       " 'FDWATOMCLEADS',\n",
       " 'FDWATOMCLIFE',\n",
       " 'FDWATOMCLIFE_5IX',\n",
       " 'FDWATOMCMFUND',\n",
       " 'FDWATOMCMTCH',\n",
       " 'FDWATOMCOCAP',\n",
       " 'FDWATOMCPC',\n",
       " 'FDWATOMCPCOX',\n",
       " 'FDWATOMCPCU',\n",
       " 'FDWATOMCPCUA',\n",
       " 'FDWATOMCPCUF',\n",
       " 'FDWATOMCPCUFS',\n",
       " 'FDWATOMCSFPP',\n",
       " 'FDWATOMCSHOP',\n",
       " 'FDWATOMCSVI',\n",
       " 'FDWATOMCTPAR',\n",
       " 'FDWATOMCWBTD',\n",
       " 'FDWATOMCYA',\n",
       " 'FDWATOMC_MOK',\n",
       " 'FDWATOMC_T']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in dbList if a.startswith('FDWATOMC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17951 tables in these schemas.\n"
     ]
    }
   ],
   "source": [
    "tblList = !sqoop list-tables --connect jdbc:db2://10.96.37.166:60100/FDW2P --username {user} --password-file {nfile}\n",
    "tblList = tblList[5:]\n",
    "print \"There are {0} tables in these schemas.\".format(len(tblList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IBMQREP_APPENVINFO',\n",
       " 'IBMQREP_APPLYENQ',\n",
       " 'IBMQREP_APPLYMON',\n",
       " 'IBMQREP_APPLYPARMS',\n",
       " 'IBMQREP_APPLYTRACE',\n",
       " 'IBMQREP_CAPENQ',\n",
       " 'IBMQREP_CAPMON',\n",
       " 'IBMQREP_CAPPARMS',\n",
       " 'IBMQREP_CAPQMON',\n",
       " 'IBMQREP_CAPTRACE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HLDI_MOTOR_CYCLE_SPEC',\n",
       " 'HLDI_MOTOR_CYCLE_VIN_PTRN',\n",
       " 'HLDI_MOTOR_CYCLE_VIN_PTRN_BKP',\n",
       " 'HLDI_VEH_FEAT',\n",
       " 'HLDI_VEH_FEAT_BKP',\n",
       " 'HLDI_VEH_MSTR',\n",
       " 'HLDI_VEH_MSTR_BKP',\n",
       " 'HLDI_VEH_SPEC',\n",
       " 'HLDI_VEH_SPEC_BKP',\n",
       " 'HLDI_WRLD_MFG_ID']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in tblList if a.startswith('HLDI')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see about pulling all of FDWATOMCSVI.HLDI_VEH_SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "15/01/22 15:54:01 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "15/01/22 15:54:02 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "15/01/22 15:54:02 INFO tool.CodeGenTool: Beginning code generation\n",
      "15/01/22 15:54:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCSVI.HLDI_VEH_SPEC AS t WHERE 1=0\n",
      "15/01/22 15:54:04 ERROR manager.SqlManager: Error executing statement: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-551, SQLSTATE=42501, SQLERRMC=KESJ;SELECT;FDWATOMCSVI.HLDI_VEH_SPEC, DRIVER=4.15.113\n",
      "com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-551, SQLSTATE=42501, SQLERRMC=KESJ;SELECT;FDWATOMCSVI.HLDI_VEH_SPEC, DRIVER=4.15.113\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:696)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:127)\n",
      "\tat com.ibm.db2.jcc.am.qo.c(qo.java:2770)\n",
      "\tat com.ibm.db2.jcc.am.qo.a(qo.java:2218)\n",
      "\tat com.ibm.db2.jcc.t4.bb.o(bb.java:850)\n",
      "\tat com.ibm.db2.jcc.t4.bb.j(bb.java:266)\n",
      "\tat com.ibm.db2.jcc.t4.bb.d(bb.java:54)\n",
      "\tat com.ibm.db2.jcc.t4.q.c(q.java:44)\n",
      "\tat com.ibm.db2.jcc.t4.rb.j(rb.java:147)\n",
      "\tat com.ibm.db2.jcc.am.qo.kb(qo.java:2213)\n",
      "\tat com.ibm.db2.jcc.am.ro.b(ro.java:4514)\n",
      "\tat com.ibm.db2.jcc.am.ro.ic(ro.java:761)\n",
      "\tat com.ibm.db2.jcc.am.ro.executeQuery(ro.java:726)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:699)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:708)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:243)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:226)\n",
      "\tat org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:347)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1305)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1110)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "15/01/22 15:54:04 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1116)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!sqoop import --connect {dbip} --username {user} --password-file {nfile} --table FDWATOMCSVI.HLDI_VEH_SPEC -m 1 --target-dir svi/hldi_veh_spec \n",
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --table FDWATOMCSVI.HLDI_VEH_SPEC -m 1 --target-dir /data/discovery/vehrepat/staging/svi/hldi_veh_spec --as-avrodatafile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "15/01/22 16:01:09 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "15/01/22 16:01:10 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "15/01/22 16:01:10 INFO tool.CodeGenTool: Beginning code generation\n",
      "15/01/22 16:01:12 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SECT AS t WHERE 1=0\n",
      "15/01/22 16:01:12 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SECT AS t WHERE 1=0\n",
      "15/01/22 16:01:12 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/06a243676686efbce410ac5df5816ce4/FDWATOMCAE_AUTO_EST_SECT.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "15/01/22 16:01:13 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/06a243676686efbce410ac5df5816ce4/FDWATOMCAE.AUTO_EST_SECT.jar\n",
      "15/01/22 16:01:13 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.AUTO_EST_SECT\n",
      "15/01/22 16:01:13 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "15/01/22 16:01:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SECT AS t WHERE 1=0\n",
      "15/01/22 16:01:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SECT AS t WHERE 1=0\n",
      "15/01/22 16:01:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SECT AS t WHERE 1=0\n",
      "15/01/22 16:01:14 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-kesj/compile/06a243676686efbce410ac5df5816ce4/FDWATOMCAE_AUTO_EST_SECT.avsc\n",
      "15/01/22 16:01:14 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/01/22 16:01:15 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5401 for kesj on ha-hdfs:nameservice1\n",
      "15/01/22 16:01:15 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5401 for kesj)\n",
      "15/01/22 16:01:15 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm707\n",
      "15/01/22 16:01:17 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "15/01/22 16:01:17 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/01/22 16:01:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1421706220043_0122\n",
      "15/01/22 16:01:17 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5401 for kesj)\n",
      "15/01/22 16:01:17 INFO impl.YarnClientImpl: Submitted application application_1421706220043_0122\n",
      "15/01/22 16:01:17 INFO mapreduce.Job: The url to track the job: http://da74wbrmgr1.opr.statefarm.org:8088/proxy/application_1421706220043_0122/\n",
      "15/01/22 16:01:17 INFO mapreduce.Job: Running job: job_1421706220043_0122\n",
      "15/01/22 16:01:31 INFO mapreduce.Job: Job job_1421706220043_0122 running in uber mode : false\n",
      "15/01/22 16:01:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/22 16:01:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/01/22 16:01:44 INFO mapreduce.Job: Job job_1421706220043_0122 completed successfully\n",
      "15/01/22 16:01:44 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=116816\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=87\n",
      "\t\tHDFS: Number of bytes written=42103\n",
      "\t\tHDFS: Number of read operations=4\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=45564\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=7594\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7594\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=46657536\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=476\n",
      "\t\tMap output records=476\n",
      "\t\tInput split bytes=87\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tCPU time spent (ms)=2180\n",
      "\t\tPhysical memory (bytes) snapshot=508846080\n",
      "\t\tVirtual memory (bytes) snapshot=5248720896\n",
      "\t\tTotal committed heap usage (bytes)=2058354688\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=42103\n",
      "15/01/22 16:01:44 INFO mapreduce.ImportJobBase: Transferred 41.1162 KB in 29.3983 seconds (1.3986 KB/sec)\n",
      "15/01/22 16:01:44 INFO mapreduce.ImportJobBase: Retrieved 476 records.\n"
     ]
    }
   ],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --table FDWATOMCAE.AUTO_EST_SECT -m 1 --target-dir avrotest/ae_sect -as-avrodatafile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.4.3-1.cdh5.4.3.p862.534/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "15/10/28 15:34:37 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3\n",
      "15/10/28 15:34:38 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "15/10/28 15:34:38 INFO tool.CodeGenTool: Beginning code generation\n",
      "15/10/28 15:34:40 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.AUTO_EST_SECT AS t WHERE 1=0\n",
      "15/10/28 15:34:40 ERROR manager.SqlManager: Error executing statement: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-551, SQLSTATE=42501, SQLERRMC=KESJ;SELECT;FDWATOMCAE.AUTO_EST_SECT, DRIVER=3.65.119\n",
      "com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-551, SQLSTATE=42501, SQLERRMC=KESJ;SELECT;FDWATOMCAE.AUTO_EST_SECT, DRIVER=3.65.119\n",
      "\tat com.ibm.db2.jcc.am.cd.a(cd.java:698)\n",
      "\tat com.ibm.db2.jcc.am.cd.a(cd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.cd.a(cd.java:127)\n",
      "\tat com.ibm.db2.jcc.am.ko.c(ko.java:2764)\n",
      "\tat com.ibm.db2.jcc.am.ko.a(ko.java:2228)\n",
      "\tat com.ibm.db2.jcc.t4.bb.o(bb.java:850)\n",
      "\tat com.ibm.db2.jcc.t4.bb.j(bb.java:266)\n",
      "\tat com.ibm.db2.jcc.t4.bb.d(bb.java:54)\n",
      "\tat com.ibm.db2.jcc.t4.q.c(q.java:44)\n",
      "\tat com.ibm.db2.jcc.t4.rb.j(rb.java:147)\n",
      "\tat com.ibm.db2.jcc.am.ko.ib(ko.java:2223)\n",
      "\tat com.ibm.db2.jcc.am.lo.b(lo.java:4434)\n",
      "\tat com.ibm.db2.jcc.am.lo.fc(lo.java:740)\n",
      "\tat com.ibm.db2.jcc.am.lo.executeQuery(lo.java:709)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:753)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:762)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:270)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:241)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:227)\n",
      "\tat org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:295)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1833)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1645)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:143)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:236)\n",
      "15/10/28 15:34:40 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1651)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:143)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:236)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --table FDWATOMCAE.AUTO_EST_SECT -m 1 --target-dir ae/aes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try to pull some TOYOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 items\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 14:47 /data/discovery/vehrepat/staging/config\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 14:49 /data/discovery/vehrepat/staging/data\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 14:49 /data/discovery/vehrepat/staging/e2\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 14:54 /data/discovery/vehrepat/staging/ehunt\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 14:59 /data/discovery/vehrepat/staging/sample_data\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 15:00 /data/discovery/vehrepat/staging/toyota\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 15:01 /data/discovery/vehrepat/staging/toyotaCLM\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 15:01 /data/discovery/vehrepat/staging/toyotaFull\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 15:25 /data/discovery/vehrepat/staging/vrp\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 15:44 /data/discovery/vehrepat/staging/vrp_data\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 15:48 /data/discovery/vehrepat/staging/vrp_joined\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 15:49 /data/discovery/vehrepat/staging/vrp_jointed\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 16:20 /data/discovery/vehrepat/staging/vrpdata\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 16:21 /data/discovery/vehrepat/staging/vrptemp1ne\n",
      "drwxrwx---   - kesj vehrepat          0 2014-12-12 16:21 /data/discovery/vehrepat/staging/y1753dad.DETL\n"
     ]
    }
   ],
   "source": [
    "# look at vehrepat directory\n",
    "!hadoop fs -ls /data/discovery/vehrepat/staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/12/17 15:43:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.\n",
      "Moved: 'hdfs://nameservice1/data/discovery/vehrepat/staging/titanic' to trash at: hdfs://nameservice1/user/kesj/.Trash/Current\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /data/discovery/vehrepat/staging/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgt = '/data/discovery/vehrepat/staging/arm8192.detl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "14/12/17 15:50:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "14/12/17 15:50:22 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "14/12/17 15:50:22 INFO tool.CodeGenTool: Beginning code generation\n",
      "14/12/17 15:50:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "14/12/17 15:50:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "14/12/17 15:50:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/b6d40829dbe8fa430f0e674a50046a88/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "14/12/17 15:50:26 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/b6d40829dbe8fa430f0e674a50046a88/FDWATOMCAE.DETL.jar\n",
      "14/12/17 15:50:26 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\n",
      "14/12/17 15:50:26 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "14/12/17 15:50:26 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "14/12/17 15:50:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "14/12/17 15:50:26 INFO client.RMProxy: Connecting to ResourceManager at da74wbrmgr1.opr.statefarm.org/10.96.243.38:8032\n",
      "14/12/17 15:50:26 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 3542 for kesj on ha-hdfs:nameservice1\n",
      "14/12/17 15:50:26 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 3542 for kesj)\n",
      "14/12/17 15:50:27 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "14/12/17 15:50:27 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8192' ) )\n",
      "14/12/17 15:51:13 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "14/12/17 15:51:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1416923118494_1076\n",
      "14/12/17 15:51:14 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 3542 for kesj)\n",
      "14/12/17 15:51:14 INFO impl.YarnClientImpl: Submitted application application_1416923118494_1076\n",
      "14/12/17 15:51:14 INFO mapreduce.Job: The url to track the job: http://da74wbrmgr1.opr.statefarm.org:8088/proxy/application_1416923118494_1076/\n",
      "14/12/17 15:51:14 INFO mapreduce.Job: Running job: job_1416923118494_1076\n",
      "14/12/17 15:51:27 INFO mapreduce.Job: Job job_1416923118494_1076 running in uber mode : false\n",
      "14/12/17 15:51:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "14/12/17 15:51:52 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "14/12/17 15:51:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "14/12/17 15:52:42 INFO mapreduce.Job: Job job_1416923118494_1076 completed successfully\n",
      "14/12/17 15:52:42 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=456360\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=521\n",
      "\t\tHDFS: Number of bytes written=980539721\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1510482\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=251747\n",
      "\t\tTotal vcore-seconds taken by all map tasks=251747\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1546733568\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3290325\n",
      "\t\tMap output records=3290325\n",
      "\t\tInput split bytes=521\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=791\n",
      "\t\tCPU time spent (ms)=123280\n",
      "\t\tPhysical memory (bytes) snapshot=3634405376\n",
      "\t\tVirtual memory (bytes) snapshot=21024210944\n",
      "\t\tTotal committed heap usage (bytes)=7938244608\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=980539721\n",
      "14/12/17 15:52:42 INFO mapreduce.ImportJobBase: Transferred 935.1155 MB in 136.0949 seconds (6.8711 MB/sec)\n",
      "14/12/17 15:52:42 INFO mapreduce.ImportJobBase: Retrieved 3290325 records.\n"
     ]
    }
   ],
   "source": [
    "!sqoop import --connect {dbip} --username kesj --password-file {nfile} --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8192' )\" --split-by \"DETL_DIM_ID\" --target-dir {tgt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "December 17,2014 the above command worked !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going FORWARD:\n",
    "1. need to identify which columns cause the problems\n",
    "2. need to identify what format I want to pull the data in\n",
    "    * want to do csv (comma delimited with quotes around that?)\n",
    "3. need to specify columns to pull because of security issues\n",
    "4. try to do this in a scripted way\n",
    "5. what about data from ncluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spltkey='DETL_DIM_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try  this with a query clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "14/12/17 16:03:57 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "14/12/17 16:03:58 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n",
      "14/12/17 16:03:58 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "14/12/17 16:03:58 INFO tool.CodeGenTool: Beginning code generation\n",
      "14/12/17 16:04:00 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID < 1533425400 ) and  (1 = 0) \n",
      "14/12/17 16:04:00 INFO manager.SqlManager: Executing SQL statement: Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID < 1533425400 ) and  (1 = 0) \n",
      "14/12/17 16:04:00 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/92f4a4a255fa449cdaaf66302393d7d6/QueryResult.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "14/12/17 16:04:02 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/92f4a4a255fa449cdaaf66302393d7d6/QueryResult.jar\n",
      "14/12/17 16:04:02 INFO mapreduce.ImportJobBase: Beginning query import.\n",
      "14/12/17 16:04:02 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "14/12/17 16:04:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "14/12/17 16:04:02 INFO client.RMProxy: Connecting to ResourceManager at da74wbrmgr1.opr.statefarm.org/10.96.243.38:8032\n",
      "14/12/17 16:04:02 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 3543 for kesj on ha-hdfs:nameservice1\n",
      "14/12/17 16:04:02 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 3543 for kesj)\n",
      "14/12/17 16:04:03 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "14/12/17 16:04:03 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM (Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID < 1533425400 ) and  (1 = 1) ) AS t1\n",
      "14/12/17 16:04:55 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "14/12/17 16:04:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1416923118494_1077\n",
      "14/12/17 16:04:56 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 3543 for kesj)\n",
      "14/12/17 16:04:56 INFO impl.YarnClientImpl: Submitted application application_1416923118494_1077\n",
      "14/12/17 16:04:56 INFO mapreduce.Job: The url to track the job: http://da74wbrmgr1.opr.statefarm.org:8088/proxy/application_1416923118494_1077/\n",
      "14/12/17 16:04:56 INFO mapreduce.Job: Running job: job_1416923118494_1077\n",
      "14/12/17 16:05:09 INFO mapreduce.Job: Job job_1416923118494_1077 running in uber mode : false\n",
      "14/12/17 16:05:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "14/12/17 16:05:37 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "14/12/17 16:05:46 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "14/12/17 16:06:04 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "14/12/17 16:06:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "14/12/17 16:06:09 INFO mapreduce.Job: Job job_1416923118494_1077 completed successfully\n",
      "14/12/17 16:06:10 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=453931\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=521\n",
      "\t\tHDFS: Number of bytes written=1756987\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1317900\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=219650\n",
      "\t\tTotal vcore-seconds taken by all map tasks=219650\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1349529600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6099\n",
      "\t\tMap output records=6099\n",
      "\t\tInput split bytes=521\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=132\n",
      "\t\tCPU time spent (ms)=13110\n",
      "\t\tPhysical memory (bytes) snapshot=2232713216\n",
      "\t\tVirtual memory (bytes) snapshot=21031149568\n",
      "\t\tTotal committed heap usage (bytes)=8233418752\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1756987\n",
      "14/12/17 16:06:10 INFO mapreduce.ImportJobBase: Transferred 1.6756 MB in 127.919 seconds (13.4132 KB/sec)\n",
      "14/12/17 16:06:10 INFO mapreduce.ImportJobBase: Retrieved 6099 records.\n"
     ]
    }
   ],
   "source": [
    "tgt = '/data/discovery/vehrepat/staging/vrptemp2ne'\n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342500\"' #153342540\"' \n",
    "qclause = '\"Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA' ) AND DETL_DIM_ID < 1533425400 )\"+' and \\$CONDITIONS\"'\n",
    "#\n",
    "\n",
    "#select * from (LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540) with ur\"' \n",
    "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} -m 1  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file {nfile}  --query {qclause}  --fields-terminated-by \"\\t\" --target-dir {tgt} --split-by {spltkey}\n",
    "#--split-by {spltkey}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdbc:db2://10.96.37.166:60100/FDW2P'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls /data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "14/12/22 09:32:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "14/12/22 09:32:54 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "14/12/22 09:32:54 INFO tool.CodeGenTool: Beginning code generation\n",
      "14/12/22 09:32:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM V23.AUTO_MONTHLY_SS_V AS t WHERE 1=0\n",
      "14/12/22 09:33:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM V23.AUTO_MONTHLY_SS_V AS t WHERE 1=0\n",
      "14/12/22 09:33:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/dd8423607616acf304c1c9e84a1668dc/V23_AUTO_MONTHLY_SS_V.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "14/12/22 09:33:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/dd8423607616acf304c1c9e84a1668dc/V23.AUTO_MONTHLY_SS_V.jar\n",
      "14/12/22 09:33:03 INFO mapreduce.ImportJobBase: Beginning import of V23.AUTO_MONTHLY_SS_V\n",
      "14/12/22 09:33:03 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "14/12/22 09:33:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM V23.AUTO_MONTHLY_SS_V AS t WHERE 1=0\n",
      "14/12/22 09:33:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "14/12/22 09:33:04 INFO client.RMProxy: Connecting to ResourceManager at da74wbrmgr1.opr.statefarm.org/10.96.243.38:8032\n",
      "14/12/22 09:33:04 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 3820 for kesj on ha-hdfs:nameservice1\n",
      "14/12/22 09:33:04 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 3820 for kesj)\n",
      "14/12/22 09:33:04 WARN security.UserGroupInformation: PriviledgedActionException as:kesj@OPR.STATEFARM.ORG (auth:KERBEROS) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://nameservice1/user/kesj/data/autoMonthlySS already exists\n",
      "14/12/22 09:33:04 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://nameservice1/user/kesj/data/autoMonthlySS already exists\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:458)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:343)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1295)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1292)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1292)\n",
      "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1313)\n",
      "\tat org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:186)\n",
      "\tat org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:159)\n",
      "\tat org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:247)\n",
      "\tat org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:614)\n",
      "\tat org.apache.sqoop.manager.Db2Manager.importTable(Db2Manager.java:65)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:413)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/.mypass --table V23.AUTO_MONTHLY_SS_V  --target-dir data/autoMonthlySS -m 1   --fields-terminated-by \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgtdir = '/data/discovery/vehrepat/data/autoPREMmnthly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "14/12/22 10:06:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "14/12/22 10:06:28 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "14/12/22 10:06:28 INFO tool.CodeGenTool: Beginning code generation\n",
      "14/12/22 10:06:30 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM V23.AUTO_PREM_MNTHLY AS t WHERE 1=0\n",
      "14/12/22 10:06:30 ERROR manager.SqlManager: Error executing statement: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-204, SQLSTATE=42704, SQLERRMC=V23.AUTO_PREM_MNTHLY, DRIVER=4.15.113\n",
      "com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-204, SQLSTATE=42704, SQLERRMC=V23.AUTO_PREM_MNTHLY, DRIVER=4.15.113\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:696)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:127)\n",
      "\tat com.ibm.db2.jcc.am.qo.c(qo.java:2770)\n",
      "\tat com.ibm.db2.jcc.am.qo.d(qo.java:2758)\n",
      "\tat com.ibm.db2.jcc.am.qo.a(qo.java:2191)\n",
      "\tat com.ibm.db2.jcc.am.ro.a(ro.java:7834)\n",
      "\tat com.ibm.db2.jcc.t4.bb.h(bb.java:140)\n",
      "\tat com.ibm.db2.jcc.t4.bb.b(bb.java:40)\n",
      "\tat com.ibm.db2.jcc.t4.q.a(q.java:32)\n",
      "\tat com.ibm.db2.jcc.t4.rb.i(rb.java:135)\n",
      "\tat com.ibm.db2.jcc.am.qo.ib(qo.java:2160)\n",
      "\tat com.ibm.db2.jcc.am.ro.vc(ro.java:3658)\n",
      "\tat com.ibm.db2.jcc.am.ro.b(ro.java:4455)\n",
      "\tat com.ibm.db2.jcc.am.ro.ic(ro.java:761)\n",
      "\tat com.ibm.db2.jcc.am.ro.executeQuery(ro.java:726)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:699)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:708)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:243)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:226)\n",
      "\tat org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:347)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1305)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1110)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "14/12/22 10:06:30 ERROR manager.SqlManager: Chained exception 1: \n",
      "com.ibm.db2.jcc.am.SqlException: DB2 SQL Error: SQLCODE=-727, SQLSTATE=56098, SQLERRMC=2;-204;42704;V23.AUTO_PREM_MNTHLY, DRIVER=4.15.113\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:701)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:127)\n",
      "\tat com.ibm.db2.jcc.am.qo.c(qo.java:2770)\n",
      "\tat com.ibm.db2.jcc.am.qo.d(qo.java:2758)\n",
      "\tat com.ibm.db2.jcc.am.ro.a(ro.java:3559)\n",
      "\tat com.ibm.db2.jcc.t4.bb.a(bb.java:189)\n",
      "\tat com.ibm.db2.jcc.t4.bb.b(bb.java:83)\n",
      "\tat com.ibm.db2.jcc.t4.q.b(q.java:69)\n",
      "\tat com.ibm.db2.jcc.t4.sb.c(sb.java:244)\n",
      "\tat com.ibm.db2.jcc.am.ro.sc(ro.java:3550)\n",
      "\tat com.ibm.db2.jcc.am.ro.vc(ro.java:3659)\n",
      "\tat com.ibm.db2.jcc.am.ro.b(ro.java:4455)\n",
      "\tat com.ibm.db2.jcc.am.ro.ic(ro.java:761)\n",
      "\tat com.ibm.db2.jcc.am.ro.executeQuery(ro.java:726)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:699)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:708)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:243)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:226)\n",
      "\tat org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:347)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1305)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1110)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "14/12/22 10:06:30 ERROR manager.SqlManager: Chained exception 2: \n",
      "com.ibm.db2.jcc.am.SqlException: DB2 SQL Error: SQLCODE=-727, SQLSTATE=56098, SQLERRMC=2;-204;42704;V23.AUTO_PREM_MNTHLY, DRIVER=4.15.113\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:701)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:127)\n",
      "\tat com.ibm.db2.jcc.am.qo.c(qo.java:2770)\n",
      "\tat com.ibm.db2.jcc.am.qo.a(qo.java:2218)\n",
      "\tat com.ibm.db2.jcc.t4.bb.o(bb.java:850)\n",
      "\tat com.ibm.db2.jcc.t4.bb.j(bb.java:266)\n",
      "\tat com.ibm.db2.jcc.t4.bb.d(bb.java:54)\n",
      "\tat com.ibm.db2.jcc.t4.q.c(q.java:44)\n",
      "\tat com.ibm.db2.jcc.t4.rb.j(rb.java:147)\n",
      "\tat com.ibm.db2.jcc.am.qo.kb(qo.java:2213)\n",
      "\tat com.ibm.db2.jcc.am.ro.b(ro.java:4514)\n",
      "\tat com.ibm.db2.jcc.am.ro.ic(ro.java:761)\n",
      "\tat com.ibm.db2.jcc.am.ro.executeQuery(ro.java:726)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:699)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:708)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:243)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:226)\n",
      "\tat org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:347)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1305)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1110)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "14/12/22 10:06:30 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1116)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/.mypass --table V23.AUTO_PREM_MNTHLY  --target-dir {tgtdir} -m 1  --fields-terminated-by \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/.mypass --table V23.AUTO_MONTHLY_SS_V  --target-dir data/autoMonthlySS -m 1   --fields-terminated-by \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUTO_PREM_MNTHLY',\n",
       " 'AUTO_PROD_PRFL',\n",
       " 'AUTO_PROD_PRFL_TEMP',\n",
       " 'AUTO_PROBABLE_CAUSE_OF_LOSS',\n",
       " 'AUTO_PRFM_ROLE_GRP',\n",
       " 'AUTO_PRFM_ROLE_GRP_EXCP',\n",
       " 'AUTO_PROD_PRFL_FDW1',\n",
       " 'AUTO_PREM',\n",
       " 'AUTO_PREM_SNAPSHOT_LKUP',\n",
       " 'AUTO_PREM_TEMP',\n",
       " 'AUTO_PREM_YTD_SNAPSHOT',\n",
       " 'AUTO_PREM_YTD_SNAPSHOT_TEMP']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in tblList if t.startswith('AUTO_PR')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "14/12/17 15:47:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "14/12/17 15:47:22 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "14/12/17 15:47:22 INFO tool.CodeGenTool: Beginning code generation\n",
      "14/12/17 15:47:23 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "14/12/17 15:47:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "14/12/17 15:47:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/90e880f12c092722d6808324a98d7838/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "14/12/17 15:47:25 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/90e880f12c092722d6808324a98d7838/FDWATOMCAE.DETL.jar\n",
      "14/12/17 15:47:25 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\n",
      "14/12/17 15:47:25 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "14/12/17 15:47:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "14/12/17 15:47:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "14/12/17 15:47:25 INFO client.RMProxy: Connecting to ResourceManager at da74wbrmgr1.opr.statefarm.org/10.96.243.38:8032\n",
      "14/12/17 15:47:25 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 3541 for kesj on ha-hdfs:nameservice1\n",
      "14/12/17 15:47:26 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 3541 for kesj)\n",
      "14/12/17 15:47:27 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "14/12/17 15:47:27 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAD' )  --target-dir /data/discovery/vehrepat/staging/Y1753DAD   )\n",
      "14/12/17 15:47:27 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/kesj/.staging/job_1416923118494_1075\n",
      "14/12/17 15:47:27 WARN security.UserGroupInformation: PriviledgedActionException as:kesj@OPR.STATEFARM.ORG (auth:KERBEROS) cause:java.io.IOException: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=END-OF-STATEMENT;_VEH_CD='Y1753DAD' );), DRIVER=4.15.113\n",
      "14/12/17 15:47:27 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=END-OF-STATEMENT;_VEH_CD='Y1753DAD' );), DRIVER=4.15.113\n",
      "\tat org.apache.sqoop.mapreduce.db.DataDrivenDBInputFormat.getSplits(DataDrivenDBInputFormat.java:170)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1295)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1292)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1292)\n",
      "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1313)\n",
      "\tat org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:186)\n",
      "\tat org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:159)\n",
      "\tat org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:247)\n",
      "\tat org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:614)\n",
      "\tat org.apache.sqoop.manager.Db2Manager.importTable(Db2Manager.java:65)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:413)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=END-OF-STATEMENT;_VEH_CD='Y1753DAD' );), DRIVER=4.15.113\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:696)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:127)\n",
      "\tat com.ibm.db2.jcc.am.qo.c(qo.java:2770)\n",
      "\tat com.ibm.db2.jcc.am.qo.d(qo.java:2758)\n",
      "\tat com.ibm.db2.jcc.am.qo.a(qo.java:2191)\n",
      "\tat com.ibm.db2.jcc.t4.bb.h(bb.java:140)\n",
      "\tat com.ibm.db2.jcc.t4.bb.b(bb.java:40)\n",
      "\tat com.ibm.db2.jcc.t4.q.a(q.java:32)\n",
      "\tat com.ibm.db2.jcc.t4.rb.i(rb.java:135)\n",
      "\tat com.ibm.db2.jcc.am.qo.ib(qo.java:2160)\n",
      "\tat com.ibm.db2.jcc.am.qo.a(qo.java:3257)\n",
      "\tat com.ibm.db2.jcc.am.qo.a(qo.java:706)\n",
      "\tat com.ibm.db2.jcc.am.qo.executeQuery(qo.java:685)\n",
      "\tat org.apache.sqoop.mapreduce.db.DataDrivenDBInputFormat.getSplits(DataDrivenDBInputFormat.java:145)\n",
      "\t... 23 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )'\n",
    "tgt = '/data/discovery/vehrepat/staging/Y1753DAD'\n",
    "!sqoop import --connect {dbip} --table FDWATOMCAE.DETL --split-by \"DETL_DIM_ID\" --username {user} --password-file {nfile} --where {wclause}  --target-dir {tgt}  \n",
    "#--as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for error in Y1573AAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minID = 1403659351\n",
    "maxID = 3455806544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmil = 100000000\n",
    "maxID - minID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = linspace(minID+hmil,maxID,20)\n",
    "l, len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgtbase = 'ehunt/Y1753AAA_DETL/'\n",
    "wclause0 = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' )'\n",
    "for i in xrange(0,3):\n",
    "    #if i == 0:\n",
    "    #if i > 0:\n",
    "    #    lmin = int(l[i-1])\n",
    "    lmax = int(l[i])+1\n",
    "    tgt = tgtbase+str(i)\n",
    "        #wclause = wclause0+' AND DETL_DIM_ID > '+str(lmin)+ 'AND DETL_DIM_ID < '+str(lmax)+'\"'\n",
    "    wclause = wclause0+' AND DETL_DIM_ID < '+str(lmax)+'\"'\n",
    "    print tgt,lmax,wclause\n",
    "    !sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )'\n",
    "wclause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = \"LOS_EST_DIM_ID IN (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=\\'Y1753DAD\\')\"\n",
    "wclause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --table FDWATOMCAE.DETL --split-by \"DETL_DIM_ID\" --username kesj --password-file config/.mypasswd --where {wclause}  --target-dir {tgt}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrow in on the 2nd segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls ehunt/Y1753AAA_DETL/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wclause[-10:] = '553659352\"'\n",
    "wc = wclause[:-10]+'553659352\"'\n",
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/A --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wclause[:-10]+'517673223\"' \n",
    "#1517673223\n",
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/B --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wclause[:-10]+'535394808\"'#'517673224\"' 1\n",
    "#1517673223\n",
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/D --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wclause[:-10]+'526534015\"'#'517673224\"' 1\n",
    "#1517673223\n",
    "print wc\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/E --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wclause[:-10]+'530964411\"'#'517673224\"' 1\n",
    "#1517673223\n",
    "print wc\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/F --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wclause[:-13]+'= 1533425449\"'\n",
    "#'517673224\"' 1\n",
    "#1517673223\n",
    "print wc\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --where {wc} --target-dir ehunt/Y17533AAA/P --split-by DETL_DIM_ID --table FDWATOMCAE.DETL --as-avrodatafile --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1533425430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -rmdir ehunt/fdw1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at differences in using the db2.jcc.charsetDecoderEncoder=3 option\n",
    "12.01.14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID = 153342540\"'\n",
    "\n",
    "wc = wclause[:-29]+'\"'\n",
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pull the whole set from DETL\n",
    "tgt = 'e2/Y1753AAA_detl'\n",
    "!sqoop import --connect {dbip} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbip, wc, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note that the 'general' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat using -D db2.jcc.charsetDecoderEncoder=3 \n",
    "tgt = 'e2/Y1753AAA_detl_decodeA'\n",
    "!sqoop import -D db2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -rmdir {tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat using -Ddb2.jcc.charsetDecoderEncoder=3 \n",
    "tgt = 'e2/Y1753AAA_detl_decodeB'\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbipMsg=dbip+\":retrieveMessagesFromServerOnGetMessage=true;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbipMsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat using -D db2.jcc.charsetDecoderEncoder=3 \n",
    "tgt = 'e2/Y1753AAA_detl_decodeC'\n",
    "#org.apache.sqoop.manager.Db2Manager\n",
    "\n",
    "!sqoop import -D db2.jcc.charsetDecoderEncoder=3 --connection-manager org.apache.sqoop.manager.Db2Manager --connect {dbipMsg} --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wc} --target-dir {tgt} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID = 1533425450 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using --where clause # default\n",
    "tgt = 'e2/test1'\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --split-by DETL_DIM_ID --where {wclause} --target-dir {tgt}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat using -D db2.jcc.charsetDecoderEncoder=3 and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running with updated jar file 11.03.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8192' )\" --target-dir e2/tmp1 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" \n",
    "            #--splity-by DETL_DIM_ID\n",
    "#    \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8192' ) AND timestamp(FDW_RPLC_TSTMP)='9999-12-31 23:59:59.999999'\"\n",
    "#Y1753AAA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -rmdir e2/tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' )\" --target-dir e2/tmp2 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' )\" --target-dir e2/tmp3 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 -DcharacterEncoding=UTF8 -Dlocale=us_en --username kesj --password-file {nfile} --connect jdbc:db2://10.96.37.166:60100/FDW2P -table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' )\" --target-dir e2/tmp4 --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the list of ones that failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaaResults = !hadoop fs -ls ehunt/Y1753AAA_DETL/*/_SUCCESS\n",
    "print len(aaaResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaaResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import  -DcharacterEncoding=utf8 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512' )\" --target-dir vrp_data/ARM8512_DETL -m 1 --class-name \"sf.datascience.vrp.avro.export.DETL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listOfVNDRCDS = ['ARM8522', '', 'Y1753CAA', '910698', 'Y1753BAD', 'Y1753BAA',\n",
    "       'Y1753DAA', 'Y1753DAD', 'Y1753AAA', 'Y1753CAD', 'ARM8427',\n",
    "       'ARM8516', 'ARM8192', '911754', 'ARM8545', 'Y2114AER', 'ARM8512',\n",
    "       '910027', 'ARM8530', 'ARM8416', '910848']\n",
    "#for vcd in listOfVNDRCDS[3:]:\n",
    "#    print vcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAA' )\" --target-dir vrp_data/Y1753DAA_DETL -m 1 --class-name \"sf.datascience.vrp.avro.export.DETL\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat, pulling in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls 'vrp_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753DAA' )\" --target-dir vrp_data/Y1753DAA_DETLp --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nDETLlines = {}\n",
    "for vcd in listOfVNDRCDS:\n",
    "    if vcd != '':\n",
    "        nDETLlines[vcd]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try avoiding AVRO for one that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vndrCd = 'Y1753CAD'\n",
    "tgt = 'vrp/'+vndrCd+'_DETL'\n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'\"+vndrCd+\"'\"+' )\"'\n",
    "\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --table FDWATOMCAE.DETL --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --fields-terminated-by , --escaped-by \\\\ --enclosed-by '\\\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vndrCd = 'Y1753AAA'\n",
    "tgt = 'vrp/'+vndrCd+'_DETL'\n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'\"+vndrCd+\"'\"+' )\"'\n",
    "\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --table FDWATOMCAE.DETL --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID --fields-terminated-by , --escaped-by \\\\ --enclosed-by '\\\"'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lVNDRCDS[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8416' )\" --target-dir vrp_data/ARM8416_DETL --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls vrp_data/*_DETL/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls vrp_data/Y2*_DETL/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import -D db2.jcc.charsetDecoderEncoder=3 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8516' ) \" --target-dir vrp_data/ARM8516_DETL --split-by DETL_DIM_ID  --class-name \"sf.datascience.vrp.avro.export.DETL\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password {myPSWD} --as-avrodatafile --table FDWATOMCAE.LOS_EST --where \"WHERE VNDR_VEH_CD='ARM8522'\" --target-dir vrp_data/ARM8522_DETLLOS_EST --split-by \"\" --class-name \"sf.datascience.vrp.avro.export.LOS_EST\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for vcd in listOfVNDRCDS[3:]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10.24.14\n",
    "# looking at pulling all of DETL records?? How big is this?\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --table FDWATOMCAE.DETL  --target-dir vrp/detl --split-by DETL_DIM_ID  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls vrp_data/ARM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pulling ARM8512\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username kesj --password-file {nfile} --table FDWATOMCAE.DETL  --target-dir vrp_data/ARM8512_DETL --split-by DETL_DIM_ID   --as-avrodatafile --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512')\"  --class-name \"sf.datascience.vrp.avro.export.DETL\" -m 16 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL  --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512') and DETL_DIM_ID AND \\$CONDITIONS\" --target-dir vrp/detl --split-by DETL_DIM_ID  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at excluding the column that caused that error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clisting = '\"DETL_DIM_ID , LINE_PRT_STTS_CD , LINE_LBR_STTS_CD ,LINE_NUM ,PARNT_LINE_NUM ,LINE_INCLD_IND ,LINE_SUPP_NUM ,MSG_CD ,VNDR_REFR_CD ,LINE_ADJST_APLY_CD ,LBR_OPRTN_CD ,PRT_TYPE_CD ,LBR_TYPE_CD ,MTRL_TYPE_CD ,LINE_ADJST_TYPE_CD ,LINE_ADJST_PCT ,LINE_ADJST_AMT ,OEM_PRT_NUM ,NON_OEM_PRT_NUM ,EST_PRT_NUM ,PRT_CLAS_CD ,SUPLR_REFR_ID ,PRT_QTY_CNT ,LBR_AMT ,PRICE_AMT ,DBASE_PRICE_AMT ,PRT_PRICE_INCLD_IND ,PRT_PRICE_JDGT_IND ,LBR_NOTE_IND ,AFMRKT_PRT_USE_CD ,CRTFY_PRT_IND ,GLSS_PRT_IND ,PRT_TAX_IND ,LBR_TAX_IND ,PAINT_HR_CNT ,PAINT_HR_DBASE_CNT ,LBR_HR_CNT ,LBR_HR_CALC_CNT ,LBR_HR_DBASE_CNT ,LBR_INCLD_IND ,LBR_HR_JDGT_IND ,PAINT_HR_JDGT_IND ,LBR_TYPE_JDGT_IND ,LBR_OPRTN_JDGT_IND ,PAINT_STG_IND ,PAINT_TONE_IND ,PAINT_INCLD_IND ,UNIQ_LINE_NUM ,LINE_ITEM_CTGRY_CD ,ALT_PRT_IND ,DBASE_LBR_TYPE_CD ,LBR_ADJST_HR_CNT ,OTH_CHRG_UOM_CD ,OTH_CHRG_PRICE_INCLD_IND ,LINE_ADJST_TXBL_IND ,MANL_LINE_IND ,AUTOM_ENT_IND ,CLM_ID ,VNDR_CD ,VER_NUM ,DATA_CNTXT_CD ,FDW_RPLC_IND ,FDW_INSRT_TSTMP ,FDW_RPLC_TSTMP ,EST_VER_NUM ,SRC_INSRT_TSTMP ,LOS_EST_BUSN_ID ,LOS_EST_DIM_ID \"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listOfVNDRCDS = sort(listOfVNDRCDS)\n",
    "lVNDRCDS = listOfVNDRCDS[1:]\n",
    "lVNDRCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL  --target-dir vrp_data/arm8512_DETL --split-by DETL_DIM_ID   --as-avrodatafile --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='ARM8512')\" --class-name \"sf.datascience.vrp.avro.export.DETL\" --columns {clisting} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for vndrCd in lVNDRCDS:\n",
    "    tgt = 'vrpdata/'+vndrCd+'_DETL'\n",
    "    wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'\"+vndrCd+\"'\"+' )\"'\n",
    "    #print wclause\n",
    "    #print vndrCd,tgt\n",
    "    #!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD=910027')\" --target-dir {tgt} --split-by DETL_DIM_ID --class-name \"sf.datascience.vrp.avro.export.DETL\" \n",
    "    !sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.DETL --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID -m 8 --class-name \"sf.datascience.vrp.avro.export.DETL\" --columns {clisting}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls vrpdata/*DETL/_SUCCESS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the agent info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table V23.AGENTAFO_V  --target-dir data/agentafo -m 1 --as-avrodatafile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v23tbls = ['AGENTAFO_V','AUTO_MONTHLY_SS_V',\n",
    "           'AUTO_LOS_YTD_SNAPSHOT_V','AUTO_PREM_YTD_SNAPSHOT_V'\n",
    "           ,'FIRE_PREM_YTD_SNAPSHOT_V','FIRE_PREMIUM_MONTHLY_V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table V23.AUTO_PREM_YTD_SNAPSHOT_V  --target-dir data/autoPREMytdSS -m 1 --as-avrodatafile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username kesj --password-file config/.mypass--table V23.AUTO_MONTHLY_SS_V --target-dir /data/discovery/vehrepat/staging/v23/auto_monthly_ss_v --query \"SELECT count(*)\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try to pull all of the toyota camrys from 2007\n",
    "1. get LOS_EST tables --> restrict to 'active' ones\n",
    "2. get DETL tables -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgt = 'toyota/camry2007/losest'\n",
    "today = '2014-10-31'\n",
    "wclause0 = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' )'\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.LOS_EST --where {wclause} --target-dir {tgt} --split-by DETL_DIM_ID -m 8 --class-name \"sf.datascience.vrp.avro.export.DETL\" --columns {clisting}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try importing as a hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )\"'\n",
    "\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID --hive-import --hive-table Y1753DADdetl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = '\"SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+'\"'\n",
    "\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.LOS_EST --where {wclause} --split-by LOS_EST_DIM_ID --hive-import --hive-table Y1753DADdetl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )\"'\n",
    "\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID --null-string '\\\\N' --null-non-string '\\\\N' --hive-import\n",
    "            #--hive-import --hive-table Y1753AAA.DETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls FDWATOMCAE.DETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753DAD'\"+' )\"'\n",
    "\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID --hive-import --hive-table=Y1753DADdetl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvcd = \"'Y1753DAD'\"\n",
    "tgt = 'vrp/data/y1753dad/detl2'\n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+' )\"'\n",
    "#print wclause\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.DETL --where {wclause} --split-by DETL_DIM_ID  --fields-terminated-by \"\\t\"  --target-dir {tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now load the LOS_EST file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvcd = \"'Y1753CAA'\"\n",
    "tgt = 'vrp/data/y1753caa/los_est'\n",
    "\n",
    "wclause = '\" VNDR_VEH_CD='+vvcd+' \"'\n",
    "print wclause\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.LOS_EST --where {wclause} --split-by LOS_EST_DIM_ID  --fields-terminated-by \"\\t\" --target-dir {tgt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab all the tables corresponding to a given VNDR_VEH_CD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvc = 'ARM8521'\n",
    "vvc = 'Y1753DAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vvc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgt = 'vrp/data/'+vvc.lower()+'/'+tbl\n",
    "print tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvc = 'Y1753DAD'\n",
    "vvcd = \"'\"+vvc+\"'\" # place single quotes around the vvc\n",
    "tbl = 'TTL'\n",
    "tgt = 'vrp/data/'+vvc.lower()+'/'+tbl.lower()\n",
    "#wclause = '\" VNDR_VEH_CD='+vvcd+' \"'\n",
    "wclause = '\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
    "print wclause\n",
    "#print vvc\n",
    "#print tgt\n",
    "spltkey = 'TTL_DIM_ID'\n",
    "tblname = 'FDWATOMCAE.'+tbl\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the AUTO_EST_SUM for this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvcd = \"'Y1753CAA'\"\n",
    "tgt = 'vrp/data/y1753caa/auto_est_sum'\n",
    "wclause = '\" LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
    "\n",
    "!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table FDWATOMCAE.AUTO_EST_SUM --where {wclause} --split-by AUTO_EST_SUM_DIM_ID  --fields-terminated-by \"\\t\" --target-dir {tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbip, user, nfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = 'FDWATOMCAE.'\n",
    "tblList = ['LOS_EST','DETL','AUTO_EST_SUM','LBR_NOTE','EST_PARTY',\n",
    "           'NON_OEM','OPT','MSG','TAX','TTL','RATE']\n",
    "           #'EST_PARTY_DETL_RLTN'\n",
    "def pullForVVC(vvc,user,pfile,dbip,baseTarget,schema,tblList,sep='\"\\t\"'):\n",
    "    # define a splitKey dictionary\n",
    "    splitKey = {}\n",
    "    splitKey['LOS_EST']='LOS_EST_DIM_ID'\n",
    "    splitKey['DETL']='DETL_DIM_ID'\n",
    "    splitKey['LBR_NOTE']='LBR_NOTE_DIM_ID'\n",
    "    splitKey['EST_PARTY']='EST_PARTY_DIM_ID'\n",
    "    splitKey['TAX']='TAX_DIM_ID'\n",
    "    splitKey['NON_OEM'] ='NON_OEM_DIM_ID'\n",
    "    splitKey['OPT']='OPT_DIM_ID'\n",
    "    splitKey['AUTO_EST_SECT']='AUTO_EST_SECT_DIM_ID'\n",
    "    splitKey['AUTO_EST_SUM']='AUTO_EST_SUM_DIM_ID'\n",
    "    splitKey['AUTO_EST_TEAM']='AUTO_EST_TEAM_DIM_ID'\n",
    "    splitKey['AUTO_EST_USER']='AUTO_EST_USER_DIM_ID'\n",
    "    splitKey['AUTO_RPR_FAC']='AUTO_RPR_FAC_DIM_ID'\n",
    "    splitKey['MSG']='MSG_DIM_ID'\n",
    "    splitKey['RATE']='RATE_DIM_ID'\n",
    "    splitKey['TTL']='TTL_DIM_ID'\n",
    "    \n",
    "    vvcd = \"'\"+vvc+\"'\" # place single quotes around the vvc\n",
    "    for tbl in tblList:\n",
    "        tgt = baseTarget+vvc.lower()+'/'+tbl.lower()\n",
    "        print tbl, tgt\n",
    "        if tbl == 'LOS_EST':\n",
    "            wclause = '\" VNDR_VEH_CD='+vvcd+' \"'\n",
    "        elif tbl == 'AUTO_EST_SUM':\n",
    "            # uses LOS_EST_BUSN_ID\n",
    "            wclause = '\" LOS_EST_BUSN_ID IN ( SELECT LOS_EST_BUSN_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
    "        else:\n",
    "            wclause = '\" LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+vvcd+')\"'\n",
    "        print wclause\n",
    "        skey = splitKey[tbl]\n",
    "        tblname = schema+tbl\n",
    "        !sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect {dbip} --username {user} --password-file {pfile} --table {tblname} --where {wclause} --split-by {skey}  --fields-terminated-by {sep} --target-dir {tgt}\n",
    "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pullForVVC('910848','kesj',nfile,dbip,'vrp/data/','FDWATOMCAE.',tblList,sep='\"\\t\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls vrp/data/910848/*/_S*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to fix for AUTO_EST_SUM and LOS_EST\n",
    "also possibly for lbr_note and non_oem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pullForVVC('Y1753AAA','kesj',nfile,dbip,'vrp/data/','FDWATOMCAE.',tblList,sep='\"\\t\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA' ) AND DETL_DIM_ID = 1533425450 ) with ur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID = 153342540\"' \n",
    "wclause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID <= 153342540\"' \n",
    "tgt = 'vrpTemp1'\n",
    "spltkey='DETL_DIM_ID'\n",
    "tblname='FDWATOMCAE.DETL'\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540\"' \n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' )\"' \n",
    "tgt = 'vrpTempY17533'\n",
    "spltkey='DETL_DIM_ID'\n",
    "tblname='FDWATOMCAE.DETL'\n",
    "sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} --split-by {spltkey}  --fields-terminated-by \"\\t\" --target-dir {tgt} --verbose >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -rmdir {tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgt = 'vrptemp1ne'\n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342500\"' #153342540\"' \n",
    "qclause = '\"Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA' ) AND DETL_DIM_ID < 1533425400 )\"+' and \\$CONDITIONS\"'\n",
    "#\n",
    "\n",
    "#select * from (LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540) with ur\"' \n",
    "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} -m 1  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --query {qclause}  --fields-terminated-by \"\\t\" --target-dir {tgt} --split-by {spltkey}\n",
    "#--split-by {spltkey}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgt = 'vrptemp1le'\n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342500\"' #153342540\"' \n",
    "qclause = '\"Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 )\"+' and \\$CONDITIONS\"'\n",
    "#\n",
    "\n",
    "#select * from (LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540) with ur\"' \n",
    "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} -m 1  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3 -DcharacterEncoding=UTF8 --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --query {qclause}  --fields-terminated-by \"\\t\" --target-dir {tgt} --split-by {spltkey}\n",
    "#--split-by {spltkey}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -rm -r  {tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgt = 'vrptemp1le'\n",
    "wclause = '\"LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342500\"' #153342540\"' \n",
    "qclause = '\"Select * FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA' ) AND DETL_DIM_ID <= 1533425450 )\"+' and \\$CONDITIONS\"'\n",
    "#\n",
    "\n",
    "#select * from (LOS_EST_DIM_ID IN ( SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='+\"'Y1753AAA'\"+' ) AND DETL_DIM_ID < 153342540) with ur\"' \n",
    "#!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --table {tblname} --where {wclause} -m 1  --fields-terminated-by \"\\t\" --target-dir {tgt}\n",
    "!sqoop import -Ddb2.jcc.charsetDecoderEncoder=3  --connect jdbc:db2://10.96.37.166:60100/FDW2P --driver com.ibm.db2.jcc.DB2Driver --username kesj --password-file config/e.pswd  --query {qclause}  --fields-terminated-by \"\\t\" --target-dir {tgt} --split-by {spltkey}\n",
    "#--split-by {spltkey}\n",
    "#-DcharacterEncoding=cp1252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repulling a file to compare CDH5.0 (gbuilding) to CDH5.1 (PHD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "15/01/22 09:48:17 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "15/01/22 09:48:18 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "15/01/22 09:48:18 INFO tool.CodeGenTool: Beginning code generation\n",
      "15/01/22 09:48:19 ERROR manager.SqlManager: Error executing statement: com.ibm.db2.jcc.am.SqlInvalidAuthorizationSpecException: [jcc][t4][2013][11249][4.15.113] Connection authorization failure occurred.  Reason: User ID or Password invalid. ERRORCODE=-4214, SQLSTATE=28000\n",
      "com.ibm.db2.jcc.am.SqlInvalidAuthorizationSpecException: [jcc][t4][2013][11249][4.15.113] Connection authorization failure occurred.  Reason: User ID or Password invalid. ERRORCODE=-4214, SQLSTATE=28000\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:694)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:120)\n",
      "\tat com.ibm.db2.jcc.t4.b.f(b.java:2431)\n",
      "\tat com.ibm.db2.jcc.t4.b.b(b.java:1800)\n",
      "\tat com.ibm.db2.jcc.t4.z.r(z.java:958)\n",
      "\tat com.ibm.db2.jcc.t4.z.k(z.java:495)\n",
      "\tat com.ibm.db2.jcc.t4.z.c(z.java:139)\n",
      "\tat com.ibm.db2.jcc.t4.b.k(b.java:1376)\n",
      "\tat com.ibm.db2.jcc.t4.b.b(b.java:1289)\n",
      "\tat com.ibm.db2.jcc.t4.b.a(b.java:6438)\n",
      "\tat com.ibm.db2.jcc.t4.b.b(b.java:840)\n",
      "\tat com.ibm.db2.jcc.t4.b.a(b.java:757)\n",
      "\tat com.ibm.db2.jcc.t4.b.a(b.java:420)\n",
      "\tat com.ibm.db2.jcc.t4.b.a(b.java:395)\n",
      "\tat com.ibm.db2.jcc.t4.b.<init>(b.java:333)\n",
      "\tat com.ibm.db2.jcc.DB2SimpleDataSource.getConnection(DB2SimpleDataSource.java:233)\n",
      "\tat com.ibm.db2.jcc.DB2SimpleDataSource.getConnection(DB2SimpleDataSource.java:199)\n",
      "\tat com.ibm.db2.jcc.DB2Driver.connect(DB2Driver.java:474)\n",
      "\tat com.ibm.db2.jcc.DB2Driver.connect(DB2Driver.java:115)\n",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:571)\n",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:215)\n",
      "\tat org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:826)\n",
      "\tat org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:52)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:685)\n",
      "\tat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:708)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:243)\n",
      "\tat org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:226)\n",
      "\tat org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:347)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1305)\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1110)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "15/01/22 09:48:19 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter\n",
      "\tat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1116)\n",
      "\tat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n",
      "\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:396)\n",
      "\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:506)\n",
      "\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:222)\n",
      "\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:231)\n",
      "\tat org.apache.sqoop.Sqoop.main(Sqoop.java:240)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {nfile} --as-avrodatafile --table FDWATOMCAE.LBR_NOTE --where \"WHERE VNDR_VEH_CD='ARM8522'\" --target-dir avrotest/ARM8522_LBR_NOTE -m 1 \n",
    "#!sqoop import --connect jdbc:db2://10.96.37.166:60100/FDW2P --username kesj --password {myPSWD} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
