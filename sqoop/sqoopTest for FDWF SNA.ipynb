{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook to document and try some of the sqoop issues I've experienced in the past\n",
    "begin by initializing the variables for python and for sqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Setup parameters\n",
    "pfile = '/user/kesj/config/.myp'\n",
    "user='kesj'\n",
    "dbip = 'jdbc:db2://10.96.37.166:60100/FDW2F'\n",
    "bdir = '/home/kesj/work/sqoopIssues'\n",
    "hdir = '/user/kesj/sqooptest2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check connection is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 69 databases (schemas) within this DB\n"
     ]
    }
   ],
   "source": [
    "dbList = !sqoop list-databases --connect {dbip} --username {user} --password-file {pfile}\n",
    "dbList=dbList[4:]\n",
    "print \"There are {0} databases (schemas) within this DB\".format(len(dbList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15/11/05 13:23:39 ERROR manager.Db2Manager: Failed to list databases',\n",
       " 'com.ibm.db2.jcc.am.DisconnectNonTransientConnectionException: [jcc][t4][2057][11264][3.65.119] The application server rejected establishment of the connection.',\n",
       " 'An attempt was made to access a database, FDW2F, which was either not found or does not support transactions. ERRORCODE=-4499, SQLSTATE=08004',\n",
       " '\\tat com.ibm.db2.jcc.am.cd.a(cd.java:321)',\n",
       " '\\tat com.ibm.db2.jcc.am.cd.a(cd.java:387)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.u(z.java:1874)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.n(z.java:668)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.a(z.java:475)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.a(z.java:118)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.c(b.java:1351)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.b(b.java:1222)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.b(b.java:789)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.a(b.java:761)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.a(b.java:422)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.a(b.java:397)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.<init>(b.java:335)',\n",
       " '\\tat com.ibm.db2.jcc.DB2SimpleDataSource.getConnection(DB2SimpleDataSource.java:233)',\n",
       " '\\tat com.ibm.db2.jcc.DB2SimpleDataSource.getConnection(DB2SimpleDataSource.java:199)',\n",
       " '\\tat com.ibm.db2.jcc.DB2Driver.connect(DB2Driver.java:475)',\n",
       " '\\tat com.ibm.db2.jcc.DB2Driver.connect(DB2Driver.java:116)',\n",
       " '\\tat java.sql.DriverManager.getConnection(DriverManager.java:571)',\n",
       " '\\tat java.sql.DriverManager.getConnection(DriverManager.java:215)',\n",
       " '\\tat org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:880)',\n",
       " '\\tat org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:52)',\n",
       " '\\tat org.apache.sqoop.manager.Db2Manager.listDatabases(Db2Manager.java:95)',\n",
       " '\\tat org.apache.sqoop.tool.ListDatabasesTool.run(ListDatabasesTool.java:49)',\n",
       " '\\tat org.apache.sqoop.Sqoop.run(Sqoop.java:143)',\n",
       " '\\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)',\n",
       " '\\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)',\n",
       " '\\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)',\n",
       " '\\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)',\n",
       " '\\tat org.apache.sqoop.Sqoop.main(Sqoop.java:236)',\n",
       " '15/11/05 13:23:39 ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.RuntimeException: com.ibm.db2.jcc.am.DisconnectNonTransientConnectionException: [jcc][t4][2057][11264][3.65.119] The application server rejected establishment of the connection.',\n",
       " 'An attempt was made to access a database, FDW2F, which was either not found or does not support transactions. ERRORCODE=-4499, SQLSTATE=08004',\n",
       " 'java.lang.RuntimeException: com.ibm.db2.jcc.am.DisconnectNonTransientConnectionException: [jcc][t4][2057][11264][3.65.119] The application server rejected establishment of the connection.',\n",
       " 'An attempt was made to access a database, FDW2F, which was either not found or does not support transactions. ERRORCODE=-4499, SQLSTATE=08004',\n",
       " '\\tat org.apache.sqoop.manager.Db2Manager.listDatabases(Db2Manager.java:113)',\n",
       " '\\tat org.apache.sqoop.tool.ListDatabasesTool.run(ListDatabasesTool.java:49)',\n",
       " '\\tat org.apache.sqoop.Sqoop.run(Sqoop.java:143)',\n",
       " '\\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)',\n",
       " '\\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)',\n",
       " '\\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)',\n",
       " '\\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)',\n",
       " '\\tat org.apache.sqoop.Sqoop.main(Sqoop.java:236)',\n",
       " 'Caused by: com.ibm.db2.jcc.am.DisconnectNonTransientConnectionException: [jcc][t4][2057][11264][3.65.119] The application server rejected establishment of the connection.',\n",
       " 'An attempt was made to access a database, FDW2F, which was either not found or does not support transactions. ERRORCODE=-4499, SQLSTATE=08004',\n",
       " '\\tat com.ibm.db2.jcc.am.cd.a(cd.java:321)',\n",
       " '\\tat com.ibm.db2.jcc.am.cd.a(cd.java:387)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.u(z.java:1874)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.n(z.java:668)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.a(z.java:475)',\n",
       " '\\tat com.ibm.db2.jcc.t4.z.a(z.java:118)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.c(b.java:1351)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.b(b.java:1222)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.b(b.java:789)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.a(b.java:761)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.a(b.java:422)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.a(b.java:397)',\n",
       " '\\tat com.ibm.db2.jcc.t4.b.<init>(b.java:335)',\n",
       " '\\tat com.ibm.db2.jcc.DB2SimpleDataSource.getConnection(DB2SimpleDataSource.java:233)',\n",
       " '\\tat com.ibm.db2.jcc.DB2SimpleDataSource.getConnection(DB2SimpleDataSource.java:199)',\n",
       " '\\tat com.ibm.db2.jcc.DB2Driver.connect(DB2Driver.java:475)',\n",
       " '\\tat com.ibm.db2.jcc.DB2Driver.connect(DB2Driver.java:116)',\n",
       " '\\tat java.sql.DriverManager.getConnection(DriverManager.java:571)',\n",
       " '\\tat java.sql.DriverManager.getConnection(DriverManager.java:215)',\n",
       " '\\tat org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:880)',\n",
       " '\\tat org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:52)',\n",
       " '\\tat org.apache.sqoop.manager.Db2Manager.listDatabases(Db2Manager.java:95)',\n",
       " '\\t... 7 more']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in dbList if a.startswith('FDWDRVDECSCF')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likewise get the list of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tblList = !sqoop list-tables --connect {dbip} --username {user} --password-file {pfile}\n",
    "#jdbc:db2://10.96.37.166:60100/FDW2P \n",
    "tblList = tblList[5:]\n",
    "print \"There are {0} tables in these schemas.\".format(len(tblList))\n",
    "[a for a in tblList if a.startswith('HLDI')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check what is present in this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls -R -h {hdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove everything in this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/01/23 12:23:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.\r\n",
      "Moved: 'hdfs://nameservice1/user/kesj/sqooptest' to trash at: hdfs://nameservice1/user/kesj/.Trash/Current\r\n"
     ]
    }
   ],
   "source": [
    "# remove everything\n",
    "!hadoop fs -rm -r {hdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to pull all of FDWATOMCAE.AUTO_EST_SECT (a small file)\n",
    "1. single threaded; as avro\n",
    "2. single-threaded; text\n",
    "3. parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.AUTO_EST_SECT -m 1 --target-dir sqooptest/avrotest/ae_sect_single -as-avrodatafile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.AUTO_EST_SECT -m 1 --target-dir sqooptest/texttest/ae_sect_single_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in parallel\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.AUTO_EST_SECT -m 4 --target-dir sqooptest/texttest/ae_sect_quad --split-by 'AUTO_EST_SECT_DIM_ID'\n",
    "#!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.AUTO_EST_SECT -m 4 --target-dir sqooptest/avrotest/ae_sect_quad --split-by 'AUTO_EST_SECT_DIM_ID' --as-avrodatafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now begin to show examples that fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Y1753CAA\n",
    "tdir = hdir+'/DETL/'+'y1753caa'\n",
    "print \"putting data into this directory:\", tdir\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID in (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753CAA')\" --target-dir {tdir} -m 4 --split-by \"DETL_DIM_ID\" --fields-terminated-by \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putting data into this directory: /user/kesj/sqooptest//DETL/y1753aaa\n",
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "15/01/23 12:33:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "15/01/23 12:33:48 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "15/01/23 12:33:48 INFO tool.CodeGenTool: Beginning code generation\n",
      "15/01/23 12:33:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:33:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:33:51 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/aba950152c421b1a4680eeb6ee935057/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "15/01/23 12:33:52 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/aba950152c421b1a4680eeb6ee935057/FDWATOMCAE.DETL.jar\n",
      "15/01/23 12:33:52 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\n",
      "15/01/23 12:33:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "15/01/23 12:33:52 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:33:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/01/23 12:33:53 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5419 for kesj on ha-hdfs:nameservice1\n",
      "15/01/23 12:33:53 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5419 for kesj)\n",
      "15/01/23 12:33:53 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm707\n",
      "15/01/23 12:33:55 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "15/01/23 12:33:55 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(DETL_DIM_ID), MAX(DETL_DIM_ID) FROM FDWATOMCAE.DETL WHERE ( LOS_EST_DIM_ID in (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') )\n",
      "15/01/23 12:34:54 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "15/01/23 12:34:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1421706220043_0140\n",
      "15/01/23 12:34:54 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5419 for kesj)\n",
      "15/01/23 12:34:55 INFO impl.YarnClientImpl: Submitted application application_1421706220043_0140\n",
      "15/01/23 12:34:55 INFO mapreduce.Job: The url to track the job: http://da74wbrmgr1.opr.statefarm.org:8088/proxy/application_1421706220043_0140/\n",
      "15/01/23 12:34:55 INFO mapreduce.Job: Running job: job_1421706220043_0140\n",
      "15/01/23 12:35:08 INFO mapreduce.Job: Job job_1421706220043_0140 running in uber mode : false\n",
      "15/01/23 12:35:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/23 12:35:36 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "15/01/23 12:35:37 INFO mapreduce.Job: Task Id : attempt_1421706220043_0140_m_000000_0, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 74595\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:35:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/23 12:35:42 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "15/01/23 12:35:45 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "15/01/23 12:35:51 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "15/01/23 12:35:58 INFO mapreduce.Job: Task Id : attempt_1421706220043_0140_m_000000_1, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 192455\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:36:19 INFO mapreduce.Job: Task Id : attempt_1421706220043_0140_m_000000_2, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 209400\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:36:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/01/23 12:36:39 INFO mapreduce.Job: Job job_1421706220043_0140 failed with state FAILED due to: Task failed task_1421706220043_0140_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "15/01/23 12:36:39 INFO mapreduce.Job: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=351253\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=391\n",
      "\t\tHDFS: Number of bytes written=9413167\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=4\n",
      "\t\tLaunched map tasks=7\n",
      "\t\tOther local map tasks=7\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1389996\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=231666\n",
      "\t\tTotal vcore-seconds taken by all map tasks=231666\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1423355904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=32111\n",
      "\t\tMap output records=32111\n",
      "\t\tInput split bytes=391\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=114\n",
      "\t\tCPU time spent (ms)=16240\n",
      "\t\tPhysical memory (bytes) snapshot=1893687296\n",
      "\t\tVirtual memory (bytes) snapshot=15728566272\n",
      "\t\tTotal committed heap usage (bytes)=6175064064\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9413167\n",
      "15/01/23 12:36:39 INFO mapreduce.ImportJobBase: Transferred 8.9771 MB in 166.1729 seconds (55.3192 KB/sec)\n",
      "15/01/23 12:36:39 INFO mapreduce.ImportJobBase: Retrieved 32111 records.\n",
      "15/01/23 12:36:39 ERROR tool.ImportTool: Error during import: Import job failed!\n"
     ]
    }
   ],
   "source": [
    "# Y1753AAA\n",
    "tdir = hdir+'/DETL/'+'y1753aaa'\n",
    "print \"putting data into this directory:\", tdir\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID in (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA')\" --target-dir {tdir} -m 4 --split-by \"DETL_DIM_ID\" --fields-terminated-by \"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hunting down the line that fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putting data into this directory: /user/kesj/sqooptest//DETL/y1753aaa_huntingworks\n",
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "15/01/23 12:25:34 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "15/01/23 12:25:35 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "15/01/23 12:25:35 INFO tool.CodeGenTool: Beginning code generation\n",
      "15/01/23 12:25:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:25:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:25:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/e15576189541ea69850a9070086ba8c2/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "15/01/23 12:25:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/e15576189541ea69850a9070086ba8c2/FDWATOMCAE.DETL.jar\n",
      "15/01/23 12:25:38 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\n",
      "15/01/23 12:25:38 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "15/01/23 12:25:38 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:25:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/01/23 12:25:38 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5418 for kesj on ha-hdfs:nameservice1\n",
      "15/01/23 12:25:38 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5418 for kesj)\n",
      "15/01/23 12:25:38 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm707\n",
      "15/01/23 12:25:40 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "15/01/23 12:25:40 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/01/23 12:25:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1421706220043_0139\n",
      "15/01/23 12:25:40 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5418 for kesj)\n",
      "15/01/23 12:25:41 INFO impl.YarnClientImpl: Submitted application application_1421706220043_0139\n",
      "15/01/23 12:25:41 INFO mapreduce.Job: The url to track the job: http://da74wbrmgr1.opr.statefarm.org:8088/proxy/application_1421706220043_0139/\n",
      "15/01/23 12:25:41 INFO mapreduce.Job: Running job: job_1421706220043_0139\n",
      "15/01/23 12:25:54 INFO mapreduce.Job: Job job_1421706220043_0139 running in uber mode : false\n",
      "15/01/23 12:25:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/23 12:26:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/01/23 12:26:43 INFO mapreduce.Job: Task Id : attempt_1421706220043_0139_m_000000_0, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 27093\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:26:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/23 12:27:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/01/23 12:27:20 INFO mapreduce.Job: Task Id : attempt_1421706220043_0139_m_000000_1, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 164235\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:27:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/23 12:27:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/01/23 12:27:57 INFO mapreduce.Job: Task Id : attempt_1421706220043_0139_m_000000_2, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 184083\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:27:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/23 12:28:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/01/23 12:28:34 INFO mapreduce.Job: Job job_1421706220043_0139 failed with state FAILED due to: Task failed task_1421706220043_0139_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "15/01/23 12:28:34 INFO mapreduce.Job: Counters: 8\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=4\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=882510\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=147085\n",
      "\t\tTotal vcore-seconds taken by all map tasks=147085\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=903690240\n",
      "15/01/23 12:28:34 WARN mapreduce.Counters: Group FileSystemCounters is deprecated. Use org.apache.hadoop.mapreduce.FileSystemCounter instead\n",
      "15/01/23 12:28:34 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 175.5429 seconds (0 bytes/sec)\n",
      "15/01/23 12:28:34 WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "15/01/23 12:28:34 INFO mapreduce.ImportJobBase: Retrieved 0 records.\n",
      "15/01/23 12:28:34 ERROR tool.ImportTool: Error during import: Import job failed!\n"
     ]
    }
   ],
   "source": [
    "# Y1753AAA\n",
    "tdir = hdir+'/DETL/'+'y1753aaa_hunting'+'works'\n",
    "print \"putting data into this directory:\", tdir\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID in (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') and DETL_DIM_ID <= 1533425450\" --target-dir {tdir} -m 1 --fields-terminated-by \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putting data into this directory: /user/kesj/sqooptest//DETL/y1753aaa_huntingfails\n",
      "Warning: /opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "15/01/23 12:23:28 INFO sqoop.Sqoop: Running Sqoop version: 1.4.4-cdh5.1.2\n",
      "15/01/23 12:23:29 INFO manager.SqlManager: Using default fetchSize of 1000\n",
      "15/01/23 12:23:29 INFO tool.CodeGenTool: Beginning code generation\n",
      "15/01/23 12:23:31 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:23:31 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:23:31 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-kesj/compile/450105bff4bb0d12b8032ffea4ba6af0/FDWATOMCAE_DETL.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "15/01/23 12:23:32 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-kesj/compile/450105bff4bb0d12b8032ffea4ba6af0/FDWATOMCAE.DETL.jar\n",
      "15/01/23 12:23:33 INFO mapreduce.ImportJobBase: Beginning import of FDWATOMCAE.DETL\n",
      "15/01/23 12:23:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "15/01/23 12:23:33 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM FDWATOMCAE.DETL AS t WHERE 1=0\n",
      "15/01/23 12:23:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/01/23 12:23:33 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5417 for kesj on ha-hdfs:nameservice1\n",
      "15/01/23 12:23:33 INFO security.TokenCache: Got dt for hdfs://nameservice1; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5417 for kesj)\n",
      "15/01/23 12:23:33 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm707\n",
      "15/01/23 12:23:35 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "15/01/23 12:23:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/01/23 12:23:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1421706220043_0138\n",
      "15/01/23 12:23:35 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:nameservice1, Ident: (HDFS_DELEGATION_TOKEN token 5417 for kesj)\n",
      "15/01/23 12:23:35 INFO impl.YarnClientImpl: Submitted application application_1421706220043_0138\n",
      "15/01/23 12:23:35 INFO mapreduce.Job: The url to track the job: http://da74wbrmgr1.opr.statefarm.org:8088/proxy/application_1421706220043_0138/\n",
      "15/01/23 12:23:35 INFO mapreduce.Job: Running job: job_1421706220043_0138\n",
      "15/01/23 12:23:48 INFO mapreduce.Job: Job job_1421706220043_0138 running in uber mode : false\n",
      "15/01/23 12:23:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/01/23 12:24:34 INFO mapreduce.Job: Task Id : attempt_1421706220043_0138_m_000000_0, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 79\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:24:50 INFO mapreduce.Job: Task Id : attempt_1421706220043_0138_m_000000_1, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 79\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:25:04 INFO mapreduce.Job: Task Id : attempt_1421706220043_0138_m_000000_2, Status : FAILED\n",
      "Error: java.io.IOException: SQLException in nextKeyValue\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)\n",
      "\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)\n",
      "\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n",
      "\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n",
      "\tat org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n",
      "Caused by: com.ibm.db2.jcc.am.SqlException: [jcc][t4][1065][12306][4.15.113] Caught java.io.CharConversionException.  See attached Throwable for details. ERRORCODE=-4220, SQLSTATE=null\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:680)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:60)\n",
      "\tat com.ibm.db2.jcc.am.fd.a(fd.java:112)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2870)\n",
      "\tat com.ibm.db2.jcc.am.jc.p(jc.java:527)\n",
      "\tat com.ibm.db2.jcc.am.jc.N(jc.java:1563)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getStringX(ResultSet.java:1153)\n",
      "\tat com.ibm.db2.jcc.am.ResultSet.getString(ResultSet.java:1128)\n",
      "\tat org.apache.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:71)\n",
      "\tat com.cloudera.sqoop.lib.JdbcWritableBridge.readString(JdbcWritableBridge.java:61)\n",
      "\tat FDWATOMCAE_DETL.readFields(FDWATOMCAE_DETL.java:899)\n",
      "\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:244)\n",
      "\t... 12 more\n",
      "Caused by: java.nio.charset.MalformedInputException: Input length = 79\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:19)\n",
      "\tat com.ibm.db2.jcc.am.jc.a(jc.java:2862)\n",
      "\t... 20 more\n",
      "Caused by: sun.io.MalformedInputException\n",
      "\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:105)\n",
      "\tat com.ibm.db2.jcc.am.r.a(r.java:16)\n",
      "\t... 21 more\n",
      "\n",
      "15/01/23 12:25:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/01/23 12:25:19 INFO mapreduce.Job: Job job_1421706220043_0138 failed with state FAILED due to: Task failed task_1421706220043_0138_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "15/01/23 12:25:19 INFO mapreduce.Job: Counters: 8\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=4\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=473022\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=78837\n",
      "\t\tTotal vcore-seconds taken by all map tasks=78837\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=484374528\n",
      "15/01/23 12:25:19 WARN mapreduce.Counters: Group FileSystemCounters is deprecated. Use org.apache.hadoop.mapreduce.FileSystemCounter instead\n",
      "15/01/23 12:25:19 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 106.3217 seconds (0 bytes/sec)\n",
      "15/01/23 12:25:19 WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead\n",
      "15/01/23 12:25:19 INFO mapreduce.ImportJobBase: Retrieved 0 records.\n",
      "15/01/23 12:25:19 ERROR tool.ImportTool: Error during import: Import job failed!\n"
     ]
    }
   ],
   "source": [
    "# Y1753AAA\n",
    "tdir = hdir+'/DETL/'+'y1753aaa_hunting'+'fails'\n",
    "print \"putting data into this directory:\", tdir\n",
    "!sqoop import --connect {dbip} --username {user} --password-file {pfile} --table FDWATOMCAE.DETL --where \"LOS_EST_DIM_ID in (SELECT LOS_EST_DIM_ID FROM FDWATOMCAE.LOS_EST WHERE VNDR_VEH_CD='Y1753AAA') and DETL_DIM_ID = 1533425450\" --target-dir {tdir} -m 1 --fields-terminated-by \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
